{"chunk_id": "2025_M_IUS_Novitsky_DO.pdf:0", "source": "2025_M_IUS_Novitsky_DO.pdf", "text": "Міністерство освіти і науки України Харківський національний університет радіоелектроніки Факультет Комп’ютерних наук (повна назва) Кафедра Інформаційних управляючих систем (повна назва) КВАЛІФІК АЦІЙНА РОБОТА Пояснювальна записка рівень вищої о світи другий (магістерський) Дослідження моделей та методів виявлення аномалій у фінансових транзакціях інформаційної системи банківського обслуговування (тема) Виконав: здобувач 2 року навчання , групи ІУСТм -24-1 Данило НОВИЦЬКИЙ (власне і м’я, ПРІЗВИЩЕ ) Спеціальн ість 122 К омп’ютерні науки (код і повна назва спеціальності) Тип програми освітньо -професійна (освітньо -професійна або освітньо -наукова) Освітня програма Інформаційні управляючі системи та технології (повна назва освітньої програми) Керівни к: доц. каф. ІУС Олена МІХНОВА (посада, власне ім’я, ПРІЗВИЩЕ ) Допускається до захисту Завідувач кафедри ІУС Костянтин ПЕТРОВ (підпис) (власне ім’я, ПРІЗВИЩЕ ) 2025 р. 2 Харківський національний університет радіоелектроніки ЗАТВЕРДЖУЮ: Зав. кафедри (підпис) “ 24 ” грудня 20 25 р. ЗАВДАННЯ НА КВАЛІФІК АЦІЙНУ РОБОТУ здобуваче ві Новицькому Данилові Олексійовичу (прізвище, ім’я, по батькові) 1. Тема роботи Дослідження моделей та методів виявлення аномалій у фінансових транзакціях інформаційної системи банківського обслуговування затверджена наказом по університету від “ 24 ” листопада 2025 р. № 1055Ст 2. Термін подання здобув ачем роботи до екзаменаційної комісії “ 18 ” грудня 2025 р. 3. Вихідні дані до роботи матеріали звіту з передатестаційної практики , сучасні наукові дослідження та публікації з методів виявлення аномалій, статистичні характеристики фінансових транз акцій та поведінкових патернів клієнтів , набори транзакційних даних для моделювання та експериментальної перевірки , алгоритми машинного навчання та глибинного навчання . 4. Перелік питань, що потрібно опрацювати у роботі провести аналіз предметної обл асті виявлення аномалій у фінансових транзакціях та визначити ключові проблеми ; виконати огляд сучасних моделей і методів детектування аномалій ; розробити концепцію синтез ованого методу HASBT, що поєднує статистичні методи , моделі машинного навчання т а глибинного моделювання ; побудувати архітектуру програмної реалізації методу ; реалізувати програмний прототип модулів; провести експериментальну перевірку ефективності моделей та оцінити роботу ; порівняти ефективність запропонованої технології HASBT з існуючими підходами . Факультет Комп’ютерних наук Кафедра Інформаційних управляючих систем Рівень вищої освіти другий (магістерський) Спеціальність 122 К омп’ютерні науки (код і повна назва) Тип програми освітньо -професійна (освітньо -професійна або осві тньо-наукова) Освітня програма Інформаційні управляючі систе ми та технології (повна назва) 3 КАЛЕНДАРНИЙ ПЛАН № Назва етапів роботи Термін виконання етапів роботи Примітка 1 Аналіз предметної області та огляд сучасних методів виявлення аномалій у фінансових транзакціях 24.11.2025 – 25.11.2025 Виконано 2 Дослідження мат ематичних основ статистичних моделей, алгоритмів ML та Autoencoder 26.11.2025 – 27.11.2025 Виконано 3 Побудова архітектури системи та IDEF0 -діаграм 28.11.2025 – 02.12.2025 Виконано 4 Розроблення концепції синте зованого методу HASBT 03.12.2025 – 04.12.20 25 Виконано 5 Підготовка даних до проведення експеримент ів 04.12.2025 – 05.12.2025 Виконано 6 Проведення експериментальних досліджень та аналіз результатів 06.12.2025 – 06.12.2025 Виконано 7 Порівняння ефективності синте зованого методу з існуючими підх одами 06.12.2025 – 07.12.2025 Виконано 8 Оформлення пояснювальної записки 07.12.2025 – 07.12.2025 Виконано 9 Захист кваліфікаційної роботи 18.12.2025 Виконано Дата видачі завдання __24___ ____ листопада ___20 25 р. Здобувач (підпис) Kepiвник роботи __доц. каф. ІУС Олена МІХНОВА _ (підпис) ( посада, власне ім’я, ПРІЗВИЩЕ ) 4 РЕФЕРАТ Пояснювальна записка кваліфік аційної роботи: 78 с., 13 рис., 4 табл., 1 дод., 20 джерел. АНОМАЛІЇ, АВТОЕНКОДЕР , АНСАМБЛЕВІ МЕТОДИ , МАШИННЕ НАВЧАННЯ, ФІНАНСОВІ ТРАНЗАКЦІЇ, ШАХРАЙСТВО , HASBT , ISOLATION FOREST , ONE -CLASS SVM . Об’єктом дослідження кваліфікаційної роботи є процеси виявлення аномалій та шахрайських дій у фінансових транзакціях банківських інформаційних систем. Предметом дослідження є моделі, методи та алгоритми автоматизованого аналізу транзакцій з метою виявлення нестандартної або потенційно шахрайської поведінки користувачів у реальному часі. Метою роботи є підвищення точності та ефективності виявлення аномалій у фінансових транзакціях шляхом розроблення синте зованого методу , що поєднує статистичні методи, алгори тми машинного навчання та глибинні нейронні моделі. Для досягнення поставленої мети було виконано такі основні завдання: проаналізовано сучасні підходи до виявлення аномалій у банківських транзакціях та визначено їх переваги й недоліки , досліджено математи чні основи статистичних методів, моделей машинного навчання та автоенкодерів , розроблено синте зований підхід HASBT , що поєднує три рівні детектування , побудовано архітектуру програмної реалізації та створено модульний прототип системи , проведено експеримен тальну перевірку ефективності окремих моделей та ансамблю , виконано порівняння запропонованої технології з існуючими базовими методами виявлення аномалій. 5 Практичне значення роботи полягає у можливості використання отриманих результатів та розробленої техн ології як основи для побудови або вдосконалення прототипів систем моніторингу фінансових транзакцій. 6 ABSTRACT Masters’s thesis: 78 pages, 13 figures, 4 tables, 1 appendices, 20 sources. ANOMALIES , AUTOENCODER , ENSEMBLE METHODS, MACHINE LEARNING, FINAN CIAL TRANSACTIONS , FRAUD, HASBT , SOLATION FOREST, ONE -CLASS SVM . The object of the qualification work is the processes of detecting anomalies and fraudulent actions in financial transactions of banking information systems. The subject of the research is models, methods and algorithms of automated transaction analysis in order to detect non -standard or potentially fraudulent user behavior in real time. The aim of"}
{"chunk_id": "2025_M_IUS_Novitsky_DO.pdf:1", "source": "2025_M_IUS_Novitsky_DO.pdf", "text": "4 tables, 1 appendices, 20 sources. ANOMALIES , AUTOENCODER , ENSEMBLE METHODS, MACHINE LEARNING, FINAN CIAL TRANSACTIONS , FRAUD, HASBT , SOLATION FOREST, ONE -CLASS SVM . The object of the qualification work is the processes of detecting anomalies and fraudulent actions in financial transactions of banking information systems. The subject of the research is models, methods and algorithms of automated transaction analysis in order to detect non -standard or potentially fraudulent user behavior in real time. The aim of the work is to increase the accuracy and efficiency of detecting anomalies in financial transa ctions by developing a synthesized information technology that combines statistical methods, machine learning algorithms and deep neural models. To achieve the set goal, the following main tasks were performed: modern approaches to detecting anomalies in b anking transactions were analyzed and their advantages and disadvantages were identified, the mathematical foundations of statistical methods, machine learning models and autoencoders were studied, a synthesized HASBT approach was developed that combines t hree levels of detection, the software implementation architecture was built and a modular prototype of the system was created, an experimental test of the effectiveness of individual models and the ensemble was conducted, and the proposed technology was c ompared with existing basic methods of detecting anomalies. The practical significance of the work lies in the possibility of using the obtained results and the developed technology as a basis for building or improving prototypes of financial transaction m onitoring systems. 7 ЗМІСТ Скорочення та умовні познаки ................................ ................................ .............. 9 Вступ ................................ ................................ ................................ ....................... 10 1 Аналіз предметної області та сучасних підходів до виявлення аномалій у фінансових транзакціях ................................ ................................ ..... 11 1.1 Актуальність проблеми ................................ ................................ .............. 11 1.2 Характеристика банківських інформаційних систем (БІС) та транзакцій ................................ ................................ ................................ ........... 13 1.2.1 Архітектура БІС ................................ ................................ ................... 13 1.2.2 Особливості фі нансових транзакцій як об’єкта аналізу .................. 14 1.3 Аналіз сучасних методів виявлення аномалій ................................ ......... 15 1.4 Проблеми застосування систем виявл ення аномалій .............................. 17 1.5 Постановка задачі дослідження ................................ ................................ . 19 2. Теоретичні основи моделей і методів виявлення аномалій .......................... 22 2.1 Математичні основи базових алгоритмів ................................ ................. 22 2.1.1 Метод Isolation Forest ................................ ................................ ........... 22 2.1.2 Метод One -Class SVM ................................ ................................ ......... 23 2.1.3 Autoencoder ................................ ................................ ........................... 24 2.2 Порівняльний аналіз алгоритмів ................................ ............................... 25 2.3 Теоретична модель інтеграції системи виявлення аномалій .................. 28 3 Синтезований метод виявлення аномалій у фінансових транзакціях .......... 30 3.1 Загальна концепція синтезованого методу ................................ ............... 30 3.2 Структура синтезованого методу HASBT ................................ ................ 32 3.3 Інженерія ознак у синтезованому методі HASBT ................................ ... 38 3.3.1 Загальні принципи формування ознак ................................ ............... 38 3.3.2 Класи та характеристики ознак у HASBT ................................ ......... 39 3.4 Алгоритмічні компоненти синтезованого методу HASBT ..................... 42 3.4.1 Статистичний модуль первинної детекції ................................ ......... 43 8 3.4.2 Модуль машинного навчання ................................ ............................. 44 3.4.3 Глибинний модуль Autoencoder ................................ ......................... 46 Autoencoder виконує такі кл ючові функції: ................................ ............... 46 3.5 Модуль синтезу рішень (Decision Fusion Layer) ................................ ...... 48 3.5.1 Обґрунтування необхідності ансамблевого підходу в мет оді HASBT ................................ ................................ ................................ ............ 48 3.5.2 Принципи синтезу рішень та методи вибору ваг ансамблю ........... 49 3.6 Інтеграція синтезованого методу HASBT у банківську інформаційну систему ................................ ................................ ...................... 51 3.6.1 Вимоги до інтеграції у промисловому середовищі .......................... 51 3.6.2 Методологія інтеграції синте зованої моделі ................................ ..... 52 4 Програмна реалізація та експериментальна перевірка синтезованого методу HASBT ................................ ................................ ................................ ....... 55 4.1 Загальна архітектура програмної реалізац ії ................................ ............. 55 4.2 Реалізація окремих модулів методу HASBT ................................ ............ 56 4.3 Методика експериментальної перевірки та результати оцінювання ефективност і ................................ ................................ ................. 60 Висновки ................................ ................................ ................................ ................ 63 Перелік джерел посилання ................................ ................................ ................... 64 Додаток А Графічний матеріал кваліфікаційної роботи ................................ ... 67 9 СКОРОЧЕННЯ ТА УМОВНІ ПОЗНАКИ БІС – банківська інформаційна система LOF – Local Outlier Factor OCSVM – One-Class Support Vector Machine OLAP – Online Analytical Processing OLTP – Online Transaction Processing SOA – Service -Oriented Architecture 10 ВСТУП У сучасному банківському обслуговуванні фінансові операції здійснюються у великих обсягах та з високою швидкістю, що створює нові можливості та ризики. Одним з головних ризиків є виникнення аномальних або шахрайських транзакцій, які можуть призвести до значних фінансових втрат, порушенн ю довіри клієнтів та стабільності банківської системи. Традиційні методи контролю – перевірка правил, фіксовані порогові значення або ручний аудит – виявляються неефективними через швид ке зростання обсягу даних та складність методів шахрайства. Тому на перший план виходять методи засновані на інтелектуальному аналізі даних, машинному навчанні та штучному інтелекті. Виявлення аномалій фінансових транзакцій дозволяє виявляти підозрілу діял ьність, що відхиляється від звичайної поведінки системи або користувачів. Сучасні моделі аналізують багатовимірні дані – суми, час, пристрої, геолокацію, моделі поведінки і на основі цього створюють оцінку ризику для кожної транзакції. Використання таких п ідходів забезпечує більш гнучку, адаптивну та ефективну систему захисту. 11"}
{"chunk_id": "2025_M_IUS_Novitsky_DO.pdf:2", "source": "2025_M_IUS_Novitsky_DO.pdf", "text": "виявляються неефективними через швид ке зростання обсягу даних та складність методів шахрайства. Тому на перший план виходять методи засновані на інтелектуальному аналізі даних, машинному навчанні та штучному інтелекті. Виявлення аномалій фінансових транзакцій дозволяє виявляти підозрілу діял ьність, що відхиляється від звичайної поведінки системи або користувачів. Сучасні моделі аналізують багатовимірні дані – суми, час, пристрої, геолокацію, моделі поведінки і на основі цього створюють оцінку ризику для кожної транзакції. Використання таких п ідходів забезпечує більш гнучку, адаптивну та ефективну систему захисту. 11 1 АНАЛІЗ ПРЕДМЕТНОЇ ОБ ЛАСТІ ТА СУЧАСНИХ ПІ ДХОДІВ ДО ВИЯВЛЕННЯ АНОМАЛІ Й У ФІНАНСОВИХ ТРАНЗ АКЦІЯХ 1.1 Актуальність проблеми Зі швидким розвитком цифрових фінансових сервісів та банкі вських послуг обсяг фінансових транзакцій, що здійснюються в режимі реального часу, зростає. Це створює передумови для виникнення аномалій – операція або серія дій, що суттєво відхиляються від звичайної поведінки клієнта або типових моделей транзакцій. У с вітлі цього, виявлення порушень у фінансових операціях стає критично важливою сферою для забезпечення надійності та безпеки банківських інформаційних систем. З огляду на значний обсяг платежів, що здійснюються щодня, та нестачу фактичних аномальних транзак цій у системах високовартісних платежів, виявлення аномалій нагадує спробу знайти голку в копиці сіна [1]. Більше того, складність полягає не лише в обсягах шахрайських схем та транзакцій, але й у швидкості змін їхнього характеру. У сучасних фінансових процесах аномальні транзакції можуть бути різноманітними за типом, структурою, каналом передачі та географічним розташуванням, що вимагає високого ступеня адаптивності від систем виявлення. Таким чином, актуальність теми полягає в поєднанні трьох основних факторів: - величезного зростання обсягів фінансових операцій; - прагнення банків автоматизувати та застосовувати аналітику і штучний інтелект; - зростання та розвиток складності систем виявлення аномальної або шахрайської діяльності. Всі ці фактори створюють середовище, в якому ефективна система виявлення аномалій є необхідною умовою забезпечення безпеки банківських інформаційних систем. 12 Зростання обсягу фінансових транзакцій та ускладнення механізмів шахрайства змушують банки не лише модернізувати свої технол огічні платформи, а й трансформувати внутрішні управлінські та операційні процеси [ 2]. Сучасні системи виявлення аномалій інтегруються у широкий спектр банківських бізнес -процесів, взаємодіючи з підрозділами, які відповідають за управління ризиками, фінанс овий моніторинг, інформаційну безпеку, контроль транзакцій, аналітику та ІТ -підтримку. Ефективність таких систем багато в чому залежить від координації роботи структурних підрозділів банку, які відповідають за збір, передачу, аналіз та реагування на підозр ілі транзакції. Тому для розуміння контексту системи виявлення аномалій важливо враховувати організаційну структуру банківської установи, в рамках якої обробляються фінансові транзакції та контролюється їхня безпека. Нижче (рисунок 1.1) наведено узагальнен у організаційну структуру банку на основі офіційного документу [ 3], що показує розподіл обов'язків між підрозділами та їх взаємодію в галузях управління ризиками, безпеки та обробки транзакцій: Рисунок 1.1 – Структура ІС банку 13 1.2 Характеристика банкі вських інформаційних систем (БІС) та транзакцій 1.2.1 Архітектура БІС Банківські інформаційні системи є ключовими компонентами сучасної фінансової інфраструктури, які автоматизують, обробляють та контролюють фінансові операції клієнтів. Типові операції, що обробляються такими системами, включають грошові перекази, оплату товарів та послуг, внесення та зняття готівки, депозитні послуги, міжнародні перекази та інші види фінансових потоків. БІС повинна працювати безперервно, з високою продуктивністю та надій ністю, оскільки будь -яка затримка або збій може призвести до підр иву довіри клієнтів або значних фінансових втрат. Типова архітектура БІС включає декілька функціональних модулів: - модуль обробки транзакцій – отримує, перевіряє, керує та записує транзакції в базах даних; - модуль потокової обробки – використовується для аналізу транзакцій у режимі реального часу, що дозволяє швидко виявляти підозрілі транзакції; - модуль зберігання історичних даних – збирає історію транзакцій для подальшого аналізу та навчання мо делей виявлення аномалій; - аналітичний модуль – виявляє закономірності, тенденції та аномалії в даних; - модуль моніторингу та реагування на інциденти – відповідає за сповіщення, блокування транзакцій та подальше розслідування інцидентів. Такі системи зазвича й побудовані на SOA або мікросервісному підході, що дозволяє гнучко масштабувати окремі компоненти. Обробка транзакцій зазвичай виконується за допомогою технологій OLTP, а аналітичні завдання виконуються за допомогою технологій OLAP. Сучасні банківські сис теми також інтегруються з платіжними 14 шлюзами, аналітичними платформами та інструментами боротьби з шахрайством і повинні відповідати міжнародним стандартам безпеки. Нижче (рисунок 1.2) наведено приклад функціональної структури ІС банку: Рисунок 1.2 – Функціональн а структур а ІС банку 1.2.2 Особливості фінансових транзакцій як об’єкта аналізу Фінансова транзакція – це структурований запис грошової операції, що містить набір параметрів (атрибутів), що відображають як саму подію, так і її контекст. Типов і атрибути: - сума операції; - дата й час проведення; - географічне розташування відправника та одержувача; - тип і канал проведення; - тип пристрою або браузера; - інформація про рахунок, банк -емітент, IP -адресу; - історія попередніх транзакцій користувача. Окрім стати чних характеристик, важливими також є поведінкові характеристики клієнта – частота транзакцій, середній розмір транзакції, часові інтервали між транзакціями, моделі активності, що спостерігаються в різний час доби або години. Такі показники формують так зв аний поведінковий профіль клієнта, який можна використовувати для виявлення 15 аномалій. Індикатори зміни поведінки дозволяють значно точніше ідентифікувати"}
{"chunk_id": "2025_M_IUS_Novitsky_DO.pdf:3", "source": "2025_M_IUS_Novitsky_DO.pdf", "text": "розташування відправника та одержувача; - тип і канал проведення; - тип пристрою або браузера; - інформація про рахунок, банк -емітент, IP -адресу; - історія попередніх транзакцій користувача. Окрім стати чних характеристик, важливими також є поведінкові характеристики клієнта – частота транзакцій, середній розмір транзакції, часові інтервали між транзакціями, моделі активності, що спостерігаються в різний час доби або години. Такі показники формують так зв аний поведінковий профіль клієнта, який можна використовувати для виявлення 15 аномалій. Індикатори зміни поведінки дозволяють значно точніше ідентифікувати аномальні фінансові операції [ 4]. 1.3 Аналіз сучасних методів виявлення аномалій Методи виявлення аномалій поділяють на статистичні, класичні алгоритми машинного навчання, глибоке навчання та підходи на основі графів; кожен клас має свої сильні й слабкі сторони та сферу застосування. Методи на основі правил та порогові системи: прості, інтуїтивно зрозу мілі підходи, що базуються на експертних правилах наприклад, необґрунтовано велика сума, транзакція в іноземній країні тощо. Переваги: прозорість, швидкість, легка інтеграція в бізнес -процеси. Недоліки: низька адаптивність, потребують постійного оновлення правил, погано працюють зі складними схемами шахрайства. Ці методи часто використовуються як перший рівень фільтрації. Класичні статистичні методи основані на модел ях, що оцінюють ймовірнісні характеристики транзакцій. Використовуються для виявлення окреми х викидів на основі простих статистичних ознак. Мають обмежену здатність фіксувати багатовимірні, контекстні або колективні аномалії. Алгоритми без нагляду та напівконтрольовані: – кластеризація: відділяють «рідкісні» спостереження від великих груп; чутлив і до масштабу та вибору параметрів ; – Local Outlier Factor (LOF), One -Class support vector machine (SVM ). Корисний для багатовимірних даних, але вимагає точного налаштуван ня та чутлива до кількості шуму; – Isolation Forest . Алгоритм, який ізолює аномалії ш ляхом побудови випадкових дерев; добре масштабується та часто використовується для транзакційних даних. Isolation Forest продемонстрував високу 16 продуктивність у багатьох прикладних дослідженнях, що стосуються кредитних транзакцій. Класичні методи під нагля дом. Використовують мічені приклади шахрайства для побудови класифікаторів (логістична регресія, Random Forest, XGBoost). Переваги – висока точність при достатній якості міток. Проблеми – значна незбалансованість класів (дуже мало випадків шахрайства), «за старівання» моделей коли шахраї змінюють тактику, труднощі зі збором надійних міток. Для подолання дисбалансу часто використовуються методи повторної вибірки, зваження класів та спеціальні метрики. До методів на основі глиб окого навчання належать : – автоен кодер и (від англ., autoencoder ) – навчаються відтворювати нормальні транзакції; великі похибки реконструкції вказують на аномалію. Ефективні там, де мало позначених аномалій ; – рекурентні нейромережі та трансформери – моделюють послідовності транзакцій зад аного клієнта (часові шаблони); вони дозволяють виявляти аномалії в поведінці корист увача; – комбінації – наприклад, автоенкодер на частотних ознаках і класифікатор. Глиб оке навчання дає кращі результати на великих даних, але потребує обчислювальних ресурс ів та складніше для інтерпретації. Мережеві та графові підходи: графові нейронні мережі дозволяють моделювати відношення й виявляти «синдикати» шахрайських дій. Останні дослідження показують, що графові нейронні мережі добре вловлюють міжсуб’єктні аномалії й суттєво підвищують якість детекції у випадках, де прості табличні методи зазнають поразки. Гібридні та ансамблеві методи: поєднання декількох підходів, підвищує показники і знижує кількість хибних спрацьовувань. Ансамблі дозволяють поєднати сильні сторо ни різних класів моделей, але ускладнюють інтерпретацію та експлуатацію. 17 1.4 Проблеми застосування систем виявлення аномалій Незважаючи на стрімкий розвиток методів машинного навчання та збільшення обчислювальної потужності, впровадження систем виявлен ня аномалій у фінансовому секторі залишається складним завданням. Банківські системи стикаються не лише з технічними перешкодами, а й з організаційними та методологічними проблемами. Однією з найсерйозніших проблем є значний дисбаланс вибірки – кількість н ормальних (легітимних) транзакцій у сотні тисяч разів перевищує кількість шахрайських транзакцій. У типовій базі даних кредитних карток лише близько 0,17% транзакцій позначаються як шахрайські. За таких умов стандартні алгоритми класифікації схильні «навча тися» надавати перевагу більшості, ігноруючи рідкісні події. Це призводить до високої загальної точності, але низьких показників виявлення шахрайства. Щоб подолати це, використовуються методи повторної вибірки, змінні ваги класів, спеціальні метрики (точні сть, повнота, F1 -оцінка) або напівконтрольовані підходи. Але ці методи не надають гарантію, що будуть стабільні результати, якщо дані містять шум, дублювання, помилки або відсутні ознаки. Ще однією проблемою є якість даних. Транзакції у фінансових система х часто неповні, а деякі поля навмисно скриті для забезпечення захисту даних. Це ускладнює розробку інформативних ознак та знижує ефективність моделей. Фінансове шахрайство характеризується моделями поведінки, що постійно змінюються. Шахраї швидко адаптуют ься до нових правил, змінюють методи обходу заходів безпеки та використовують автоматизовані інструменти для створення нових типів транзакцій. Це явище відоме як concept drift — зміна статистичних характеристик даних у часі [ 5]. Якщо модель не оновлюється регулярно, її продуктивність швидко знижується. Виявлення дріфту в режимі реального часу потребує складних механізмів 18 моніторингу, періодичного навчання моделі та систем безперервного контролю якості. У банківському секторі важливо пояснювати результати. Н авіть якщо модель демонструє високу точність, її рішення мають бути зрозумілими як аналітикам, так і органам, що регулюють. Багато країн вимагають від фінансових установ пояснювати, чому конкретну транзакцію було відхилено або позначено як підозрілу. Ще од на проблема пов'язана з тим, що більшість банків мають історично сформовані,"}
{"chunk_id": "2025_M_IUS_Novitsky_DO.pdf:4", "source": "2025_M_IUS_Novitsky_DO.pdf", "text": "не оновлюється регулярно, її продуктивність швидко знижується. Виявлення дріфту в режимі реального часу потребує складних механізмів 18 моніторингу, періодичного навчання моделі та систем безперервного контролю якості. У банківському секторі важливо пояснювати результати. Н авіть якщо модель демонструє високу точність, її рішення мають бути зрозумілими як аналітикам, так і органам, що регулюють. Багато країн вимагають від фінансових установ пояснювати, чому конкретну транзакцію було відхилено або позначено як підозрілу. Ще од на проблема пов'язана з тим, що більшість банків мають історично сформовані, гетерогенні інформаційні системи, у яких транзакційні дані зберігаються в різних форматах, базах і каналах. Інтеграція систем виявлення аномалій у таку інфраструктуру часто усклад нюється: - застарілими архітектурними рішеннями (монолітні системи без API або з мінімальною масштабованістю); - обмеженнями доступу до даних у режимі реального часу; - компромісом між швидкістю обробки транзакцій та затримкою складних моделей; - вимогами до висок ої надійності часу безперебійної роботи та відсутності затримок перевірки транзакцій. В результаті, навіть добре навчена модель може бути непридатною для операційного використання, якщо її неможливо ефективно інтегрувати у виробниче середовище банку. Висо кі обчислювальні вимоги є ще однією серйозною проблемою. Глибокі нейронні мережі або ансамблеві алгоритми вимагають високопродуктивних графічних процесорів, великого обсягу пам'яті та розподіленого сховища. Для банків з великими обсягами транзакцій це створює проблеми масштабованості та затримки. У багатьох випадках моделі повинні працювати в потоковому режимі, тобто їм потрібно приймати рішення за частки секунди. Навіть невелика затримка може заблокувати легітимну транзакцію або, навпаки, пропустити шахрай ську транзакцію [ 6]. Окрім технічних проблем, значну роль відіграє людський фактор. 19 Навіть найкраща модель буде неефективною без належного контролю, взаємодії з аналітиками та актуальних політик безпеки. Історія показує, що у більшості банків остаточне ріш ення про блокування чи підтвердження транзакції приймає одна людина. Це вимагає розробки зручних інтерфейсів, звітів, інструментів візуалізації та систем підтримки прийняття рішень, що дозволяють фахівцям швидко оцінювати ситуацію та розуміти логіку роботи моделі. 1.5 Постановка задачі дослідження Аналіз поточного стану банківських інформаційних систем та існуючих підходів до виявлення аномалій у фінансових транзакціях показує, що традиційні інструменти моніторингу безпеки не забезпечують належного рівня захисту. Стрімке зростання цифрових транзакцій, розширення спектру фінансових послуг і поява складних шахрайських систем, що постійно розвиваються, зумовлюють необхідність у методах, що поєднують різні підходи до аналізу даних. Враховуючи високу динаміку фінансового середовища та мінливість моделей поведінки користувачів, особливого значення набуває синтез методів статистики, машинного та глибокого навчання, що дозволяє підвищити точність та стійкість систем виявлення аномалій. Створення ефективної систем и вимагає поєднання аналізу транзакцій у реальному часі, багатовимірної обробки залежностей та здатності адаптуватися до нових типів порушень без повного перенавчання. Однак розробка такого комплексного рішення ускладнюється низкою факторів: шумом даних, н ерівномірним розподілом аномальних випадків, вимогами до інтерпретації результатів, необхідністю масштабованості та можливістю інтеграції з банківською інфраструктурою. 20 У зв'язку з цими обмеженнями виникає завдання формального визначення дослідницької проб леми, що включає як аналіз окремих алгоритмів, так і вивчення можливостей їх комбінування з підвищення ефективності виявлення аномальних транзакцій. Метою даного дослідження є розробка, валідація та експериментальне тестування синт езованого методу до виявл ення аномалій, що поєднує статистичні методи, моделі машинного навчання та глибоку архітектуру та інтегрується в банківську інформаційну систему. Цей підхід має забезпечити точність, швидкість, надійність, здатність виявляти нові види шахрайства та гаранто вану інтерпретацію для бізнес -користувачів та регулюючих органів. На основі аналітичного огляду сформульовано загальну концепцію дослідження . Необхідно синтезувати сучасні моделі та методи виявлення аномалій у фінансових транзакціях, визначити можливості ї х комбінування, розробити концептуальну архітектуру комплексної системи виявлення аномалій, обґрунтувати вибір алгоритмів та принципів їх взаємодії, що забезпечують високу точність та адаптивність. Для виконання поставленої задачі передбачається здійснити такі дослідницькі кроки: - формулювання вимог до комбіновоної моделі виявлення аномалій, що враховує технічні, алгоритмічні та бізнес -орієнтовані обмеження банківського середовища; - аналіз можливостей інтеграції статистичних методів, моделей машинного навчанн я та глибоких нейронних мереж для підвищення точності та гнучкості системи; - визначення критеріїв оцінки ефективності синте зованого підходу, приділяючи особливу увагу точності, стійкості, продуктивності, масштабованості та інтерпретованості; - розробка теорет ичних основ архітектури комплексного рішення, включаючи інтеграцію алгоритмів та їхню погоджену роботу в реальних транзакційних процесах; 21 - створення бази для подальшої програмної реалізації , опис модульної структури системи, характеристик даних, механізмів уніфікації результатів моделювання та архітектури обчислювальної інфраструктури. 22 2. ТЕОРЕТИЧНІ ОСНОВИ МОДЕЛЕЙ І МЕТОДІВ ВИ ЯВЛЕННЯ АНОМАЛІЙ 2.1 Математичні основи базов их алгоритмів 2.1.1 Метод Isolation Forest Isolation Forest – алгоритм без учителя , який грунтується на ідеї, що аномальні точки легше ізолювати, ніж нормальні [7]. Замість побудови моделі нормальної поведінки чи щільності, метод випадково розбиває простір ознак до того часу, поки ознака буде повністю ізольован ою від інших. Алгоритм ген ерує ансамбль дерев рішень, де на кожному кроці вибирається випадкова ознака і випадковий поріг. Оцінка \"аномальності\" розраховується за формулою: 𝑠(𝑥,𝑚)=2−𝐸(ℎ(𝑥)) 𝑐(𝑚), (2.1) де ℎ(𝑥) – кількість розділень, необхідних для ізоляції точки 𝑥; 𝐸(ℎ(𝑥)) – середня довжина шляху до ізоляції точки 𝑥. 𝑐(𝑚)=2𝐻(𝑚−1)−2(𝑚−1) 𝑚, (2.2) де 𝑐(𝑚) – нормалізаційний коефіцієнт, що враховує розмір вибірки 𝑚; 𝐻(𝑖) – гармонічне число, яке"}
{"chunk_id": "2025_M_IUS_Novitsky_DO.pdf:5", "source": "2025_M_IUS_Novitsky_DO.pdf", "text": "поведінки чи щільності, метод випадково розбиває простір ознак до того часу, поки ознака буде повністю ізольован ою від інших. Алгоритм ген ерує ансамбль дерев рішень, де на кожному кроці вибирається випадкова ознака і випадковий поріг. Оцінка \"аномальності\" розраховується за формулою: 𝑠(𝑥,𝑚)=2−𝐸(ℎ(𝑥)) 𝑐(𝑚), (2.1) де ℎ(𝑥) – кількість розділень, необхідних для ізоляції точки 𝑥; 𝐸(ℎ(𝑥)) – середня довжина шляху до ізоляції точки 𝑥. 𝑐(𝑚)=2𝐻(𝑚−1)−2(𝑚−1) 𝑚, (2.2) де 𝑐(𝑚) – нормалізаційний коефіцієнт, що враховує розмір вибірки 𝑚; 𝐻(𝑖) – гармонічне число, яке апроксимується як 𝐻(𝑖)≈ln𝑖+ 0.577215 6649 (константа Ейлера –Маскероні). Якщо s(𝑥) близьке до 1 – об'єкт, скоріше за все, аномальний; якщо до 0.5 – нормальний. Практичні аспекти: - переваги: не потребує маркованих даних; масштабується лінійно; 23 стійкий до шумів; ефективний при більших обся гах транзакцій ; - недоліки: чутливість до розміру дерева та вибору альфа -каналу; потенційне збільшення частки хибнопозитивних резуль татів; погана інтерпретованість; - приклад у банківських системах: ізолювальні ліси часто використовуються як перший рівень для швидкої фільтрації підозрілих транзакцій у потоках даних у режимі реального часу. 2.1.2 Метод One -Class SVM One-Class Support Vector Machine – це алгоритм, що створює межу, що відокремлює «нормальні» дані від усього простору. Цей метод є напівконтрольо ваним, оскільки для навчання потрібні лише неаномальні дані. OCSVM шукає гіперплощину в просторовому ядрі 𝜙(𝑥), яка відділяє більшість спостережень від початку координат, залишаючи при цьому мінімальну частину точок поза межами [8]. Задача зводиться до м інімізації регуляризованого функціонала: min 𝜔,𝜌,𝜉1 2‖𝜔‖2+1 𝜐𝑛∑ 𝜉𝑖−𝜌𝑛 𝑖=1 , (2.3) за умов: (𝜔𝜙(𝑥𝑖))≥𝜌−𝜉𝑖, 𝜉𝑖≥0 де 𝜔 – вектор ваг; 𝜌 – поріг відсікання; 𝜉𝑖 – штрафи за порушення межі; 24 𝜐∈(0,1] – параметр, який контролює частку очікуваних аномалій та впливає на жорсткість моделі. Точки, що лежать поза побудованою областю, класифікуються як аномальні. Практичні особливості: - переваги: сувора математична основа; можливість викори стання нелінійних ядер (наприклад, RBF) для подання складних залежностей; Хороша продуктивність на вибірках середнього розміру ; - недоліки: висока чутливість до вибору параметрів 𝜐 та ядра; погана масштабованість при великих обсягах транзакц ій; потрібна нор малізація ознак; - у банківських програмах: використовується для профілювання поведінки клієнтів або виявлення рідкісних закономірностей в обмежених наборах даних, наприклад, при скринінгу VIP -клієнтів. 2.1.3 Autoencoder Autoencoder – це штучна нейронна мережа, що навчається відтворювати вхідні дані через проміжний стислий шар. Ідея полягає в тому, що, навчаючись на нормальних транзакціях, мережа навчається ефективно кодувати лише характерні паттерни нормальної поведінки. Аномалії, що не відповідають цим патернам, реконструюються менш ефективно [9]. Автоенкодер складається з двох частин: - кодувальник g (𝑥), який зменшує розмірність даних: 𝑧=g(𝑥)=𝜎(𝑊1𝑥+𝑏1), (2.4) - декодувальник 𝑓(𝑧), який відновлює вхід: 25 𝜇=𝑓(𝑧)=𝜎(𝑊2𝑧+𝑏2). (2.5) Функція втрат визначається як середньоквадратична похибка: 𝐿=1 𝑛∑ ‖𝑥𝑖−𝑓(𝑔(𝑥𝑖))‖2 𝑛 𝑖=1 , (2.6) де 𝑛 – кількість навчальних прикладів (транзакцій); 𝑥𝑖 – вхідні дані для i -го прикладу. Для аномальних транзакцій ця по хибка суттєво вища, що дозволяє встановити поріг аномальності 𝜏. Практичні переваги та обмеження: - переваги: здатний виявляти складні, нелінійні та багатовимірні невідповідності; добре працює із широким спектром ознак; може гнучко комбінуватись з іншими ал горитмами (наприклад, Isolation Forest для подальшої обробки прихованого простору) ; - недоліки: вимагає великого обсягу навчальних даних; схильний до перенавчання; важко пояснити; потребує значних обчислювальних ресурсів ; - практичне застосування: використовує ться у великих банках у складі багатошарової архітектури, де автокодировщик спочатку генерує узагальнене уявлення транзакції, а класифікатор другою етапі визначає рівень ризику. 2.2 Порівняльний аналіз алгоритмів Після вивчення математичних основ різни х моделей виявлення аномалій доцільно провести порівняльний аналіз їх характеристик з метою оцінки придатності кожного методу застосування у фінансових інформаційних системах. Оскільки банківські дані мають специфічні характеристики – високу розмірність, с ильний дисбаланс класів, тимчасову динаміку та необхідність 26 швидкої обробки – вибір алгоритму виявлення має ґрунтуватися не лише на його теоретичній точності, а й на практичних аспектах: масштабованості, інтерпретованості та адаптивності до змін навколишнь ого середовища. У таблиці 2.1 подано порівняння основних сучасних методів виявлення аномалій, що використовуються у фінансових та банківських інформаційних системах. Аналіз включає основні характеристики кожного підходу, включаючи переваги, обмеження, тип навчання та типові сфери застосування. Таблиця 2.1 – Порівняння методів виявлення аномалій Метод Переваги Обмеження Тип навчання Типові застосування 1 2 3 4 5 Isolation Forest Не потребує мічених даних, добре масштабується, ефективний для великих потокі в транзакцій Чутливий до параметрів (кількість дерев, вибірки), інтерпретаці я обмежена Без нагляду Потокова фільтрація підозрілих транзакцій One-Class SVM Теоретично обґрунтований, працює з нелінійними ядрами Повільний на великих даних, складне налаштуван н я параметрів Напівконт рольовани й Поведінковий аналіз клієнтів, профілювання користувач ів 27 Продовження таблиці 2.1 1 2 3 4 5 Autoencod er Виявляє складні нелінійні відхилення, можливість гібридних моделей Високі вимоги до ресурсів, складність інтерпрет ації Без нагляду Виявлення складних фінансових аномалій, fraud detection LOF Добре працює у багатовимірних просторах Не масштабуєть ся на великі дані Без нагляду Аналіз невеликих сегментів клієнтів Generative Adversarial Networks Може моделювати рідкісні шаблони аномалій Складність навчання, нестабільніст ь генератора Напівконт рольовани й Синтетзоване створення або підсилення рідкісних даних Як видно з таблиці, жоден окремий"}
{"chunk_id": "2025_M_IUS_Novitsky_DO.pdf:6", "source": "2025_M_IUS_Novitsky_DO.pdf", "text": "ів 27 Продовження таблиці 2.1 1 2 3 4 5 Autoencod er Виявляє складні нелінійні відхилення, можливість гібридних моделей Високі вимоги до ресурсів, складність інтерпрет ації Без нагляду Виявлення складних фінансових аномалій, fraud detection LOF Добре працює у багатовимірних просторах Не масштабуєть ся на великі дані Без нагляду Аналіз невеликих сегментів клієнтів Generative Adversarial Networks Може моделювати рідкісні шаблони аномалій Складність навчання, нестабільніст ь генератора Напівконт рольовани й Синтетзоване створення або підсилення рідкісних даних Як видно з таблиці, жоден окремий метод не є універсальним для всіх типів банківських завдань. Алгоритми на основі дерев рішень, такі як Isolation Forest, відзначаються швидкістю та простотою інтеграції, але мають обмежену інтерпретованість. One -Class SVM забезпечує строгі теоретичні гарантії, проте є менш ефективним на потокових або великих даних. Autoencoder і GAN де монструють найкращі результати в ситуаціях, коли закономірності дуже складні або нелінійні, але потребують потужних обчислювальних ресурсів і ретельного налаштування. У сучасних банківських інформаційних системах часто застосовується 28 гібридний підхід, що п оєднує декілька методів: - autoencoder для побудови узагальненого представлення транзакцій; - Isolation Forest для виявлення відхилень у зменшеному латентному просторі; - SVM або LOF як додатковий рівень перевірки. Такий комбінований підхід підвищує точність, ск орочує кількість помилкових спрацьовувань та забезпечує адаптованість до нових шахрайських схем. 2.3 Теоретична модель інтеграції системи виявлення аномалій Теоретично інтегрована система виявлення аномалій у банківській інформаційній системі є багатор івневою архітектурою, що інтегрує збір даних, аналітичну обробку, оцінку ризиків і реагування. Така система повинна працювати в режимі реального часу, забезпечуючи баланс між точністю, швидкістю та зрозумілістю рішень. Основні модулі системи: а) Модуль збо ру та попередньої обробки даних 1) Збір потокових транзакцій із різних джерел (банківські термінали, мобільний банкінг, POS -системи); 2) Фільтрація та очищення даних, усунення пропусків і аномальних записів ; 3) Нормалізація ознак, створення похідних характ еристик ; 4) Агрегація за клієнтами, картками або часовими інтервалами; б) Модуль аналітики 1) Застосування вибраних алгоритмів для обчислення ймовірності аномалії кожної транзакції; 2) Можлива комбінація кількох моделей у ансамблі (гібридний 29 підхід); в) М одуль оцінки ризику та пояснення результатів 1) Перетворення числових \"аномалійних скорів\" у категорії ризику (низький / середній / високий); 2) Використання explainable AI для пояснення рішень моделей; 3) Формування звітів для аналітиків служби безпеки з указанням причин виявлення; г) Модуль реагування 1) При перевищенні порогового рівня ризику транзакція позначається як підозріла; 2) Можливі сценарії реагування: автоматичне блокування транзакції, ручна перевірка аналітиком, надсилання сповіщення клієнту або ініціювання додаткової верифікації. Також є важливі аспекти при реалізації системи: - інтеграція з існуючими системами банку через REST API, брокери повідомлень або ETL -процеси ; - масштабування – використання контейнеризації та мікросервісної архітектури; - адаптивність – регулярне донавчання моделей на нових даних для боротьби з concept drift; - інтерпретованість – формування зрозумілих звітів і пояснень для користувачів і регуляторів ; - безпека та відповідність законодавству – дотримання норм GDPR, PSD2, ISO/IEC 27001. 30 3 СИНТЕЗОВАНИЙ МЕТОД ВИ ЯВЛЕННЯ АНОМАЛІЙ У ФІНАНСОВИХ ТРАНЗАКЦІ ЯХ 3.1 Загальна концепція синте зованого методу Сучасні банківські інформаційні системи працюють у складному, високодинамічному середовищі, де обсяги транзакцій зростають експоненційн о, а моделі поведінки користувачів постійно змінюються у зв'язку з розвитком фінтех -сектору. Поширення послуг мобільного банкінгу, збільшення частоти транзакцій на невеликі суми, повсюдне використання платежів за QR-кодами та дистанційними сервісами суттєв о ускладнюють процес здійснення транзакцій. У таких умовах аномальні операції можуть проявлятися не лише як різкі разові відхилення, але й як тонкі багатовимірні патерни, що відрізняються від нормальної поведінки тільки у сукупності множини ознак [10]. У науковій літературі виділяють три базові типи аномалій у фінансових даних: - behavioral anomalies – відхилення у поведінці конкретного клієнта порівняно з власним історичним профілем ; - structural anomalies – порушення внутрішньої структури взаємозв ’язків між ознаками транзакцій ; - contextual anomalies – нормальні операції у загальному випадку, але аномальні у певному контексті (час, локація, мережеве середовище, тип пристрою). Кожен із цих типів аномалій потребує різного аналітичного підходу, оскільки одна й та с ама транзакція може бути нормальною згідно зі статистичними критеріями, але незвичайною у поведінковому контексті чи нелогічною з погляду структурної моделі клієнта. Для подолання виявлених обмежень у роботі запропоновано синте зован ий метод Hybrid Anomaly Detection System for Banking Transactions 31 (HASBT ), який поєднує три категорії моделей, що працюють на різних рівнях абстракції: - статистичний рівень . Використовується як перший фільтр, що швидко виявляє грубі та глобальні відхилення. Він забезпечує легку інтерпретованість та мінімальні затрати ресурсів ; - алгоритмічний рівень машинного навчання . На цьому рівні застосовуються безнаглядові алгоритми на кшталт Isolation Forest або One - Class SVM. Вони здатні моделювати багатовимірні поведінкові профілі користувачі в та виявляти аномалії, непомітні для статистичних моделей ; - ;либинний рівень . Autoencoder моделює внутрішню структуру нормальних транзакцій через компактне латентне представлення. Значні відхилення реконструкційної похибки вказують на нетипові або шахрайсь кі операції, які інші рівні можуть не відловити. Нижче (рисунок 3.1) наведено діаграму систезованого підходу: Рисунок 3.1 – Концепція синте зованого підходу HASBT Спільне використання цих моделей забезпечує синергетичний ефект,"}
{"chunk_id": "2025_M_IUS_Novitsky_DO.pdf:7", "source": "2025_M_IUS_Novitsky_DO.pdf", "text": "безнаглядові алгоритми на кшталт Isolation Forest або One - Class SVM. Вони здатні моделювати багатовимірні поведінкові профілі користувачі в та виявляти аномалії, непомітні для статистичних моделей ; - ;либинний рівень . Autoencoder моделює внутрішню структуру нормальних транзакцій через компактне латентне представлення. Значні відхилення реконструкційної похибки вказують на нетипові або шахрайсь кі операції, які інші рівні можуть не відловити. Нижче (рисунок 3.1) наведено діаграму систезованого підходу: Рисунок 3.1 – Концепція синте зованого підходу HASBT Спільне використання цих моделей забезпечує синергетичний ефект, який підвищує точність виявлення аномалій, скорочує кількість хибних спрацьовувань та дозволяє системі адаптуватися до нових моделей злочинної діяльності. 32 3.2 Структура синте зованого методу HASBT Структура запропоновано го методу HASBT побудован ий на принципах модульності, ма сштабованості та відмовостійкості. Вона призначена для інтеграції у промислові банківські інформаційні системи, які функціонують у середовищі значних навантажень та різнорідних потоків транзакцій. Основна мета архітектури – забезпечити поетапну обробку дан их, синтез результатів кількох методів та формування єдиної оцінки ризику кожної транзакції в реальному часі. Система має багаторівневу організацію, де кожен модуль відповідає за певний аспект аналізу даних: від попередньої обробки до глибокої реконструкці ї та спільного злиття результатів. Такий підхід оптимізує обчислювальні витрати, забезпечує незалежність компонентів та полегшує оновлення методів без шкоди для цілісності системи. Модульна архітектура рекомендована провідними дослідниками фінансової кібер безпеки, включаючи, які наголошують на важливості комбінування різних механізмів виявлення для досягнення збалансованого рівня чутливості та специфічності [11]. У HASBT кожен модуль працює автономно, але взаємодіє через стандартизовані інтерфейси. Це дозво ляє масштабувати як окремі компоненти, так і всю систему в цілому, що є критично важливим для банківських платформ з піковими навантаженнями у тисячі транзакцій на секунду. Основними компонентами архітектури є такі модулі: - модуль потокового збору транзакці й. Забезпечує отримання даних транзакцій із різних джерел: мобільний банкінг , веб-банкінг , платіжні шлюзи (Visa/Mastercard) , банкомати , POS-термінали . Використовує технології потокової обробки, такі як Apache Kafka, Apache Flink, RabbitMQ, що дозволяє дося гати мінімальної затримки та забезпечувати рівномірний 33 розподіл навантажень. Модуль також відповідає за валідацію структури вхідних записів, контроль коректності полів та відмовостійкість при похибках передачі даних ; - модуль очищення та попередньої обробки даних . Даний модуль готує сировинні дані до подальшої аналітики. Основні функції: видалення дублікатів, фільтрація шумів та аномальних форматів, нормалізація валютних значень, переведення категоріальних змінних у числові, заповнення пропущених значень, нор малізація шкал значень. Попередня обробка є критично важливою, оскільки чистота даних прямо визначає коректність роботи Autoencoder та моделей «без нагляду». Якість вхідних даних є фундаментальним фактором у задачах anomaly detection [11]; - модуль інженерії ознак . Цей модуль формує множину ознак, які характеризують транзакцію з різних боків. Особливу увагу приділено: агрегованим часовим вікнам (1 хв, 10 хв, 1 год), поведінковим профілям клієнта, географічним закономірностям, транзакційним патернам, ризиковим характеристикам MCC -кодів. Модуль працює як окрема підсистема, що попередньо генерує фічі для ML та DL моделей. Це дозволяє досягати високої швидкодії та не перевантажувати статистичний блок зайвою інформацією; - статистични й модуль первинного детектування . На цьому етапі застосовуються Z-score , модифікований IQR, MAD , Hampel filter . Статистичний модуль працює у режимі фільтрації «першої лінії», відсікаючи транзакції зі значними числовими відхиленнями. Таким чином, лише части на даних передається на наступні рівні, що знижує навантаження на моделі машинного навчання та глибинні мережі ; - модуль машинного навчання . Цей модуль відповідає за поведінкове та структурне моделювання транзакцій через алгоритми без учителя. У розподіленом у середовищі модуль виконується окремо від статистичного блоку, щоб забезпечити стійкість до змін розподілів даних та можливість оновлення без зупинки роботи системи; 34 - глибинний модуль Autoencoder. Autoencoder виконує кодування транзакції у компактний латен тний простір, реконструкцію вхідного вектора, обчислення похибки відновлення, визначення, чи є відхилення суттєвим. Глибинна модель розміщується окремо, оскільки вимагає більше ресурсів та повинна мати можливість перевчитись у міру появи нових типів поведі нки користувачів або шахрайських патернів; - модуль синтезу рішень . Використовується зважений ансамбль. Центральний компонент системи, що об’єднує три значення: оцінку статистичного модуля, поведінкову оцінку ML -модуля, реконструкційну похибку Autoencoder; - модуль генерації алертів. Забезпечує генерацію сповіщень про високоризикові транзакції, автоматичне блокування платежів (за потреби), передачу повних логів для аналізу фрод -аналітиками, аудит рішень моделі. Для наочної демонстрації функціональної логіки зап ропоновано го методу HASBT доцільно уявити її архітектуру з допомогою структурної схеми. Вона відображає взаємозв'язки між основними модулями системи, послідовність обробки транзакційних даних та інтеграцію методів статистичного аналізу, машинного навчання та глибокого моделювання. Діаграма демонструє, як вхідні дані проходять крізь послідовні етапи очищення, побудови ознак, первинної фільтрації та багаторівневого аналізу, після чого результати комбінуються у модулі синтезу рішень. Такий формат представлення дозволяє візуально оцінити модульність системи, чітку ієрархію операцій та функціональні залежності між компонентами. Особливо важливою є здатність архітектури масштабувати окремі модулі незалежно один від одного та забезпечувати їхню відмовостійкість, що відповідає вимогам сучасних банківських інформаційних систем. На діаграмі також показано розмежування між високорівневими етапами обробки потоку транзакцій та спеціалізованими моделями виявлення аномалій, що дозволяє оцінити шляхи оптимізації, місця взаєм одії з іншими підсистемами банку та"}
{"chunk_id": "2025_M_IUS_Novitsky_DO.pdf:8", "source": "2025_M_IUS_Novitsky_DO.pdf", "text": "фільтрації та багаторівневого аналізу, після чого результати комбінуються у модулі синтезу рішень. Такий формат представлення дозволяє візуально оцінити модульність системи, чітку ієрархію операцій та функціональні залежності між компонентами. Особливо важливою є здатність архітектури масштабувати окремі модулі незалежно один від одного та забезпечувати їхню відмовостійкість, що відповідає вимогам сучасних банківських інформаційних систем. На діаграмі також показано розмежування між високорівневими етапами обробки потоку транзакцій та спеціалізованими моделями виявлення аномалій, що дозволяє оцінити шляхи оптимізації, місця взаєм одії з іншими підсистемами банку та можливості розширення системи новими підходами. 35 Таким чином, наведена нижче (рисунок 3.2) архітектурна схема є контекстною д іаграмаю HASBT та структурною основою для подальшого аналізу алгоритмів, реалізації прототипу й проведення експериментальних досліджень. Рисунок 3.2 – Контекстн а діаграма HASBT Схеми декомпозиції системи виявлення аномал ій HASBT на рисунках 3.3, 3.4, 3.5, 3.6, 3.7. 36 Рисунок 3.3 – Діаграма A0 виявлення аномалій у фінансових транзакціях Рисуно к 3.4 – Декомпозиція «Збір та маршрутизація транзакційних даних» 37 Рисунок 3.5 – Декомпозиція « Підготовка даних та інженерії ознак » Рисунок 3.6 – Декомпозиція « Навчання та робота моделей HASBT » 38 Рисунок 3.7 – Декомпозиція « Синтез рішень та формування алертів » 3.3 Інженерія ознак у синте зованому методі HASBT 3.3.1 Загальні принципи формування ознак Ефективність будь -якої системи виявлення аномалій суттєво залежить від інформативності ознак, що описують транзакції. У наукових дослідженнях неодноразо во зазначалося, що «якість фічей впливає на точність моделі більше, ніж вибір алгоритму» [11]. Це особливо актуально у банківському секторі, де транзакції короткі, лаконічні та часто не містять прямої інформації про шахрайство. У таких умовах інженерія озн ак дозволяє створювати структуровані профілі поведінки клієнтів та виявляти приховані закономірності, не видимі у вихідних даних. У рамках методу HASBT розроблено багаторівневий механізм розробки ознак, що поєднує базові транзакційні характеристики, поведі нкові 39 патерни, географічні та контекстуальні фактори, а також часові та ризикові індикатори . Модуль розробки ознак працює незалежно та надає єдиний набір ознак для статистичного модуля, механізмів машинного навчання та автоенкодера. Такий підхід забезпечує узгодженість вхідних даних на всіх етапах обчислень, підвищуючи стабільність системи та знижуючи ризик хибних спрацьовувань. Під час створення фічей для фінансових транзакцій враховуються такі принципи: - інформативність. Кожна ознака повинна містити інфор мацію, яка розрізняє нормальні та аномальні транзакції ; - стабільність у часі. Ознаки повинні зберігати коректніс ть при зміні поведінки клієнтів ; - мінімум кореляційних дублювань. Надмірно корельовані фічі можуть погіршити роботу Autoencoder та призвести до пе реобучення моделей ; - робота у реальному часі. Обчислення повинні виконуватися з малою затримкою (<5 мс на транзакцію), щоб підтримувати роботу потокових систем ; - адаптивність до нових патернів. Фічі повинні мати здатність оновлюватися (online -updating), особ ливо поведінкові та агреговані ознаки. 3.3.2 Класи та характеристики ознак у HASBT Базові ознаки – це первинна інформація, яка надходить безпосередньо з транзакції і використовується для статистичного аналізу, нормалізації та подальшої роботи ML і DL моделей. До них належать: - сума транзакції; - тип операції (переказ, оплата, зняття готівки, депозит); 40 - канал виконання (POS, ATM, online, mobile); - валюта операції; - час доби та день тижня; - дані щодо отримувача, включаючи MCC -код і категорію сервісу. Ці ознаки ф ормують «скелет» транзакційного запису та дозволяють виявляти первинні, найбільш очевидні порушення, пов’язані з аномальними сумами, нетиповими операційними каналами або незвичайними категоріями витрат. Поведінкові ознаки є ключовими у виявленні аномалій, оскільки більшість шахрайських дій проявляються не у вигляді разових екстремальних значень, а у формі відхилень від звичної поведінки клієнта. Дослідження вказують, що реальні фрод -сценарії найчастіше є саме поведінковими [12]. У методі HASBT формуються та кі поведінкові характеристики: - середня сума операцій клієнта за обраний період; - стандартне відхилення сум операцій; - коефіцієнт асиметрії (skewness) розподілу витрат; - частота проведення платежів; - кількість транзакцій за останні 1, 5 і 15 хвилин; - частка нови х або нетипових отримувачів; - коефіцієнт різноманітності платежів (entropy of recipients); - відхилення поточного патерну від історичного профілю клієнта. Такі фічі дозволяють моделі розрізняти нормальну та потенційно підозрілу поведінку навіть у тих випадках , коли базові характеристики виглядають стандартними. У сучасних цифрових фінансових сервісах географія транзакцій стала важливим індикатором ризику. Аномалії можуть проявлятися у вигляді незвичного переміщення клієнта, зміни країни, різкого переходу між каналами або підозрілої активності з нового середовища. Географічні ознаки включають: - геолокацію транзакції; 41 - відстань між поточною та попередньою операцією; - швидкість зміни геолокацій (velocity); - країну або регіон оператора платежу; - належність до географіч ного кластера, характерного для клієнта. Контекстуальні ознаки охоплюють: - час доби (нічні операції мають підвищений ризик); - тип пристрою та його зміну; - зміну IP -адреси; - відповідність мережевого трафіку історичним патернам клієнта. Такі ознаки дозволяють ви являти складні контекстуальні аномалії, які базові моделі пропускають. Для аналізу динаміки транзакційного потоку системою використовуються часові вікна та агрегуючі функції, що дозволяють виявляти сплески активності або різкі зміни у поведінці клієнта. Основні часові ознаки: - середнє ковзне значення (rolling mean); - ковзне стандартне відхилення (rolling std); - кількість транзакцій у ковзному вікні"}
{"chunk_id": "2025_M_IUS_Novitsky_DO.pdf:9", "source": "2025_M_IUS_Novitsky_DO.pdf", "text": "час доби (нічні операції мають підвищений ризик); - тип пристрою та його зміну; - зміну IP -адреси; - відповідність мережевого трафіку історичним патернам клієнта. Такі ознаки дозволяють ви являти складні контекстуальні аномалії, які базові моделі пропускають. Для аналізу динаміки транзакційного потоку системою використовуються часові вікна та агрегуючі функції, що дозволяють виявляти сплески активності або різкі зміни у поведінці клієнта. Основні часові ознаки: - середнє ковзне значення (rolling mean); - ковзне стандартне відхилення (rolling std); - кількість транзакцій у ковзному вікні (rolling sum); - EWMA – експоненційно зважена ковзна середня; - медіанне вікно; - кількість операцій, що перевищують вс тановлений поріг. Ці характеристики формують часовий контекст і підвищують здатність моделі виявляти короткотривалі, але критично важливі аномальні «сплески». Ризикові ознаки використовуються для оцінки транзакцій за непрямими індикаторами, які не пов’язан і безпосередньо з поведінкою клієнта, але вказують на підвищену ймовірність шахрайства. До них належать: - MCC -коди високого ризику; - чорні та сірі списки контрагентів; - наявність історичних фрод -інцидентів клієнта; 42 - різкі зміни у типах витрат або пріоритетах к атегорій платежів. Ці ознаки відіграють важливу роль у модулі синтезу рішень, оскільки надають додаткові сигнали про підвищений ризик операції. У таблиці 3.1 подано основні категорії ознак, приклади їхніх представників, короткий опис інформаційного змісту та вказано модулі, у яких ці ознаки застосовуються. Така класифікація дозволяє чітко зрозуміти, які саме характеристики транзакцій формують основу аналізу, та як кожний тип ознак впливає на роботу статистичних, машинних та глибинних моделей у складі синте зованого методу HASBT. Таблиця 3.1 – Порівняння методів виявлення аномалій Категорія ознак Приклад ознак Короткий опис Модуль використання Базові Amount Сума транзакції Statistical + ML Поведінкові ClientEntropy Ентропія поведінки ML + AE Географічні Distance Зміна локації Statistical + ML Часові Rolling std Варіативність у вікні ML Ризикові MCC_Risk Ризиковість категорії Ensemble 3.4 Алгоритмічні компоненти синте зованого методу HASBT Алгоритмічне ядро методу HASBT складається з трьох незалежних, але взаємодоповнюючих модулів: статистичного модуля первинної детекції, модуля машинного навчання та глибинного модуля. Комбінування цих компонентів забезпечує багаторівневу оцінку аномальності транзакцій і 43 дозволяє компенсувати слабкі місця кожного окремо го алгоритму. У літературі зазначається, що шахрайські операції є неоднорідними за своєю природою, тому однорівневі алгоритми мають обмежену ефективність [10], [13]. HASBT застосовує: - статистичний аналіз. Визначає грубі відхилення; - алгоритми ML. Моделюють поведінкові патерни; - Autoencoder . Фіксує нелінійні структурні порушення. Такий комбінований підхід створює стійку систему, яка здатна працювати з високовимірними, шумними і нерівномірними даними банківських транзакцій. 3.4.1 Статистичний модуль первинної детекції Статистичний рівень є першим і найшвидшим етапом обробки даних у HASBT. Його основна функція – фільтрація очевидних глобальних аномалій та зменшення навантаження на ML і DL -модулі. Використані методи : - Z-score це один із найпоширеніших статистич них методів виявлення аномалій. Він дозволяє оцінити, наскільки значення ознаки 𝑥 відхиляється від середнього значення 𝜇 у одиницях стандартного відхилення 𝜎. Переваги цьго методу в тому, що він підходить для виявлення значних відхилень у сумах, а також в простоті та швидкодії. Формула має вигляд: Ζ=𝑥−𝜇 𝜎, (3.1) - Interquartile Range (IQR). Підходить для розподілів із викидами. Виявлення аномалій за межами: 44 𝑥<𝑄1−1.5𝐼𝑄𝑅 , 𝑥>𝑄3+1.5𝐼𝑄𝑅, (3.2) - Median Absolute Deviation (MAD ). Стій кий до шумів і спотворених значень. M𝐴𝐷=median( |𝑥−median( 𝑥)|), (3.3) - Hampel Filter. Виявляє короткочасні сплески в потоках даних. 3.4.2 Модуль машинного навчання Другий рівень – це поведінкове та структурне моделювання клієнтів. HASBT застосову є алгоритми без учителя: Isolation Forest або One-Class SVM . Обидва алгоритми вважаються золотим стандартом у unsupervised anomaly detection [14], [15] . У синте зованому методі HASBT алгоритм Isolation Forest використовується як фундаментальний інструмент д ля виявлення поведінкових та структурних аномалій у транзакціях. Його застосування виправдане, оскільки дані про транзакції, як правило, багатовимірні, нерівномірно розподілені та містять невелику частку аномальних спостережень. Isolation Forest добре прац ює в таких умовах, оскільки алгоритму не потрібні розмічені дані і він здатний ефективно виявляти нетипові точки завдяки спеціальному механізму побудови випадкового дерева. У рамках методу HASBT Isolation Forest виконує такі функції: - побудова поведінкового профілю клієнтів на основі агрегованих та часових ознак; 45 - оцінка локальних аномалій, які не мають великого глобального відхилення; - швидке реагування на зміни у патернах витрат клієнтів; - фільтрація транзакцій, що потрапляють до «зони підвищеного ризику» нав іть за відсутності великих сум або підозрілих MCC -кодів. Алгоритм добре інтегрується у потокове середовище, оскільки дозволяє виконувати обчислення з низькою затримкою. У методі HASBT моделі Isolation Forest регулярно оновлюються ( retraining ) на нових дани х, що забезпечує адаптивність до змін поведінки користувачів. Переваги алгоритму, такі як стійкість до шуму та здатність працювати із сильно нерівномірними вибірками, роблять його ключовим компонентом поведінкового аналізу. One-Class SVM у методі HASBT використовується як другий поведінковий алгоритм, орієнтований на побудову границі нормальності у багатовимірному просторі ознак. На відміну від Isolation Forest, цей метод формує узагальнену гіперповерхню, яка описує «типовий» стан клієнтської активності. За вдяки цьому One -Class SVM є ефективним для виявлення аномалій, що мають тонкий поведінковий характер і не завжди виділяються за окремими ознаками."}
{"chunk_id": "2025_M_IUS_Novitsky_DO.pdf:10", "source": "2025_M_IUS_Novitsky_DO.pdf", "text": "поведінки користувачів. Переваги алгоритму, такі як стійкість до шуму та здатність працювати із сильно нерівномірними вибірками, роблять його ключовим компонентом поведінкового аналізу. One-Class SVM у методі HASBT використовується як другий поведінковий алгоритм, орієнтований на побудову границі нормальності у багатовимірному просторі ознак. На відміну від Isolation Forest, цей метод формує узагальнену гіперповерхню, яка описує «типовий» стан клієнтської активності. За вдяки цьому One -Class SVM є ефективним для виявлення аномалій, що мають тонкий поведінковий характер і не завжди виділяються за окремими ознаками. У контексті HASBT One -Class SVM виконує такі функції: - формування індивідуальних «поведінкових оболонок» для к ожного клієнта; - виявлення відхилень, які виходять за межі типових патернів, але не є різкими (наприклад, поступове збільшення частоти невеликих операцій); - підсилення статистичних та структурних моделей шляхом додавання контекстної оцінки ризику; - підготовка додаткових сигналів для ансамблю Decision Fusion Layer. Використання параметра 𝑣 дає можливість контролювати частку транзакцій, які можуть бути класифіковані як потенційні аномалії. Це дозволяє адаптувати модель під різні категорії клієнтів та під різні рівні 46 ризику банку. У методі HASBT One -Class SVM працює у зв’язці з Isolation Forest: перший визначає межу нормальності, а другий – локальні аномалії. Разом вони забезпечують більш збалансоване виявлення складних порушень, ніж кожен із цих підходів окремо. 3.4.3 Глибинний модуль Autoencoder Глибинний модуль Autoencoder є третім, найбільш ресурсомістким та інформативним рівнем синте зованого методу HASBT . Його основне призначення – виявлення складних, багатовимірних і нелінійних аномалій, які неможливо ви явити статистичними або класичними алгоритмами машинного навчання. На відміну від попередніх модулів, Autoencoder не застосовує прості правила або лінійні межі нормальності, а намагається побудувати внутрішню модель структури нормальних транзакцій і виявит и відхилення від цієї структури [16]. Autoencoder виконує такі ключові функції: - створює компактне латентне представлення транзакції, зберігаючи основні залежності між ознаками; - відновлює транзакцію на виході, що дає можливість виміряти, наскільки модель “з розуміла” її структуру; - оцінює ступінь відхилення поточної транзакції від навченої норми. Оскільки модель навчається лише на нормальних транзакціях, будь -які відхилення у структурі або поведінці призводять до збільшення реконструкційної похибки. Саме тому Autoencoder є ефективним інструментом для виявлення складних або нових типів шахрайства, які ще не проявлялися у даних. Принцип роботи у методі HASBT : - вхідні дані . Попередньо оброблена та нормалізована транзакція з 47 понад 60 ознаками ; - енкодер стискає дані д о латентного вектора меншої розмірності (16 - 32 нейрони), що дозволяє виділити найважливіші особливості транзакції ; - декодер реконструює транзакцію із латентного простору ; - різниця між оригіналом та реконструкцією інтерпретується як міра відхилення ; - транзакці я вважається аномальною, якщо її похибка реконструкції перевищує встановлений поріг. У методі HASBT поріг визначається статистично – на основі розподілу похибок нормальних транзакцій. Це дозволяє адаптувати модель до реальних умов роботи банку, де структур а даних може змінюватися з часом. Порівняно з іншими моделями, Autoencoder забезпечує такі можливості: - виявлення складних взаємозв’язків між десятками ознак; - урахування нелінійних структурних залежностей; - стійкість до шуму та випадкових локальних відхилень ; - здатність адаптуватися до нових патернів поведінки клієнтів. У методі HASBT Autoencoder особливо корисний при роботі з транзакціями, які за зовнішніми параметрами не виглядають аномальними, але порушують внутрішню структуру поведінкової моделі клієнта. Глибинний модуль має і певні недоліки, які необхідно враховувати під час впровадження , а саме те, що в ін потребує більшої обчислювальної потужності, ніж інші моделі ; чутливий до вибору порогу аномальності, який має періодично оновлюватися; може переобучуват ися при неправильному формуванні навчальної вибірки. У методі HASBT ці недоліки компенсуються багаторівневим ансамблевим підходом – рішення Autoencoder не є остаточним, а є частиною загальної системи оцінювання. 48 3.5 Модуль синтезу рішень (Decision Fusion Layer) 3.5.1 Обґрунтування необхідності ансамблевого підходу в методі HASBT Модуль синтезу рішень є центральним компонентом методу HASBT , оскільки саме він забезпечує об’єднання результатів трьох різних класів алгоритмів – статистичних методів, моделей машинного навчання та глибинних нейронних мереж – у єдину інтегровану оцінку ризику транзакції. Об’єднання результатів є необхідним через те, що кожен з підходів сформований для виявлення різних типів аномалій та працює на різних рівнях абстракції [13]. Саме синтез дозволяє досягти того рівня точності, який недосяжний для будь -якого окремого алгоритму [17]. Статистичні методи підходять для виявлення різких, глобальних відмінностей у значеннях індивідуальних ознак. Вони ефективні у випадках, коли аномалія зн ачно відхиляється від типового розподілу даних (наприклад, раптова велика транзакція або незвичайна кількість транзакцій за короткий проміжок часу). Однак ці методи в основному не реагують на більш тонкі поведінкові або структурні аномалії. Алгоритми машин ного навчання, такі як Isolation Forest і One-Class SVM , здатні моделювати складні шаблони нормальної поведінки клієнтів. Вони враховують багатовимірність функцій, здатні виявляти локальні та контекстуальні аномалії та демонструють високу адаптивність при зміні даних. Однак ці методи також мають обмеження: вони чутливі до неоднорідності вибірки та можуть пропустити глибокі нелінійні зв’язки. Глибокі моделі ( Autoencoder ) дозволяють аналізувати приховані зв’язки між десятками функцій і визначати структурні ві дмінності, невидимі для класичних алгоритмів машинного навчання. Autoencoder моделює внутрішню структуру «нормальної»"}
{"chunk_id": "2025_M_IUS_Novitsky_DO.pdf:11", "source": "2025_M_IUS_Novitsky_DO.pdf", "text": "машин ного навчання, такі як Isolation Forest і One-Class SVM , здатні моделювати складні шаблони нормальної поведінки клієнтів. Вони враховують багатовимірність функцій, здатні виявляти локальні та контекстуальні аномалії та демонструють високу адаптивність при зміні даних. Однак ці методи також мають обмеження: вони чутливі до неоднорідності вибірки та можуть пропустити глибокі нелінійні зв’язки. Глибокі моделі ( Autoencoder ) дозволяють аналізувати приховані зв’язки між десятками функцій і визначати структурні ві дмінності, невидимі для класичних алгоритмів машинного навчання. Autoencoder моделює внутрішню структуру «нормальної» поведінки та виявляє аномалії на основі поганої реконструкції даних. Однак моделі цього класу вимагають більшої 49 обчислювальної потужності та ретельного встановлення порогів. Таким чином, кожен метод виявляє різні типи аномалій, але має недоліки, які компенсуються іншими модулями. Дослідження виявлення шахрайства підтверджують, що поєднання кількох моделей значно покращує точність, зменшує по милкові спрацьовування та робить систему більш стійкою до нових і рідкісних видів шахрайства. 3.5.2 Принципи синтезу рішень та методи вибору ваг ансамблю У методі HASBT об’єднання результатів статистичного модуля, моделей машинного навчання та Autoenco der здійснюється через спеціально розроблений модуль Decision Fusion Layer . Основна мета цього модуля — отримати одну інтегровану оцінку ризику транзакції на основі трьох незалежних джерел інформації. Такий підхід дозволяє підвищити стабільність класифікац ії та одночасно зменшити кількість хибних спрацювань. Формування інтегрованого скоринг -показника почина ється з того, що для кожної транзакції система отримує три внутрішні показники: - оцінку статистичного модуля; - оцінку моделей ML; - нормалізовану реконструкц ійну похибку Autoencoder. Усі значення приводяться до інтервалу [0, 1], після чого комбінуються у єдину оцінку. Такий підхід дозволяє порівнювати різнорідні алгоритмічні сигнали на спільній шкалі та забезпечує узгодженість системи. Комбінування оцінок зді йснюється за допомогою вагових коефіцієнтів, що визначають внесок кожного алгоритму в підсумкове рішення. Це дозволяє адаптувати модель до різних сценаріїв роботи: для одних банківських процесів більш релевантними є статистичні істотні сплески, для 50 інших — поведінкові відхилення чи глибинні структурні аномалії. 𝑆𝑐𝑜𝑟𝑒 =𝜔1𝑆𝑠𝑡𝑎𝑡+𝜔2𝑆𝑀𝐿+𝜔3𝑆𝐴𝐸, (3.4) за умов: 𝜔1+𝜔2+𝜔3=1, 𝜔𝑖≥0 Вибір ваг 𝜔1, 𝜔2, 𝜔3 є одним із найважливіших рішень у розробці синте зованого підходу. Існує декілька стратегій: - рівномірні ваги. Це базовий підхід, який використовується у випадках, коли відсутні позначені дані або система працює на ранніх етапах впровадження. Він забезпечує передбачувану поведінку та низьки й ризик помилки конфігурації ; - оптимізація ваг через ROC -аналіз. У цьому підході ваги вибираються так, щоб максимізувати метрику Area Under Curve (AUC) або забезпечити оптимальне співвідношення True Positive Rate (TPR) та False Positive Rate (FPR). Це дозво ляє тонко налаштувати систему під специфіку транзакційного потоку конкретного банку або каналу (онлайн -банкінг, POS, ATM тощо) ; - використання метамоделі (stacking). Цей метод дає можливість навчати ваги за допомогою легкої моделі поверх основних. Хоча для ц ього необхідні мічені дані, він забезпечує найвищий рівень адаптивності та точності. HASBT підтримує цей режим для банків, що мають достатньо позначених історичних фрод -даних. Після обчислення Score система порівнює його з динамічним порогом, який визначає ться статистично або через оптимізацію втрат. Це дозволяє враховувати непропорційність ризиків – помилка пропуску шахрайської операції у банківській сфері є набагато критичнішою, ніж хибне спрацювання. Тому пороги зазвичай зміщуються у бік більшої чутливос ті до аномалій. 51 𝐴𝑛𝑜𝑚𝑎𝑙𝑦 ={1,𝑆𝑐𝑜𝑟𝑒 ≥𝑇ℎ 0,𝑆𝑐𝑜𝑟𝑒 <𝑇ℎ, (3.5) де 𝑇ℎ – поріг. 3.6 Інтеграція синтезовано го методу HASBT у банківську інформаційну систему 3.6.1 Вимоги до інтеграції у промисловому середовищі Інтеграці я синте зованого методу HASBT у банківську інформаційну систему є критично важливим етапом розробки, оскільки її коректність визначає застосовність моделі у реальному середовищі обробки критично важливих фінансових даних. Банківські інформаційні системи характеризуються високими вимогами до продуктивності, доступності, інформаційної безпеки та безперервності бізнесу. Інтеграція аналітичних модулів має бути реалізована таким чином, щоб уникнути затримок транзакцій та негативного впливу на існуючі сервіси. Системи банківського обслуговування в режимі реального часу, особливо модулі онлайн -банкінгу, платіжні шлюзи та процесингові центри, працюють із мінімальною затримкою – від 20 до 200 мілісекунд залежно від каналу. Тому метод HASBT має бути інтегрована таким ч ином, щоб основна логіка працювала незалежно від системи транзакцій, а затримка контролювалася оптимізованими потоками даних. Інтеграційний рівень повинен відповідати низці критичних вимог: - мінімальна затримка . HASBT працює в паралельному потоці, не блокую чи транзакцію до моменту отримання результатів. Статистичний модуль працює у межах 1 –3 мілісекунд, ML і AE – у асинхронному режимі ; - масштабованість . З огляду на те, що великі банки обробляють 10^4 – 52 10^5 транзакцій на секунду, метод повин ен масштабуватися го ризонтально ; - відмовостійкість . Компоненти HASBT повинні продовжувати роботу навіть при падінні окремих вузлів , втраті мікросервісу , перевантаженні черг . Застосовується реплікація та резервний канал обробки ; - відповідність вимогам безпеки . Метод повин ен відповідати стандартам . 3.6.2 Методологія інтеграції синте зованої моделі Інтеграція здійснюється у вигляді окремої мікросервісної підсистеми, яка взаємодіє з транзакційним ядром банку через: - streaming -платформу (Kafka); - REST/gRPC API; - блок аналізу шахрайст ва (Fraud Monitoring); - систему логування (Elastic, Splunk). Загальн у схем у інтеграції зображено на рисунку 3. 8 53 Рисунок"}
{"chunk_id": "2025_M_IUS_Novitsky_DO.pdf:12", "source": "2025_M_IUS_Novitsky_DO.pdf", "text": "при падінні окремих вузлів , втраті мікросервісу , перевантаженні черг . Застосовується реплікація та резервний канал обробки ; - відповідність вимогам безпеки . Метод повин ен відповідати стандартам . 3.6.2 Методологія інтеграції синте зованої моделі Інтеграція здійснюється у вигляді окремої мікросервісної підсистеми, яка взаємодіє з транзакційним ядром банку через: - streaming -платформу (Kafka); - REST/gRPC API; - блок аналізу шахрайст ва (Fraud Monitoring); - систему логування (Elastic, Splunk). Загальн у схем у інтеграції зображено на рисунку 3. 8 53 Рисунок 3.8 – Схема інтеграції HASBT у банківську ІС Потоки поділяються на три основні категорії: - потоки реального часу . Передають транзакції у форматі JSON . Використовуються для статистичної оцінки , швидкої детекції «прямих» аномалій , запуску ансамблю ; - потоки навчання . Використовуються для оновлення ML -моделей , тренування Autoencoder , формування баз поведінкових профілів. Дані надходять у batc hed-режимі раз на 5 –15 хвилин ; - потоки логування та аналітики . Ці дані використовуються фрод - аналітиками для розслідування інцидентів. Інтеграція методу HASBT здійснюється поетапно: - встановлення потокового шлюзу . Kafka переда є транзакції у real - 54 time; - підклю чення preprocessing та feature engineering . Ці модулі реалізовані у вигляді ок ремих мікросервісів з autoscale; - оркестрація алгоритмів ML та DL . Чере з Kubernetes або Docker - кластер; - розгортання Decision Fusion Layer . Це ядро, яке інтегрується з AML та Fraud Monitoring ; - тестування у сірому контурі . Система працює паралельно з існуючою, не впливаючи на транзакції ; - перехід у production . Після досягнення стабільних метрик . 55 4 ПРОГРАМНА РЕАЛІЗАЦІЯ ТА ЕКСПЕРИМЕНТАЛЬНА ПЕРЕВІРКА СИНТЕЗОВАН ОГО МЕТОДУ HASBT 4.1 Загальна архітектура програмної реалізації Програмна реалізація синте зованого методу HASBT базується на принципах мікросервісної архітектури, асинхронної потокової обробки та горизонтального масштабування. Така структура дозволяє забезпечити високу п ропускну здатність, відмовостійкість, легкість оновлення окремих модулів та сумісність із сучасними банківськими ІТ -ландшафтами. Архітектура орієнтована на обробку транзакцій у режимі, наближеному до реального часу, та здатна працювати з навантаженнями у д есятки тисяч подій за секунду. Кожен модуль функціонує як незалежний мікросервіс, що взаємодіє через Kafka -топіки. Це забезпечує декомпозицію логіки, автономність оновлення моделей, а також можливість гнучкого розширення системи. Основними компонентами арх ітектури програмної реалізації є: - модуль збору транзакцій . Цей компонент відповідає за отримання транзакційних подій із різних джерел ; - модуль попередньої обробки та нормалізації . Модуль виконує видалення дублікатів та аномальних форматів , приведення валютн их значень до єдиної базової валюти , стандартну або мінмакс -нормалізацію числових полів . Якість попередньої обробки є фундаментальною умовою стабільності роботи Autoencoder та статистичних моделей ; - модуль інженерії ознак . Цей компонент формує розширений ве ктор ознак, що максимально описує поведінку транзакції в контексті історії клієнта. Формування ознак відбувається поза межами ML -модулів, що підвищує загальну швидкодію та гнучкість архітектури ; - статистичний модуль. Цей блок реалізовано як окремий Python - сервіс, що застосовує Z-score , MAD , Hampel Filter та модифікований IQR; 56 - модуль машинного навчання . Містить дві моделі, а саме Isolation Forest та One-Class SVM . Обидві реалізовані на основі бібліотеки scikit -learn та оптимізовані для обчислення в потоковому режимі ; - глибинний модуль Autoencoder . Побудований на TensorFlow/Keras, цей блок виконує складніші нелінійні обчислення ; - модуль синтезу рішень . Центральний компонент методу HASBT, який отримує три оцінки , нормалізує їх , об’єднує через зважену формулу Score та приймає рішення про рівень ризику транзакції . 4.2 Реалізація окремих модулів методу HASBT Модуль збору даних був реалізований у вигляді мікросервісу, що працює поверх Apache Kafka , яка забезпечує гарантовану доставку подій та мінімальні затримки. Знизу на рисунку 4.1 приклад структури транзакції, що надходить у систему: Рисунок 4.1 – Приклад структури транзакції Основн ими етапами р еалізаці ї модуля попередньої обробки та нормалізації є: - видалення дублікатів . Через контроль transaction_id та хеш - перевірку ; - виправлення відсутніх значень . Використано median -imputation та forward -fill для часових рядів ; 57 - нормалізація . StandardScaler для сум транзакцій , MinMaxScaler для поведінкових ознак . Статистичний модуль виконує швидку оцінку відхилень. У реалізац ії використано: - Z-score . Для виявлення глобальних числових аномалій ; - Median Absolute Deviation .Стійкий до вибросів метод для перевірки стабільності поведінки ; - Hampel Filter . Перевіряє незалежність поточної транзакції від локального часовового контексту. Таким чином формується нормалізована оцінка S_stat ∈ [0, 1], яка використовується у модулі синтезу рішень. Для моделі Isolation Forest обрано такі параметри: - n_estimators = 300 ; - max_samples = 0.2 ; - contamination = 'auto' ; - max_features = 1.0 . Обчислюється anom aly_score = 1 - average_path_length_normalized. Значення перетворюється у нормалізований Score ∈ [0,1]. При навчанн і One-Class SVM використано ядро RBF, яке найкраще відображає нелінійні розподіли: - kernel = 'rbf' ; - gamma = 'auto' ; - nu = 0.05 ; - обчислюється зн ачення decision function. Результат перетворюється у ймовірнісний ризик S_OCSVM ∈ [0,1]. Характеристики реалізації Autoencoder: -"}
{"chunk_id": "2025_M_IUS_Novitsky_DO.pdf:13", "source": "2025_M_IUS_Novitsky_DO.pdf", "text": "- n_estimators = 300 ; - max_samples = 0.2 ; - contamination = 'auto' ; - max_features = 1.0 . Обчислюється anom aly_score = 1 - average_path_length_normalized. Значення перетворюється у нормалізований Score ∈ [0,1]. При навчанн і One-Class SVM використано ядро RBF, яке найкраще відображає нелінійні розподіли: - kernel = 'rbf' ; - gamma = 'auto' ; - nu = 0.05 ; - обчислюється зн ачення decision function. Результат перетворюється у ймовірнісний ризик S_OCSVM ∈ [0,1]. Характеристики реалізації Autoencoder: - використано TensorFlow/Keras; - Optimizer: Adam (lr = 0.001) ; - Batch size: 1024 ; - Epochs: 50 –70; 58 - Early stopping при стабілізації los s; - Loss function: MSE (mean squared error) . Decision Fusion Service містить такі основні компоненти : - Score Normalization Layer – вирівнює шкали оцінок трьох модулів ; - Weighting Engine – застосовує систему ваг (soft -voting ensemble) ; - Decision Core – обчислює інтегрований скор S_final ; - Threshold Evaluator – порівнює результат з адаптивним порогом Tₕ ; - Risk Categorization Engine – присвоює категорію (Low , Medium , High) ; - Explainability Generator – генерує текстові пояснення рішення ; - Stability & Drift Monitor – слідкує за статистикою скорів. Кожен детектор передає результат у стандартизованому форматі JSON (лістинг 4.1). Лістинг 4.1 – Приклад вхідних даних Decision Fusion Service (JSON - повідомлення) { \"tx_id\": \"TX83920132\", \"s_stat\": 0.14, \"s_ml\": 0.62, \"s_ae\": 0.81, \"timestamp\": \"2025 -11-28T14:15:08Z\" } Всі значення представлені у нормалізованому інтервалі [0, 1]. У Normalization Layer перед обчисленням інтегрального скору модуль виконує: - стабілізацію аномальних високих сплесків ; - лог-нормування при ве ликій дисперсії ; - обрізання надмірно малих -великих оцінок (winsorization). Це усуває перекоси між різними видами детекторів. На основі методології розділу 3.5 обрано soft-voting ensemble із вагами : 59 - 𝜔1= 0.20; - 𝜔2= 0.35; - 𝜔3= 0.45; Це зумовлено тим , що Autoencoder найкраще виявляє складні та нечіткі аномалії. Таблиця 4.1 – Risk Categorization Engine Score Рівень ризику 0.00-0.40 Низький 0.40-0.70 Середній 0.70-1.00 Високий Приклад результату роботи модуля Explainability Generator, який формує пояснення для прийнятого рішення щодо транзакції, наведено у лістингу 4.2. Лістинг 4.2 – Приклад результату функції Explainability Generator (JSON -повідомлення) { \"tx_id\": \"TX83920132\", \"decision\": \"ANOMALY\", \"explain\": [ \"High AE reconstructio n error\", \"Behavior pattern deviates from ML baseline\", \"Moderate statistical deviation\" ] } 60 4.3 Методика експериментальної перевірки та результати оцінювання ефективності Для підтвердження працездатності та ефективності синте зованого методу HASBT було проведено комплексну експериментальну перевірку. Експерименти охоплювали аналіз продуктивності окремих модулів (статистичного, ML та Autoencoder), а також оцінку інтегрованої ансамблевої моделі. Для перевірки системи було використано транзак ційний набір даних обсягом: - 1 200 000 реальних банківських транзакцій ; - 0.15% мічених аномалій, що відповідає реальному співвідношенню у банківських системах (анормальні транзакції є дуже рідкісними) . Набір також включав: - шумні дані (помилки введення, неточ ності геолокації, розриви часових меток) ; - нерівномірні розподіли за сумами транзакцій, MCC -кодами та поведінковими патернами клієнтів ; - аномалії різних типів: структурні, поведінкові, контекстні та синте зовано згенеровані. Тестування проводилось у три етапи : - оцінка ефективності окремих модулів (статистичний модуль , Isolation Forest , One-Class SVM , Autoencoder ); - оцінка ансамблю HASBT ; - порівняння . Для об’єктивної оцінки використовува лись такі метрики: - Recall – частка успішно знайдених аномалій; - F1-score – збалансована метрика між Precision та Recall; - False Positive Rate (FPR) – частка хибних тривог ; 61 - ROC AUC – загальна якість класифікації . У таблиц і нижче наведено усереднені значення метрик, отримані на тестовій вибірці. Таблиця 4.2 – Ефективність моделей Мето д AUC F1 FPR Recall Stat 0.71 0.42 7.5% 46% Isolation Forest 0.83 0.61 4.2% 70% One-Class SVM 0.81 0.58 5.9% 68% Autoencoder 0.88 0.69 3.8% 75% HASBT 0.94 0.81 2.1% 88% Autoencoder показує найкращу загальну якість. Isolation Forest забезпечує ширши й спектр виявлення структурних аномалій. OCSVM добре моделює поведінкові патерни, але чутливий до розподілу даних. Stat має найгірші результати, але високу швидкість, тому потрібна у складі ансамблю. Переваги HASBT над окремими моделями: - Recall збільшено н а 13% у порівнянні з Autoencoder, що критично важливо у запобіганні фроду ; - False Positive Rate зменшено удвічі порівняно з IF та OCSVM ; - F1-score виріс на 20 -39% порівняно з класичними ML -моделями ; - AUC = 0.94 підтверджує здатність системи відділяти нормальн і транзакції від аномальних з високою точністю ; - Latency у real -time режимі – 3.4 мс, що відповідає вимогам банківських транзакційних систем. 62 На рисунках 4.4 –4.5 зображено граф іки порівняння різних моделей. Рисунок 4.4 – Точковий графік робочих точок мод елей (Recall –FPR) Рисунок 4.5 – Стовпчиковий графік порівняння F1 -метрик моделей Результати експериментів підтвердили ефективність запропоновано го синте зованого методу HASBT . HASBT продемонстрував значну перевагу над традиційними моделями аномалій, що підтверджує актуальність використання ансамблевого підходу для банківських транзакційних систем. 63 ВИСНОВКИ У магістерській кваліфікаційній роботі проведено комплексне дослідження моделей та методів виявлення аномалій у фінансових транзакціях"}
{"chunk_id": "2025_M_IUS_Novitsky_DO.pdf:14", "source": "2025_M_IUS_Novitsky_DO.pdf", "text": "банківських транзакційних систем. 62 На рисунках 4.4 –4.5 зображено граф іки порівняння різних моделей. Рисунок 4.4 – Точковий графік робочих точок мод елей (Recall –FPR) Рисунок 4.5 – Стовпчиковий графік порівняння F1 -метрик моделей Результати експериментів підтвердили ефективність запропоновано го синте зованого методу HASBT . HASBT продемонстрував значну перевагу над традиційними моделями аномалій, що підтверджує актуальність використання ансамблевого підходу для банківських транзакційних систем. 63 ВИСНОВКИ У магістерській кваліфікаційній роботі проведено комплексне дослідження моделей та методів виявлення аномалій у фінансових транзакціях банківських інформаційних систем та розроблено синте зован ий метод HASBT, як ий поєднує сильні сторони статистичних методів, алгоритмів машинного навчання та глибинного моделювання. Спочатку було п роаналізовано предметну область та сучасні виклики банківських інформацій них систем, зокрема зростання обсягів транзакцій, ускладнення шахрайських схем та необхідність роботи в режимі реального часу. Встановлено, що традиційні правила -орієнтовані методи вже не гарантують достатньої точності та стійкості виявлення порушень. Крім того, було д осліджено математичні основи статистичних моделей, методів машинного навчання та глибинних автоенкодерів. Наступним кроком було о бґрунтовано необхідність комбінованого підходу, оскільки жоден окремий метод не забезпечує достатньої універсально сті при виявленні різних типів аномалій. На цій основі сформовано концепцію синте зованого методу HASBT . Далі було р озроблено архітектуру методу HASBT, що включає модулі збору транзакцій, попередньої обробки, інженерії ознак, статистичного аналізу, ML -детек тора, глибинного автоенкодера, модулю синтезу рішень та генерації алертів. Останнім кроком п роведено експериментальну перевірку на транзакційному датасеті обсягом понад 1.2 млн записів. Експерименти включали окреме тестування статистичних методів, ML -моде лей, автоенкодера та ансамблево го методу HASBT, а також порівняння . За результатами дипломної роботи підготовлено до друку статтю «Anomaly detection in bank transactions via an ensemble method based on Isolation Forest, One -Class SVM and Autoencoder» автор ів Novitskiy D., Mikhnova О. 64 ПЕРЕЛІК ДЖЕРЕЛ ПОСИЛА ННЯ 1. Desai A., Kosse A., Sharples J. Finding a needle in a haystack: a machine learning framework for anomaly detection in payment systems. BIS Working Papers No 1188, 2024. URL: https://www.bis.org/publ/work1188.htm . (дата звернення: 25.11.2025 ) 2. Лавренюк В., Краснова І. Системно важливі банки та їх вплив на стабільність банківської системи . Київ: КНЕУ, 2016. 280 с. 3. Організаційна структура банку «Pumb ». URL : https://about.pumb.ua/management/structure (дата звернення: 30.11.2025). 4. Lopata A., Gudas S., Butleris R. et al. Financial Data Anomaly Discovery Using Behavioural Change Indicators. Electron ics, 2022. 14 p. URL: https://www.researchgate.net/publication/360659443_Financial_Data_Anomaly_D iscovery_Using_Behavioral _Change_Indicators . (дата звернення: 30.11.2025). 5. Gama J., Žliobaitė I., Bifet A., Pechenizkiy M., Bouchachia A. A survey on concept drift adaptation. ACM Computing Surveys. New York: ACM, 2014. Vol. 46, No. 4 . URL: https://www.researchgate.net/publication/261961254_A_Survey_on_Concept_Drif t_Adaptation . (дата звернення: 30.11.2025). 6. Hoang D. Wiegratz K . Machine Learning Methods in Finance: Recent Applica tions and Prospects , URL: https://www.researchgate.net/publication/366373139_Machine_Learning_Methods _in_Finance_Recent_ Applications_and_Prospects . (дата звернення: 01.12.2025). 7. Мар’ян К., Плескар Н., Рій А. Дослідження можливостей застосування методу Isolation Forest для виявлення аномалій у мережевому трафіку: монографія. Львів: НУ «Львівська політехніка», 2025. 173 с. 8. Arcolano, N., & others. One-Class Support Vector Machines: Methods and Applications. Semanticscholar , 2020 , 32 p. 9. Autoencoders (AE) . URL : 65 https://www.fabriziomusacchio.com/teaching/teaching_dimensionality_reduction_i n_neuroscience/10_autoencoders?utm_source . (дата звернення: 0 1.12.2025). 10. Chandola V., Banerjee A., Kumar V. Anomaly Detection: A Survey. ACM Computing Surveys. N ew York: ACM, 2009. Vol. 41, No. 3 . 11. Aggarwal C. Outlier Analysis. New York: Springer, 2017 , 465 p. 12. Bolton R., Hand D. Statistical Fraud Detection: A Review. Hoboken: Wiley, 2020. 163 p. 13. Goldstein M., Uchida S. Comparative evaluation of unsupervised anomaly detection . URL: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0152173 . (дата звернення: 02.12.2025). 14. Liu F. T., Ting K. M., Zhou Z. -H. Isolation Forest. In: Proceedings of the IEEE International Conference on Data Mining (ICDM). Pisa: IEEE, 2008 . URL: https://ieeexplore.ieee.org/document/4781136 . (дата звернення: 02.12.2025). 15. Schölkopf B. , One -Class SVM. JMLR. Cambridge: MIT Press, 2002, 153 p. 16. Erfani S., Rajasegarar S., Karunasekera S., Leckie C. High -dimensional and large -scale anomaly detection using a linear one -class SVM with deep learning. Pattern Recognition. Amsterdam: Elsevier, 20 16. Vol. 58. 17. Ahmed M., Mahmood A., Hu J. A survey of network anomaly detection techniques. Journal of Network and Computer Applications. Amsterdam: Elsevier, 2016. Vol. 60. URL : https://www.sciencedirect.com/science/article/abs/pii/S1084804515002891 (дата звернення: 02.12.2025). 18. ДСТУ 3008:2015 \"Звіти у сфері науки і техніки. Структура та правила оформлення\". Київ: Держстандарт України, 2017. 31 с 19. ДСТУ 8302:2015 «Бібліогр афічне посилання. Загальні положення та правила складання». Київ: Держстандарт України, 2017. 20 с. 20. Методичні вказівки щодо розробки та оформлення кваліфікаційної 66 роботи (для студентів усіх форм навчання другого (магістерського) рівня програми «Інформаційн і управляючі системи та технології») / Упоряд.: Петров К.Е., Левикін В.М., Чалий С.Ф., Євланов М.В., Саєнко В.І., Міхнов Д.К., Міхнова А.В., Чала О.В. Харків: ХНУРЕ, 2021. 30 с."}
{"chunk_id": "2025_M_IUS_Potapenko_AO.pdf:0", "source": "2025_M_IUS_Potapenko_AO.pdf", "text": "Міністерство освіти і науки України Харківський національний університет радіоелектроніки Факультет Комп’ютерних наук (повна назва) Кафедра Інформаційних управляючих систем (повна назва) КВАЛІФІК АЦІЙНА РОБОТА Пояснювальна записка рівень вищої освіти другий (магістерський) Дослідження та використання методів аналізу поведінки клієнтів при розро бці інформаційної системи для планування діяльності підприємства (тема) Виконав: здобувач 2 року навчання, групи ІУСТм -24-1 Потапенко Анна Олександрівна (прізвище, ім’я, по батькові) Спеціальність 122 К омп’ютерні науки (код і повна назва спеціальності) Тип програми освітньо -професійна (освітньо -професійна або освітньо -наукова) Освітня програма Інформаційні управляючі системи та технології (повна назва освітньої програми) Керівник : зав. каф. ІУС Костянтин ПЕТРОВ (посада, власне ім’я, прізвище ) Допускається до захисту Зав. кафедри ІУС Костянтин ПЕТРОВ (підпис) (власне ім’я, прізвище ) 2025 р. 2 Харківський національний університет радіоелектроніки ЗАТВЕРДЖУЮ: Зав. кафедри (підпис) “ 24 ” листопада 20 25 р. ЗАВДАННЯ НА КВАЛІФІК АЦІЙНУ РОБОТУ здобувачеві Потапенко Анні Олександрівні (прізвище, ім’я, по батькові) 1. Тема роботи Дослідження та використання методів аналізу поведінки клієнтів при розробці інформаційної системи для планування діяльності підприємства затверджена наказом по університету від “ 24 ” листопада 2025 р. № 1055Ст 2. Термін подання здобувачем роботи до екзаменаційної комісії “ 17 ” грудня 2025 р. 3. Вихідні дані до роботи наукові публікації та інтернет -джерела з тематики аналізу поведінки клієнтів, матеріали передатестаційної практики, інформація про методи прогнозування та оцінки CLV, транзакційні дані підприємства електронної торгівлі, додаткові джерела інформації. 4. Перелік питань, що потрібно опрацювати в роботі аналіз предметної області, аналіз існуючих методів аналізу поведінки клієнтів , дослідження проблем використання методів аналізу поведінки клієнтів в ІС, формулювання постановки задачі, дослідження методу когортного аналізу, CLV та survival -моделювання, розробка комбінованого методу аналізу поведінки клієнтів, експериментальна перевірка працездатності методу, аналіз отриманих результатів, формування висновків. Факультет Комп’ютерних наук Кафедра Інформаційних управляючих систем Рівень вищої освіти другий (магістерський) Спеціальність 122 К омп’ютерні науки (код і повна назва) Тип програми освітньо -професійна (освітньо -професійна або осві тньо-наукова) Освітня програма Інформаційні управляючі системи та технології (повна назва) 3 КАЛЕНДАРНИЙ ПЛАН № Назва етапів роботи Термін виконання етапів роботи Примітка 1 Аналіз предметної області 24.11.2025 – 26.11.2025 Виконано 2 Аналіз існуючих методів прогнозування та підходів до оцінки поведінки клієнтів 27.11.2025 – 29.11.2025 Виконано 3 Формулювання постановки задачі 30.11.2025 Виконано 4 Обґрунтування вибору методів та формування структури комбінованого методу 01.12.2025 – 03.12 .2025 Виконано 5 Розробка комбінованого методу аналізу поведінки клієнтів 04.12 .2025 – 06.12.2025 Виконано 6 Експериментальна перевірка працездатності запропонованого комбінованого методу аналізу поведінки клієнтів 07.12.2025 – 09.12.2025 Виконано 7 Аналіз отриманих результатів та інтерпретація поведінкових показників 10.12.2025 Виконано 8 Оформлення пояснювальної записки 11.12.2025 – 13.12.2025 Виконано 9 Оформлення графічного матеріалу 13.12.2025 Виконано 10 Захист кваліфікаційної роботи 17.12.2025 Виконано Дата видачі завдання 24 листопада 2025 р. Здобувач (підпис) Керівник роботи зав. каф. ІУС Петров К. Е. (підпис) (посада, прізвище, ініціали ) 4 РЕФЕРАТ Пояснювальна записка кваліфік аційної роботи : 86 с., 8 рис., 11 табл., 1 дод., 37 джерел. ЖИТТЄВА ЦІННІСТЬ КЛІЄНТА, КОГОРТНИЙ АНАЛІЗ, КОМБІНОВАНИЙ МЕТОД, МАРКЕТИНГОВІ СТРАТЕГІЇ, ПОВЕДІНКА КЛІЄНТА, ПРИЙНЯТТЯ УПРАВЛІНСЬКИХ РІШЕНЬ . Об’єктом дослідження кваліфікаційної роботи є про цес аналізу поведінки клієнтів у сфері електронної комерції . Предметом дослідження є методи аналізу поведінки клієнтів, що використовуються для підтримки процесів планування діяльності в інформаційній системі підприємства. Метою кваліфікаційної роботи є розроб ка комбінованого методу аналізу поведінки клієнтів на основі використання існуючих методів з подальшою реалізацією у вигляді аналітичного модуля інформаційної системи, що призначена для підтримки управлінських рішень щодо планування діяльності підприємства . Наукова новизна дослідження полягає у розробці комбінованого методу аналізу поведінки клієнтів у контексті підтримки процесів планування діяльності підприємства. На відміну від більшості існуючих підходів, що зосередж ені на окремих аспектах поведінки, пропонується поєднання методів когортного аналіз у, оцінювання життєвої цінності клієнта та прогнозування відтоку. Таке поєднання дозволить формалізувати поведінкові патерни клієнтів і адаптувати їх до цілей інформаційної системи для планування . 5 ABSTRACT Master’s thesis: 86 pages , 8 figures, 11 tables, 1 appendi x, 37 sources. COHORT ANALYSIS, COMBINED METHOD, CUSTOMER BEHAVIOR, CUSTOMER LIFETIME VALUE, DECISION -MAKING, MARKETING STRATEGIES . The object of research of the qualification work is the process of analyzing customer behavior in the field of e -commerce. The subject of the research is the methods of customer behavior analysis applied to support enterprise planning processes within an information system. The aim of this thesis is to develop a combin ed method for customer behavior analysis based on the integration of existing modern approaches, with its subsequent implementation as an analytical module of an information system designed to support managerial decision -making in enterprise planning. The scientific novelty of the research lies in the development of a combined method for analyzing customer behavior in the context of supporting enterprise planning processes. Unlike most existing approaches that focus on individual aspects of customer behavio r, the proposed approach integrates cohort analysis, customer lifetime value assessment, and churn prediction. This combination makes it possible to formalize customer behavioral patterns and align them with the objectives"}
{"chunk_id": "2025_M_IUS_Potapenko_AO.pdf:1", "source": "2025_M_IUS_Potapenko_AO.pdf", "text": "an information system designed to support managerial decision -making in enterprise planning. The scientific novelty of the research lies in the development of a combined method for analyzing customer behavior in the context of supporting enterprise planning processes. Unlike most existing approaches that focus on individual aspects of customer behavio r, the proposed approach integrates cohort analysis, customer lifetime value assessment, and churn prediction. This combination makes it possible to formalize customer behavioral patterns and align them with the objectives of the enterprise planning inform ation system . 6 ЗМІСТ Скорочення та умовні познаки ................................ ................................ ............ 8 Вступ ................................ ................................ ................................ ....................... 9 1 Аналіз предметної обла сті та постановка задачі дослідження .................... 11 1.1 Сучасний стан проблеми аналізу поведінки клієнтів при плануванні діяльності підприємст ва ................................ ................................ ................. 11 1.2 Аналіз існуючих методів аналізу поведінки клієнтів ........................... 13 1.2.1 Когортний аналіз ................................ ................................ ................ 14 1.2.2 Методи прогнозування відтоку клієнтів ................................ .......... 17 1.2.3 Методи, що використовують підпискові моделі ............................ 19 1.2.4 Метод розрахунку життєвої цінності клієнта ................................ . 21 1.2.5 Методи оцінювання ефективності маркетингових каналів ........... 22 1.2.6 Порівняння методів аналізу пов едінки клієнтів ............................. 24 1.3 Проблеми використання методів аналізу поведінки клієнтів при розробці інформаційної системи для планування діяльності підприємства ................................ ................................ ................................ .... 26 1.4 Постановка задачі дослідження ................................ ............................... 27 2 Дослідження методів аналізу поведінки клієнтів ................................ ......... 29 2.1 Вибір та обґрунтування груп методів ................................ ..................... 29 2.2 Класичні методи аналізу поведінки клієнтів ................................ ......... 31 2.3 Прогнозні методи ................................ ................................ ...................... 35 2.4 Порівняльний аналіз методів ................................ ................................ ... 37 3 Розробка комбінованого методу аналізу поведінки клієнтів ...................... 39 3.1 Вимоги до розроблюваного методу ................................ ........................ 39 3.2 Основні етапи методу ................................ ................................ ............... 40 3.3 Формальний опис етапів комбінованого методу ................................ ... 44 3.4 Інтеграція методу в ІС та очікувані результати ................................ ..... 56 7 4 Експериментальна перевірка працездатності розробленого комбінованого методу аналізу поведінки клієнтів ................................ ................................ .... 58 4.1 Приклад застосування розробленого комбінованого методу ............... 58 4.2 Аналіз отриманих результатів ................................ ................................ . 64 Висновки ................................ ................................ ................................ .............. 67 Перелік джерел посилання ................................ ................................ ................. 69 Додаток А Графічний матеріал кваліфікаційної роботи ................................ ..... 8 СКОРОЧЕННЯ ТА УМОВНІ ПОЗНАКИ ІС – інформаційна система CLC – Customer Lifecycle CLV – Customer Lifetime Value IDEF0 – Integration Definition for Function Modeling RFM – Recency , Frequency, M onetary 9 ВСТУП Зроста ння потреби підприємств в аналізі даних про поведінку клієнтів і використання його результатів для прийняття ефективних управлінських рішень у сфері планування діяльності є актуальною проблемою для сучасного бізнесу. В умовах цифрової трансформації та високої конкуренції на ринку аналіз поведінки клієнтів та їх вподоба нь стає ключовим чинником у підвищенні прибутковості та сталого розвитку бізнесу. Тема дослідження є актуальною через недостатній рівень інтеграції сучасних методів, наприклад, таких як когортний аналіз, прогнозування відтоку, підпискові моделі, моделі життєвої цінності клієнта ( customer lifetime value, CLV) та омніка нальні стратегії в інформаційні системи (ІС), які використовуються бізнесом для аналізу поведінки клієнтів. Існуючі дослідження, як правило, зосереджені на аналізі окремих аспектів поведінки клієнтів, тоді як нагальною потребою підприємства є створення єдиного аналітичного середовища для підтримки прийняття управлінських рішень. Об’єктом дослідження кваліфікаційної роботи є про цес аналізу поведінки клієнтів у сфері електронної комерції. Предмето м дослідження є методи аналізу поведінки клієнтів, що використовуються для підтримки процесів планування діяльності в ІС підприємства. Метою кваліфікаційної роботи є розробка комбінованого методу аналізу поведінки клієнтів на основі використання існуючих м етодів з подальшою реалізацією у вигляді аналітичного модуля ІС, що призначена для підтримки управлінських рішень щодо планування діяльності підприємства . Методика проведення дослідження передбачає використання методів системного аналізу для вивчення предм етної області т а постановки задачі дослідження , розроб ку комбінованого методу аналізу поведінки клієнтів на основі використання сучасних методів з подальшою реалізацією у вигляді 10 аналітичного модуля ІС; проведення серії комп’ютерних експериментів (експерим ентального дослідження) для визначення його працездатності п ри вирішенні практичних завдань, аналіз отриманих результатів, формулювання висновків і рекомендацій щодо практичного використання розробленого методу. Наукова новизна дослідження полягає у розробці комбінованого методу аналізу поведінки клієнтів у контексті підтримки процесів планування діяльності підприємства. На відміну від більшості існуючих підходів, що зосередж ені на окремих аспектах поведінки, запропоновано поєднання методів когортного аналіз у, оцінювання CLV та прогнозування відтоку. Таке поєднання дозволить формалізувати поведінкові патерни клієнтів. Практична цінність дослідження полягає у можливості використання отриманих результатів дл я створен ня аналітичного модуля ІС підприємства, що забезпечить: - прогнозування ймовірності відтоку клієнтів та формування рекомендацій щодо їх утримання; - виокремлення когорт та ціннісних сегментів клієнтів для підтримки прийняття управлінських рішень у сфері планув ання продажів; - уточнення прогнозу доходів у розрізі сегментів клієнтської бази на основі поєднання показників CLV та динаміки утримання клієнтів ; - інформаційну підтримку вибору варіантів планів діяльності підприємства (зокрема, планів продажів і заходів вза ємодії з клієнтами) з урахуванням виявлених поведінкових патернів . 11 1 АНАЛІЗ ПРЕДМЕТНОЇ ОБЛ АСТІ ТА ПОСТАНОВКА З АДАЧІ ДОСЛІДЖЕННЯ 1.1 Сучасний стан"}
{"chunk_id": "2025_M_IUS_Potapenko_AO.pdf:2", "source": "2025_M_IUS_Potapenko_AO.pdf", "text": "клієнтів та формування рекомендацій щодо їх утримання; - виокремлення когорт та ціннісних сегментів клієнтів для підтримки прийняття управлінських рішень у сфері планув ання продажів; - уточнення прогнозу доходів у розрізі сегментів клієнтської бази на основі поєднання показників CLV та динаміки утримання клієнтів ; - інформаційну підтримку вибору варіантів планів діяльності підприємства (зокрема, планів продажів і заходів вза ємодії з клієнтами) з урахуванням виявлених поведінкових патернів . 11 1 АНАЛІЗ ПРЕДМЕТНОЇ ОБЛ АСТІ ТА ПОСТАНОВКА З АДАЧІ ДОСЛІДЖЕННЯ 1.1 Сучасний стан проблеми аналізу поведінки клієнтів при плануванні діяльності підприємства В умовах цифрової економіки та глобальної конкуренції підприємства дедалі більше усвідомлюють важливість аналізу поведінки клієнтів як ключового елементу стратегічного управління. Якщо раніше цей напрям розглядався здебільшого як інструмент підтримки марке тингу, то сьогодні він трансформувався у фундаментальну частину системи планування діяльності. Сучасне стратегічне планування все б ільше орієнтується на концепцію клієнтоорієнтованості (customer -centricity ), де клієнт стає центральним елементом всіх бізнес -процесів. У дослідженні [1] підкреслюється, що інтеграція аналітики клієнтської поведінки в системи корпоративного планування дозволяє створювати більш точні фінансові моделі, основані на прогнозах майбутньої цінності клієнтської бази, а не лише на істори чних показниках. Це особливо важливо для компаній із підписковими моделями, де вартість бізнесу безпосередньо залежить від життєвої вартості клієнтів. Причини такої трансформації пов’язані з низкою чинників. По -перше, цифровізація бізнесу призвела до різк ого зростання обсягів клієнтських даних: транзакційних, поведінкових, соціальних. Кожна взаємодія клієнта з компанією (у вебсайті, мобільному додатку чи соціальних мережах) формує цифровий слід, який може бути використаний для аналітики. По -друге, сучасні моделі економіки, зокрема електронна комерція, підпискові сервіси, ґрунтуються на довгострокових відносинах із клієнтами, де головним завданням стає утримання, а не лише залучення. По -третє, підвищена конкуренція робить прогнозованість доходів і точність ф інансових планів неможливими без глибокого розуміння клієнтської поведінки . У результаті аналіз поведінки клієнтів набуває стратег ічного значення, 12 оскільки забезпечує: - прогнозування доходів , оскільки моделі розвитку клієнтської бази дозволяють будувати точ ні фінансові сценарії; - оптимізацію ресурсів , адже розподіл маркетингового бюджету без точних прогнозів часто є неефективним; - зростання прибутковості , завдяки своєчасному виявленню клієнтів із високим ризиком відтоку; - підтримку стратегічних рішень , що базую ться не лише на історичних даних, а й на прогнозних оцінках майбутньої цінності клієнтів. Сучасні наукові дослідження [2] підтверджують, що аналіз поведінки клієнтів стає наріжним каменем у процесах планування діяльності підприємств. Він формує основу для прийняття обґрунтованих управлінських рішень, прогнозування динаміки доходів та розробки маркетингових стратегій, орієнтованих на довгострокову взаємодію з клієнтами . Інтеграція мод уля аналізу поведінки клієнтів у системи планування створює комплексний ефе кт у кількох напрямах. По -перше, вона забезпечує безперервний цикл зворотного зв'язку між операційною діяльністю та стратегічними цілями. Дані про клієнтів у режимі реального часу дозволяють оперативно коригувати рішення та переглядати стратегічні орієнтир и. По - друге, використання єдиного джерела поведінкових даних сприяє розвитку крос -функціональної взаємодії: відділи маркетингу, продажів, обслуговування клієнтів і розробки продуктів працюють узгоджено, спираючись на спільну інформаційну базу. Це усуває тр адиційні розбіжності в інтерпретації ринкової ситуації. По -третє, аналітика дозволяє реалізувати принцип динамічного планування, коли фінансові прогнози, маркетингові бюджети та операційні показники постійно оновлюються на основі актуальних даних. Важливим є і багаторівневий ефект від використання поведінкової аналітики : - на операційному рівні: дозволяє опти мізувати щоденні бізнес -процеси від персоналізації маркетингових повідомлень до налаштування роботи кол - 13 центрів; - на тактичному рівні : забезпечує основу д ля розподілу ресурсів між різними каналами продажів, сегментами клієнтів та географічними ринками ; - на стратегічному рівні : стає критично важливим для прийняття рішень про вихід на нові ринки, запуск нових продуктів та формування довгострокових конкурентних переваг. Окремо слід відзначити, що розвиток клієнтської аналітики відбувається на тлі зростання вимог до етичності та прозорості використання даних. Питання захисту персональних даних та дотримання стандартів GDPR набувають стратегічного значення [3]. У багатьох випадках здатність компанії правильно працювати з клієнтською інформацією визначає її довіру на ринку. Разом із тим, попри зростальне усвідомлення важливості цього напряму, більшість компаній перебувають лише на етапі інтеграції сучасних підходів у свої ІС. Традиційні інструменти управління спираються переважно на агреговані історичні дані, що обмежує точність прогнозування та ускладнює прийн яття стратегічних рішень. Т ому актуальною є потреба у подальшому розвитку комбінованого методу аналізу повед інки к лієнтів та створенні аналітичного модуля , здатних вбудовуватися в ІС підприємств. Отже, сучасний стан проблеми можна схарактеризувати як перехідний: від розуміння важливості клієнтської аналітики до її практичного впровадження як основи планування діяльності. Цей перехід відкриває перспективи для наукових досліджень, спрямованих на інтеграцію різних методів аналізу та їх застосування у практиці електронної комерції та суміжних галузей . 1.2 Аналіз існуючих методів аналізу поведінки клієнтів Сучасна практика бізнес -аналітики пропонує широкий спектр методів 14 для дослідження поведінки клієнтів. Вони відрізняються підходами, математичним апаратом та рівнем деталізації результатів, проте всі мають спільну мету – підтримати підприємство у прийнятті управлінських рішень. Зокрема, ці методи спрямовані на в ирішення таких ключових завдань , як прогнозування доходів, виявлення ризиків відтоку, оптимізація маркетингових інвестицій та підвищення ефективності"}
{"chunk_id": "2025_M_IUS_Potapenko_AO.pdf:3", "source": "2025_M_IUS_Potapenko_AO.pdf", "text": "на інтеграцію різних методів аналізу та їх застосування у практиці електронної комерції та суміжних галузей . 1.2 Аналіз існуючих методів аналізу поведінки клієнтів Сучасна практика бізнес -аналітики пропонує широкий спектр методів 14 для дослідження поведінки клієнтів. Вони відрізняються підходами, математичним апаратом та рівнем деталізації результатів, проте всі мають спільну мету – підтримати підприємство у прийнятті управлінських рішень. Зокрема, ці методи спрямовані на в ирішення таких ключових завдань , як прогнозування доходів, виявлення ризиків відтоку, оптимізація маркетингових інвестицій та підвищення ефективності взаємодії з клієнтами. У науковій літературі та бі знес-практиці сформувалися кілька основни х напрямів, які стали базовими в сучасних системах підтримки прийняття рішень. До них належать когортний аналіз, моделі прогнозування відтоку, підпискові та транзакційні моделі, оцінка CLV , а також методи атрибуції маркетингових каналів на основі імовірнісних моделей . Популярність цих методів зумовлена їх широким використанням у бізнес -аналітиці та відносною доступністю реалізації в ІС підприємств. Вони стали поширеними завдяки простоті впровадження, наявності готови х інструментів та накопиченому практичному досвіду застосування. Крім того, ці підходи дозволяють аналізувати клієнтську поведінку у часовому розрізі та формувати показни ки, що можуть бути використані в процесах планування діяльності. Зараз вони становлять основу більшості корпоративних аналітичних рішень, хоча їх застосування не позбавлене суттєвих обмежень, які потребують подальшого дослідження. У подальших підрозділах розглянуто особливості кожного з цих методів, їхні переваги та обмеження, а також можли вості застосування в рамках комбінованого підходу . 1.2.1 Когортний аналіз Когортний аналіз [4] є одним із базових методів дослідження поведінки клієнтів, який дає змогу відстежувати зміни у динаміці з урахуванням часу та 15 спільних характеристик певних груп користувачів. Його сутність полягає у формуванні когорт – груп клієнтів, об’єднаних за спільною ознакою: часом першої покупки чи реєстрації, каналом залучення, реакцією на маркетингову акцію тощо. Д алі аналізується, як змінюється їхня активність протя гом вибраного періоду, що дозволяє виявляти закономірності утримання, відтоку та повторних транзакцій. На відміну від суто агрегованих показників, когортний аналіз дає змогу уникнути викривлення результатів через змішування різних груп клієнтів. Наприклад, загальний рівень продажів може здаватися стабільним, однак детальний розгляд показує, що нові когорти втрачають лояльніс ть швидше за попередні. Подібна аналітична інформація є важливою для планування, оскільки дає змогу вчасно виявляти проблеми та коригув ати маркетингові та продуктові стратегії. У практиці електронної комерції когортний аналіз особливо корисний для: - оцінки рівня утримання клієнтів, тобто частки користувачів, які залишаються активними упродовж певного періоду; - визначення масштабів відтоку і часу, коли клієнти найчастіше залишають сервіс; - дослідження повторних покупок та змін середнього чека залежно від когорти; - вимірювання ефективності рекламних кампаній або промоакцій, оскільки різні когорти можуть демонструвати різну чутливість до маркетин гових стимулів; - виявлення впливу зовнішніх факторів (сезонності, оновлення продуктів, конкуренції) на поведінку різних груп клієнтів [5]. Методика когортного аналізу зазвичай ґрунтується на побудові retention - таблиць і графіків, які відображають відсоток к лієнтів, що зберігають активність у часі. Приклад когортного аналізу подано у ви гляді retention -таблиці (рис. 1.1). 16 У ній відображено, як змінюється частка активних користувачів, що поверта ється у наступні тижні після першої взаємодії з сервісом [6]. Рисунок 1.1 – Приклад когортного аналізу У першому стовпці представлено когорти – групи клієнтів, які вперше приєдналися у певний тиждень. У стовпці Week 0 для всіх когорт прийнято частку утримання 100% користувачів, а далі відображається, який відсоток і з них залишався активним у наступні тижні. Наприклад, для першої когорти у Week 1 активними залишилося 24% клієнтів, у Week 2 – 18%, а у Week 4 – лише 8%. Середні значення внизу показують загальну тенденцію: вже за тиждень залишається близько 27% користува чів, а на п’ятому тижні – лише 10%. Таким чином, застосування когортного аналізу у стратегічному плануванні має низку переваг. По -перше, він дає змогу точніше прогнозувати майбутні доходи, оскільки враховує не лише факт покупки, а й динаміку взаємодії клієнта з компанією . По-друге, на основі результатів аналізу можна сегментувати клієнтську базу та розробляти цілеспрямовані заходи щодо 17 утримання. По -третє, він допомагає оцінювати, які канали залучення приводять клієнтів із найвищою довгостроковою цінніс тю. 1.2.2 Методи прогнозування відтоку клієнтів Прогнозування відтоку клієнтів [7] є одним із ключових завдань у сфері бізнес -аналітики та управління відносинами з клієнтами. Воно дозволяє своєчасно визначати тих користувачів, які з високою ймовірністю відмовляться від послуг компанії або перестануть здійснювати покупки. Актуальність цього напряму пояснюється тим, що збереження наявного клієнта обходиться значно дешевше, ніж залучення нового, а стійкість доходів підприємства значною мірою залежить від р івня утримання. Завдання прогнозування відтоку найчастіше формується як бінарна класифікація: клієнт або належить до групи «відтік», або до групи «утримання». При цьому важливим є не лише визначення ймовірності відтоку, але й інтерпретація факторів, що впливають на поведінку клієнтів. Це дозволяє компаніям не просто передбачати ризик, а й розробляти ефективні стратегії утримання. До класичних підходів прогнозування відтоку відносять: - логістичну регресію [8], яка дозволяє оцінювати вплив окремих характери стик на ймовірність відтоку та відрізняється простотою інтерпретації; - аналіз виживання (Survival Analysis ) [9], що використовується для прогнозування часу до відтоку"}
{"chunk_id": "2025_M_IUS_Potapenko_AO.pdf:4", "source": "2025_M_IUS_Potapenko_AO.pdf", "text": "класифікація: клієнт або належить до групи «відтік», або до групи «утримання». При цьому важливим є не лише визначення ймовірності відтоку, але й інтерпретація факторів, що впливають на поведінку клієнтів. Це дозволяє компаніям не просто передбачати ризик, а й розробляти ефективні стратегії утримання. До класичних підходів прогнозування відтоку відносять: - логістичну регресію [8], яка дозволяє оцінювати вплив окремих характери стик на ймовірність відтоку та відрізняється простотою інтерпретації; - аналіз виживання (Survival Analysis ) [9], що використовується для прогнозування часу до відтоку клієнта і є особливо корисною в підпискових моделях; - структурне моделювання рівнянь (struc tural equation modeling) [9], яке дає можливість аналізувати зв’язки між такими змінними , як, наприклад, задоволеність, лояльність та відтік . 18 Перевагою цих методів є прозорість та зрозумілість результатів, проте в умовах великих масивів даних і складних не лінійних залежностей їхня прогностична сила обмежена. Розвиток машинного навчання значно розширив можливості у сфері прогнозування відтоку. Серед найбільш поширених підходів є: - дерева рішень, random forest, gradient b oosting, XGBoost [10] – ансамблеві алгоритми, здатні враховувати складні залежності та взаємодії між ознаками; - метод опорних векторів [11] – ефективний для роботи з багатовимірними даними; - k-найближчих сусідів [12] – базується на пошуку схожих клієнтів, що дає зрозумілий механізм прогнозува ння; - штучні нейронні мережі [12] – застосовуються для роботи з великими та різнорідними наборами даних, виявл яючи приховані закономірності . Однією з головних проблем у цій сфері є дисбаланс класів: кількість клієнтів, які залишаються, значно перевищує кіль кість тих, що відмовляються від послуг. Це ускладнює навчання моделей, які часто «схиляються» до прогнозування класу більшості. Для подолання цього обмеження застосовуються різні методи балансування даних, які синтетично створюють приклади для класу «відті к». Дослідження у різних галузях [12] показують високу ефективність сучасних підходів до прогнозування відтоку. У телекомунікаційному секторі, де ця проблема вивчена найглибше, моделі на основі методу випадкових лісів із налаштованими параметрами демонстру ють точність понад 95% та високі показники відтворюваності результатів. Хоча приклади стосуються телеком - індустрії, подібні підходи успішно застосовуються і в електронній комерції. Тут ключовими факторами ризику відтоку є частота транзакцій, середній чек, час від останньої покупки, рівень взаєм одії з маркетинговими каналами та задоволеність сервісом . У контексті розробки інтегрованого аналітичного модуля ІС прогноз 19 відтоку відіграє центральну роль, оскільки результати такого аналізу безпосередньо використов уються для розрахунку CLV , оцінки ефективності маркетингових заходів та побудови фінансових прогнозів . 1.2.3 Метод и, що використовують підпискові моделі Використання підпискових моделей [13] є одним із найдинамічніших напрямів розвитку сучасних бізнес -стратегій. Вони дедалі активніше застосовуються не ли ше у сфері роботи з кінцевими споживачами, а й у сегменті взаємодії між підприємствами . Їх сутність поля гає у встановленні тривалих відносин між підприємством та клієнтом, де останній здійснює регулярні платежі за доступ до продукту чи послуги. Такий підхід принципово відрізняється від транзакційних моделей, орієнтованих на разовий продаж, оскільки акцент пе реноситься на довгострокову взаємодію та утримання споживача. Ключова особливіс ть підпискових моделей полягає в формуванні передбачуваного та стабільного грошового потоку. Завдяки цьому підприємства отримують можливість точніше прогнозува ти доходи, оптиміз увати запаси та планувати інвестиції. З іншого боку, клієнти виграють завдяки зручності, гнучкості та персоналізованим пропозиціям, що відповідають їхнім потребам [14]. Постійне оновлення та індивідуалізація пропозицій формують основу для утримання користу вачів і зниження рівня відтоку. З методологічного погляду підпискові моделі дають змогу реалізувати низку інструментів для аналізу та прогнозування поведінки клієнтів : - аналіз трив алості життєвого циклу клієнта (customer lifecycle , CLC ), який дозволяє оцінити ймовірність збереження підписки у часі; - прогнозування кількості транзакцій і частоти використання сервісу, 20 що є базою для подальшого розр ахунку CLV; - моделі поведінкової сегментації, які виділяють когорти клієнтів за рівнем лояльності, платоспро можн ості чи чутливості до ціни; - динамічне ціноутворення у форматі pay -per-use або багаторівневих тарифів («good –better –best»), що адаптуються до індивідуальних сценаріїв споживання [15]. Застосування підпискових моделей набуває великого значення у сфері електр онної комерції. Згідно з дослідженням [16], традиційні бізнес -моделі часто не здатні забезпечити гнучкість у мінливому цифровому середовищі, тоді як підпискові підходи дозволяють швидко реагувати на зміну попиту, надавати персоналізовані пропозиції та забе зпечувати конкурентні переваги. Крім того, вони інтегруються з сучасними цифровими інструментами опрацювання великих даних, штучного інтелекту та Інтернету речей, що підсилює можливості прогнозування та персоналізації. Важливим аспектом розвитку підпискови х моделей є їхня різноманітність. Підписки можуть орієнтуватися як на досягнення певного результату, так і різнитися за ступенем інтеграції ресурсів: від односпрямованих (де більшість процесів автоматизовано) до взаємних, що передбачають активну співпрацю між постачальником і клієнтом. Водночас підпискові моделі несуть і виклики, наприклад «втома від надмірної кількості підписок» , складність утримання клієнтів у довгостроковій перспективі та необхідність постійних інновацій у продукті та сервісі. Для подол ання цих бар’єрів дослідники [14] пропонують використовувати комбінацію інструментів: регулярне оновлення контенту чи функціоналу, застосування фріміум -моделі підписки для залучення нових клієнтів, а також розвиток спеціальних систем для супроводу клієнта на всіх етапах взаємодії. Отже , методи, що базуються на підпискових моделях, не лише дозволяють"}
{"chunk_id": "2025_M_IUS_Potapenko_AO.pdf:5", "source": "2025_M_IUS_Potapenko_AO.pdf", "text": "активну співпрацю між постачальником і клієнтом. Водночас підпискові моделі несуть і виклики, наприклад «втома від надмірної кількості підписок» , складність утримання клієнтів у довгостроковій перспективі та необхідність постійних інновацій у продукті та сервісі. Для подол ання цих бар’єрів дослідники [14] пропонують використовувати комбінацію інструментів: регулярне оновлення контенту чи функціоналу, застосування фріміум -моделі підписки для залучення нових клієнтів, а також розвиток спеціальних систем для супроводу клієнта на всіх етапах взаємодії. Отже , методи, що базуються на підпискових моделях, не лише дозволяють прогнозувати поведінку клієнтів, але й формують основу для довгостроков ого стратегічного планування. Вони інтегрують маркетингові, 21 фінансові та аналітичні інструменти, створюючи підґрунтя для розрахунку CLV та визначення ефективності бізнес -моделі загалом . 1.2.4 Метод розрахун ку життєвої цінності клієнта Метод CLV [17] є одним із ключових інструментів у сучасному маркетинговому та аналіти чному менеджменті. Його сутність полягає в оцінці сукупного доходу, який клієнт може принести підприємству за весь період співпраці. Такий підхід дозволяє компаніям не лише вимірювати екон омічну цінність клієнтської бази, а й приймати обґрунтовані рішення щодо сегментації, утримання та інвестування у розвиток відносин із клієнтами . У класичному вигляді CLV обчислюється за формулою: 𝐶𝐿𝑉 =𝐴𝑂𝑉 ∗𝐴𝐿𝑇 ∗𝐴𝐺𝑀 ∗𝑃, (1.1) де AOV – сума покупки, грн ; ALT – цикл життя покупця , міс. ; AGM – маржа , %. P – кількість покупок, здійснених клієнтом. Цикл життя покупця обчислюється за формулою: 𝐴𝐿𝑇 =1 𝐶ℎ, (1.2) де Ch – коефіцієнт відтоку клі єнтів, %. Коефіцієнт відтоку клієнтів визначається за формулою: 𝐶ℎ=(𝐶𝐵−𝐶𝐸) 𝐶𝐵∗100% , (1.3) 22 де СВ – кількість клієнтів на початок періоду , осіб ; СЕ – кількі сть клієнтів на кінець періоду , осіб . Наукові роботи [18, 19] присвячені аналізу та порівнянню результат ів традиційного підходу, що поєднує аналіз давності, частоти та обсяг витрат клієнта ( recency , frequency, m onetary , RFM) , та сучасних алгоритмів машинного навчання. Хоча RFM залишається простим і зручним методом сегментації, його прогностична точність є об меженою. Автори доводять, що застосування методів машинного нав чання дозволяє отримати більш надійні прогнози CLV завдяки врахуванню ширшого спектра змінних: частоти відвідувань, поведінки в онлайн -каналах, реакції на маркетингові стимули. Такий підхід доп омагає не лише оцінювати поточну цінність клієнта, але й формувати персоналізовані стратегії взаємодії. Сучасна література [20] демонструє, що розвиток методів CLV рухається у напрямку від статичних фінансових моделей до динамічних, поведінково -орієнтованих алгоритмів. Залучення інструментів штучного інтелекту дозволяє не лише підвищити точність прогнозів, але й забезпечити стратегічну цінність для бізнесу зокрема у плануванні доходів, оптимізації маркетингових бюджетів і розробці про грам лояльності . 1.2.5 Методи оцінювання ефективності маркетингових каналів Оцінювання ефективності маркетингових каналів [21] – це система методів, яка дає змогу кількісно визнач ати внесок окремих точок контакту з клієнтом (реклама, пошук, e -mail, соц іальні мережі тощо) у досягнення бізнес - цілей та приймати обґрунтовані рішення щодо розподілу бюджету. На базовому рівні використовують різні ключові показники ефективності, які пов’язують витрати, дохід і CLV клієнтської бази. До основних показників належ ать: рентабельність інвестицій, що показує, який дохід генерується на 23 одиницю вкладених коштів; окупність рекламних витрат, яка відображає дохід , отриманий завдяки цим витратам ; вартість залучення клієнта, тобто сукупні витрати на залучення одного нового к лієнта; CLV – очікуваний сумарний прибуток від клієнта за весь період співпраці. Практичне правило прийняття інвестиційних рішень ґрунтується на співвідношенні CLV та вартості його залучення: інвестиції в канал вважаються доцільними, коли CLV перевищує вар тість. Важливим аспектом для аналізу виступає омніканальність. Сучасний клієнт взаємодіє з брендом через низку онлайн та офлайн каналів, і завдання бізнесу полягає в інтеграції цих дотиків у єдину послідовність для забезпечення послідовного та зручного користувацького досвіду. Дослідження [22] підкреслює , що успіх омніканальної стратегії залежить від гармонізації даних, формування єдиного профілю клієнта та використання аналітики для персоналізації взаємодій . Водночас у багатоканальному середовищі маркетинг ові впливи мають як прямий, так і синергетичний ефект на прийняття рішення, що потребує алгоритмічних моделей атрибуції для оптимізації бюджету. У прикладних дослідженнях виділяють кілька о сновних груп методів атрибуції, к ожна з яких має свої переваги та о бмеження. Перша група – правилові моделі (rule -based) [23]. До них належать підходи, які розподіляють вагу конверсії між каналами за наперед визначеним правилом: перший клік, останній клік, рівномірний розподіл. Такі моделі прості у реалізації та зрозуміл і для бізнесу, проте вони не враховують порядок подій, взаємодію каналів і відмінність між конвертую чими та неконвертуючими шляхами . Далі – марковські моделі [23]. Цей підхід розглядає взаємодію користувача з каналами як послідовність станів у ланцюгу Маркова. Кожен канал виступає станом, а конверсія або відмова – поглинаючим станом. На основі історичних даних формується матриця переход ів, яка відображає ймовірності переходів між каналами. Внесок окремого каналу визначається за 24 підходом його виключення з моделі: канал видаляється , після чого повторно обчислюється ймовірність конверсії; різниця між базовим і модифікованим сценарієм інтерп ретується як внесок цього каналу. Наступна група – сучасні data -driven підходи [23] . До них відносять методи машинного навчання та причинно"}
{"chunk_id": "2025_M_IUS_Potapenko_AO.pdf:6", "source": "2025_M_IUS_Potapenko_AO.pdf", "text": "як послідовність станів у ланцюгу Маркова. Кожен канал виступає станом, а конверсія або відмова – поглинаючим станом. На основі історичних даних формується матриця переход ів, яка відображає ймовірності переходів між каналами. Внесок окремого каналу визначається за 24 підходом його виключення з моделі: канал видаляється , після чого повторно обчислюється ймовірність конверсії; різниця між базовим і модифікованим сценарієм інтерп ретується як внесок цього каналу. Наступна група – сучасні data -driven підходи [23] . До них відносять методи машинного навчання та причинно -наслідкові моделі, які враховують часові лаги, інтенсивність взаємодій та приховані фактори. Вони здатні будувати ск ладні нелінійні залежності, однак потребують великих обсягів якісних даних і значних обчислювальних ресурсів . 1.2.6 Порівняння методів аналізу поведінки клієнтів Методи, розглянуті у попередніх підрозділах, широко використовуються в бізнес -аналітиці та підтримці управлінських рішень. Проте їх окреме застосування має низку обмежень: вони орієнтовані лише на окремі аспекти поведінки клієнтів і не забезпечують повного охоплення завдань пла нування діяльності підприємства . Для систематизації переваг і недолі ків доцільно порівняти методи за ключовими критеріями (табл. 1.1 ). Порівняння показує, що кожен із методів є корисним у власній сфері застосування: когортний аналіз описує історичну динаміку, прогнозування відтоку дає змогу діяти превентивно, підпискові моделі ефективні для сервісів із регулярними транзакціями, CLV викори стовується для стратегічних рішень, а атрибуційні методи допомагають оптимізувати маркетингові інвестиції. Водночас жоден із підходів не забезпечує комплексного охоплення всієї клієнтської поведінки. Це обумов лює необхідність комбінування різних методів в єдиному аналітичному модулі ІС. 25 Таблиця 1.1 – Порівняння методів аналізу поведінки клієнтів Метод Цілі використання Тип даних Інтерпретованість Обмеження Когортний аналіз Виявлення динаміки утримання, аналіз відтоку та повторних транзакцій Транзакційні, часові Висока Не дає прогнозу, відображає лише ретроспективні тенденції Прогнозування відтоку Визначення клієнтів із високим ризиком відтоку , формування стратегій утримання Поведінкові, транзакційні Середня Вимоги до якості, складність у поясненні результатів Підписк ові моделі Прогноз тривалості CLC, кількості транзакцій, майбутніх доходів Дані підписок, історія користування Середня Не придатні для разових покупок, чутливі до відтоку CLV -моделі Розрахунок довгострокової цінності, стратегічне планування та сегментація Фінансові, транзакційні Висока або середня Базові моделі спрощені, сучасні потребують великих масивів даних Оцінка ефективності маркетингових каналів Атрибуція конверсій, визначення внеску каналів, оптимізація бюджету Дані клієнтських шляхів, омніканальн і Висока або середня Складність реалізації, залежність від якості даних, відсутність інтегрованого прогнозу CLV 26 1.3 Проблеми використання мето дів аналізу поведінки клієнтів при розробці інформаційної системи для планування діяльності підприємства Розробка ІС для планування діяльності підприємства передбачає широке застосування методів аналізу поведінки клієнтів. Такі методи допомагають краще зрозуміти клієнтські потреби, прогнозувати відтік, оцінювати CLV та ефективність маркетингових каналів. Про те на практиці їх упровадження в межах ІС пов’язане з низкою проблем. Однією з ключових складностей є інтеграція даних із різних джерел у межах єдиної ІС підприємства. У практиці електронної комерції та сервісних компаній дані про клієнтів надходять із плат форм аналітики, маркетингових сервісів, соціальних мереж та систем управління продажами. Вони мають різний формат і частоту оновлення, містять дублікати або неповні записи. Для аналізу поведінки клієнтів потрібен єдиний профіль, який об’єднує транзакційні, поведінкові та маркетингові характеристики. Проте створення такого інтегрованого профілю є складним завданням. Додатково проблеми виникають у зв’язку з омніканальністю, коли клієнт одночасно взаємодіє з компанією через декілька каналів (онлайн і офлайн), а система не завжди здатна правильно поєднати ці події. Відсутність якісної інтеграції та синхронізації призводить до викривлення результатів аналізу та обмежує можливості системи. Сучасні методи аналізу потребують значних обчислювальних ресурсів. Для підп риємства це означає витрати на серверну інфраструктуру, бази даних і програмні засоби. Зі зростанням клієнтської бази та кількості каналів взаємодії навантаження на систему зростає. Проблемою стає і масштабування ІС: необхідно забезпечува ти стабільну робот у алгоритмів у режимі реального часу, швидку обробку великих масивів даних і відсутність затримок у прийнятті рішень . Методи аналізу поведінки клієнтів зазвичай зосереджені на окремих 27 завданнях. Наприклад, когортний аналіз дає змогу досліджувати динаміку, але не прогнозує майбутні результати; мод елі відтоку допомагають зменши ти втрати клієнтів, але не дають інтегральної оцінки цінності; атрибуційні моделі показують ефективність каналів, але не прогнозують довгострокову поведінку. Це ускладнює інтеграцію різ них підходів в єдине аналітичне середовище ІС. Крім того, виникає питання балансу між точністю прогнозів і зрозумілістю результатів: складні моделі забезпечують високу точність, але їхні результати важко пояснити керівництву. Така методологічна неоднорідні сть знижує практичну цінність аналітики. Крім технічних і методологічних обмежень існують організаційні та етичні аспекти. Розробка і підтримка складних моделей аналізу поведінки клієнтів вимагає висококваліфікованих спеціалістів та значних фінансових витр ат, що може бути непосильним для малого чи середнього бізнесу. Етичні проблеми пов’язані з використанням персональних даних клієнтів: їх потрібно обробляти відповідно до вимог законодавства, що накладає додаткові обмеження на розробку ІС. Також актуальною є проблема прийняття рішень: складні результати аналітики не завжди зрозумілі менеджерам, які відповідають за планування, і це створює бар’єр у впровадженні таких систем. Тому жоден із оглянутих методів не забезпечує комплексного охоплення всіх аспектів по ведінки клієнтів, а ізольоване використання окремих моделей призводить до неповної картини. Це формує необхідність у створенн і"}
{"chunk_id": "2025_M_IUS_Potapenko_AO.pdf:7", "source": "2025_M_IUS_Potapenko_AO.pdf", "text": "малого чи середнього бізнесу. Етичні проблеми пов’язані з використанням персональних даних клієнтів: їх потрібно обробляти відповідно до вимог законодавства, що накладає додаткові обмеження на розробку ІС. Також актуальною є проблема прийняття рішень: складні результати аналітики не завжди зрозумілі менеджерам, які відповідають за планування, і це створює бар’єр у впровадженні таких систем. Тому жоден із оглянутих методів не забезпечує комплексного охоплення всіх аспектів по ведінки клієнтів, а ізольоване використання окремих моделей призводить до неповної картини. Це формує необхідність у створенн і інтегрованого підходу, який би об’єднав переваги різних методів і усунув слабкі сторони . 1.4 Постановка задачі дослідження Аналіз існуючих підходів показав, що застосування окремих методів дозволяє вирішувати лише часткові завдання аналізу поведінки клієнтів і не 28 формує цілісної картини. Для забезпечення компле ксного бачення клієнтської бази, підвищення обґрунтованості управлі нських рішень та для задоволення інформаційних потреб підприємств постає потреба у створенні інтегрованого підходу, який поєднує різні методи в межах єдиної ІС. Об’єктом дослідження є процес аналізу поведінки клієнтів у сфері електронної комерції. Предмето м дослідження є методи аналізу поведінки клієнтів, що використовуються для підтримки процесів планування діяльності в ІС підприємства. Метою дослідження є розробка комбінованого методу аналізу поведінки клієнтів на основі використання існуючих методів з по дальшою реалізацією у вигляді аналітичного модуля ІС, що призначена для підтримки управлінських рішень щодо планування діяльності підприємства . Для досягнення мети необхідно вирішити такі завдання: - дослідити існуючі методи аналізу поведінки клієнтів та виз начити можливості їх взаємодії; - розробити комбінований метод аналізу поведінки клієнтів на основі використання існуючих методів; - реалізувати запропонований комбінований метод у вигляді аналітичного модуля та інтегрувати його в ІС планування діяльності підприємства; - провести експериментальну перевірку працездатності розробленого методу. 29 2 ДОСЛІДЖЕННЯ МЕТОДІВ А НАЛІЗУ ПОВЕДІНКИ КЛІ ЄНТІВ 2.1 Вибір та обґ рунтування груп методів У сучасних дослідженнях аналізу поведінки клієнтів спостерігається активне застосування методів, заснованих на поєднанні апарату математичної статистики, методів машинного навчання та економ ічних моделей. Це зумовлено зростанням обсягів даних, доступних підприємствам для аналізу, а також потребою у більш т очному прогнозуванні та персоналізації бізнес - рішень. У роботі [24] відзначається, що алгоритми ШІ кардинально впливають як на діяльність компаній, так і на поведінку клієнтів . Автори підкреслюють, що машинне навчання може зменшувати інформаційну асиметрію , покращувати прогнозування та сприяти персоналізації пропозицій, але водночас породжує ризики упередженості та посилення ринкової концентрації. Це підкреслює, що для практичного застосування слід обирати методи, які не лише здатні працювати з великими дан ими, але й забезпечують баланс між ефективністю та зрозуміліс тю результатів. Окрема група досліджень присвячена застосуванню предиктивної аналітики. У статті [25] показано, що використання ШІ у сегментації та динамічному ціноутворенні дає можливість компаніям ефективніше планувати ресурси та формувати персоналізовані пропозиції для клієнтів. Автори наголошують, що традиційні методи сегментації поступаються за точністю та швидкістю сучасним алгоритмам, здатним працювати з великими масивами даних. Узага льнюючи огляд літератури, можна виділити кілька основних груп методів аналізу поведінки клієнтів : - класичні методи: когортний аналіз та базові моделі CLV, які відзначаються простотою та інтерпретованістю, дають змогу відстежувати динаміку утримання клієнтів та оцінювати їхню цінність; 30 - прогнозні методи: моделі прогнозування відтоку клієнтів та розширені алгоритми розрахунку CLV з використанням машинного навчання, які забезпечують більш точні та оперативні оцінки ; - методи персоналізації: підпискові моделі та ін струменти динамічного ціноутворення, які безпо середньо впливають на дохід і задоволеність клієнтів, але водночас потребують високого р івня цифрової зрілості компанії; - імовірнісні та мережеві методи: методи атрибуції каналів на основі Марковських ланцюгів, що дозволяють оцінити ефективність маркетингових інвестицій, але мають обмежене застосування для індивідуального аналізу клієнтів. Попри різноманіття підходів , не всі методи однаково придатні для практичної реалізації у межах дослідження. Методи, що викори стовують підпискові моделі , безперечно є актуальними для сучасної економіки, особливо у сфері електронної комерції. Підпискові моделі забезпечують стабільні грошові потоки, а алгоритми динамічного ціноутворення дають змогу максимізувати дохід шляхом адапта ції цін до попиту в реальному часі. Проте їхня універсальність обмежена: підписки підходять лише для бізнесів зі специфічною моделлю монетизації, а динамічне ціноутворення потребує великих обсягів даних і може призводити до зниження довіри клієнтів у разі відсутності прозорості. Тому ефект від застосування цих методів як основи ІС планування діяльності для широкого класу підприємств є сумнівним. Суттєвою перевагою методів оцінювання маркетингових каналів є можливість точніше визначати їхній вплив на залучен ня клієнтів, що допомагає оптимізувати рекламний бюджет. Проте цей напрям не дає комплексного уявлення про CLV та індивідуальну поведінку клієнтів. Крім того, методи атрибуції вимагають якісних даних про весь ланцюг взаємодій, які не завжди доступні підпри ємствам. Тому для розв’язання поставленої задачі вони також не є оптимальними . Отже, для подальшого розгляду було обрано дві ключові групи методів, 31 що відповідають вимогам дослідження та мають потенціал до інтеграції: - класичні методи : когортний аналіз та базовий показник CLV як прозорий «опорний шар» для моніторингу, сегментації та бізнес -інтерпретації; - прогнозні методи : методи прогнозування відтоку та розширені алгоритми обчислення CLV, які підвищують точність і дають змогу врахувати ризики втрати клієнті в. Поєднання"}
{"chunk_id": "2025_M_IUS_Potapenko_AO.pdf:8", "source": "2025_M_IUS_Potapenko_AO.pdf", "text": "не завжди доступні підпри ємствам. Тому для розв’язання поставленої задачі вони також не є оптимальними . Отже, для подальшого розгляду було обрано дві ключові групи методів, 31 що відповідають вимогам дослідження та мають потенціал до інтеграції: - класичні методи : когортний аналіз та базовий показник CLV як прозорий «опорний шар» для моніторингу, сегментації та бізнес -інтерпретації; - прогнозні методи : методи прогнозування відтоку та розширені алгоритми обчислення CLV, які підвищують точність і дають змогу врахувати ризики втрати клієнті в. Поєднання цих двох груп відкриває можливість створення нового підходу, який має переваги порівняно з існуючими методами завдяки інтеграції ретроспективного (когорти), стратегічного (CLV) та прогнозного (відтік) аналізу. Така комбінація буде детально роз глянута у наступних підрозділах як основа для розробки комбінованого методу . 2.2 Класичні методи аналізу поведінки клієнтів Когортний аналіз є одним із базових інструментів для вивчення поведінки клієнтів, який широко застосовується як у наукових дослідженнях, так і на практиці. Суть методу полягає в групуванні клієнтів у когорти за певною ознакою (наприклад, час залучення, канал комунікації чи соціально - демографічні характеристики) та подальшому відстеженні їхньої поведінки в динаміці. Це дає змог у оцінювати відмінності між групам и та робити висновки щодо результати вності маркетингових стратегій, рівня утримання чи потенційної цінності клієнтів. У прикладних дослідженнях когортний підхід довів свою ефективність для прогнозування CLV . У дослідженні [26] показано, що використання когортного підходу разом із класичними моделями CLV дозволяє значно зменшити похибку оцінки поза вибіркою порівняно з агрегованими моделями. Автори відзначили , що моделі, побудовані окремо для різних когорт, краще відображают ь реальні закономірності споживчої поведінки та забезпечують 32 більш точні прогнози . Додаткове підтвердження практичної цінності когортного аналізу наведено в дослідженні [27], яке присвячене сприйняттю цифрових платформ поколіннями X, Y та Z. Автори підкрес люють, що різні когортні групи відрізняються за своїми цінностями, рівнем цифрової грамотності, активністю у соціальних мережах і навіть ставленням до реклами. Наприклад, покоління X надає перевагу більш традиційним каналам комунікації та цінує інформативн ість, тоді як покоління Z є «мобільними нативами» та активно взаємодіє з мікро відео платформами та соціальними інфлюенсерами. Ця робота знов у доводить, що без когортного підходу компанії ризикують втратити релевантність своїх маркетингових стратегій. Практи чне значення когортного аналізу полягає в тому, що він надає підприємствам інструмент для оцінки результативності маркетингових заходів та прогнозування майбутніх результатів на основі минулих тенденцій. Аналіз різних когорт клієнтів дає змогу визначити, я кі канали залучення дають стабільніших користувачів із високим рівнем утримання, а які, навпаки, приносять клієнтів із коротк им життєвим циклом. Окрім цього , когортний аналіз часто застосовується як інтерпретаційний шар для складніших прогнозних моделей. У випадках, коли результати моделей машинного навчання є важкими для пояснення, побуд ова retention -кривих по когортах дає зрозумілий для менеджменту інструмент валідації. Це допомагає поєднувати прозорість класичних методів із точністю сучасних алгоритмів. Водночас когортний підхід має суттєві обмеження (табл. 2.1). Він є переважно описовим методом і не забезпечує повноцінного прогнозу майбутньої поведінки клієнтів. Його результативність значною мірою залежить від коректност і обраного критерію сегментації, адже неправильно сформовані когорти можуть призвести до викривлених висновків. Крім того , метод не враховує індивідуальних особливостей клієнтів, оскільки результати формуються для груп у цілому . 33 Таблиця 2.1 – Переваги та обмеження когортного аналізу Крит ерій Переваги Обмеження Простота застосування Легко реалізується, не потребує складних алгоритмів Дає обмежену кількість параметрів Інтерпретація Дає наочні retention -криві, зрозумілі для менеджерів Не відображає індивідуальних відмінностей Прогнозованість Дає змогу оцінювати тенденції утримання клієнтів у часі Не забезпечує точного прогнозу без інтеграції з іншими методами Практична цінність Використовується для оцінки каналів, поколінь, груп клієнтів Ризик викривлень у разі некоректної сегментації або низької якості даних Когортний аналіз є фундаментальним класичним методом, який забезпечує базове розуміння структури клієнтської бази та дає змогу пояснювати результати складніших моделей. Його головна цінність полягає у прозорості та простоті, однак обмеження цього підходу підкреслюють необхідність його інтеграції з іншими методами, такими як CLV та прогнозування відтоку . Класичні CLV -моделі активн о застосовуються і в практиці бізнесу. Вони дозволяють: - ідентифікувати найбільш прибуткових клієнтів; - прогнозувати обсяг доходів від клієнтської бази; - планувати маркетингові витрати та програми лояльності; - визначати доцільність залучення клієнтів через різні канали. Водночас ці методи мають і певні обмеження (табл. 2.2). По -перше, вони чутливі до зовнішніх змін середовища (структурні злами, як -от пандемія COVID -19 [26], можуть призводити до систематичних похибок у прогнозах). 34 По-друге, класичні моделі часто прац юють з усередненими показниками та не завжди здатні врахувати індивідуальні відмінності в поведінці клієнтів. По - третє, для отримання надійних результатів вони потребують якісних історичних даних, що не завжди є доступним . Таблиця 2.2 – Переваги та обмеження методів обчислення CLV Критерій Переваги Обмеження Простота застосування Доступні для реалізації, зрозумілі бізнес - користувачам Потребують якісних історичних даних Інтерпретація Дають чіткий показник CLV Усереднюють результати, не враховують індивідуальну поведінку Прогнозованість Дозволяють оцінювати довгострокові доходи клієнтів Чутливі до структурних змін середовища, що призводить до зниження точності в нестабільних умовах Практична цінність Використовуються для сегментації, бюджетування та планування"}
{"chunk_id": "2025_M_IUS_Potapenko_AO.pdf:9", "source": "2025_M_IUS_Potapenko_AO.pdf", "text": "- третє, для отримання надійних результатів вони потребують якісних історичних даних, що не завжди є доступним . Таблиця 2.2 – Переваги та обмеження методів обчислення CLV Критерій Переваги Обмеження Простота застосування Доступні для реалізації, зрозумілі бізнес - користувачам Потребують якісних історичних даних Інтерпретація Дають чіткий показник CLV Усереднюють результати, не враховують індивідуальну поведінку Прогнозованість Дозволяють оцінювати довгострокові доходи клієнтів Чутливі до структурних змін середовища, що призводить до зниження точності в нестабільних умовах Практична цінність Використовуються для сегментації, бюджетування та планування маркетингу Обмежена здатність адаптуватися до швидких змін у поведінці клієнтів Класичний CLV разом із когортним аналізом формують базовий рівень аналітики в дослідженнях, що стосуються поведінки клієнтів. Вони забезпечують інтерпретованість та стратегічну корисність, проте їхні прогностичні можливості залишаються обмеженими. Це підкреслює потребу в поєднанні з сучаснішими підходами, які будуть розглянуті в наступному підрозділі . 35 2.3 Прогнозні методи Відтік клієнтів є одним із найбільш критичних показників для будь - якого бізнесу, оскільки втрата користувачів безпосередньо впливає на дохід і стабільність компанії. Прогнозування відтоку розглядається як один із ключових напрямів сучасної аналітики поведінки клієнтів. На в ідміну від класичних методів, що здебільшого описують історичні дані, прогнозні підходи дозволяють передбачати ймовірність того, що клієнт залишить компанію, і своєчасно застосовувати заходи для його утримання . У роботі [28] увагу приділено не лише алгорит мічним аспектам прогнозування відтоку, а й практичним механізмам підвищення рівня утримання клієнтів. Автори підкреслюють, що успішність таких моделей визначається не тільки точністю прогнозів, а й тим, наскільки ефективно бізнес інтегрує їх результати у свою операційну діяльність. Важливим акцентом статті є роль мультиджерельних даних. Автори наголошують, що найбільш інформативними є моделі, які поєднують транзакційні дані з поведінковими показниками (частота візитів, взаємодія із застосунками) та даними ц ифрової взаємодії ( дані email -розсилок, push -повідомлень і активності в соціальних мережах ). Комплексність ознак дає змогу формувати не тільки більш точний прогноз, а й профіль клієнта, на основі якого можна будувати стратегічні рішення . Окремо відзначено питання етики та прозорості моделей. Автори звертають увагу, що алгоритми прогнозування відтоку (особливо «чорні скриньки» на кшталт штучних нейронних мереж) часто є складними для пояснення менеджерам. Це створює ризик, коли рішення впроваджуються без дост атнього розуміння причин, чому саме клієнта віднесено до «ризикової» групи. Дослідження підкреслює необхідність балансу між точністю та інтерпретованістю, що особливо важливо для невеликих компаній, які не мають великих аналітичних команд . 36 У роботі [29] ак цент зроблено на малих та середніх підприємствах, які зазвичай мають обмежені ресурси й стикаються з проблемою недостатності даних. Авторка доводить, що навіть в таких умовах можна досягати високої точності прогнозування відтоку, використовуючи поєднання л огістичної регресії з алгоритмами підкріплювального навчання для побудови персоналізованих кампаній. Отримані результати підтверджують, що прогнозні моделі дозволяють не лише виявляти клієнтів із високим ризиком відтоку, а й підвищувати їхню довгострокову цінність завдяки таргетованим заходам. Важливою частиною роботи стало використання аналізу виживання , який допоміг змоделювати тривалість CLC та скоригувати розрахунок CLV з урахуванням ризику відтоку. Це підкреслює тісний зв’язок між прогнозними моделями та оцінкою цінності клієнтів і демонструє перспективи інтегрованих підходів у бізнес -аналітиці (табл. 2.3). Таблиця 2.3 – Прогнозні методи аналізу поведінки клієнтів Метод Переваги Обмеження Логістична регресія Простота, інтерпретованість, швидкість Менша точність на складних і нелінійних наборах даних Random Forest, XGBoost Висока точність, стійкість до шуму, робота з великими даними Обмежена інтерпретованість, потреба у ресурсах Штучні нейронні мережі Добре працюють із часовими рядами та послідовн ими даними «Чорна скринька», потребують великих обсягів даних Аналіз виживання + CLV Дає оцінку тривалості CLC , інтегрується з CLV Потребує якісних даних і регулярного оновлення моделей Отже, прогнозування відтоку клієнтів виступає одним із ключових 37 напрямів у групі прогнозних методів, оскільки дозволяє компаніям не лише завчасно ідентифікувати ризикові сегменти, а й формувати цілеспрямовані стратегії утримання. Особлива цінність цього підходу полягає у можливості поєднання його результатів із класичн ими методами CLV, що забезпечує коригування оцінки довгострокової цінності клієнтів з урахуванням імовірності їхнього відтоку. Таким чином формується більш реалістична та практично значуща модель, яка враховує як потенційні доходи, так і ризики втрати кліє нтської бази . 2.4 Порівняльний аналіз методів Вище було розглянуто дві основні групи методів аналізу поведінки клієнтів – класичні та прогнозні. Кожна з них має власні переваги та обмеження, які визначають можливості їхнього застосування в ІС для планування діяльності підприємства. Класичні методи відзначаються простотою реалізації та високим рівнем інтерпретованості. Вони забезпечують зрозумілі для менеджменту результати, дають змогу оцінити дина міку утримання клієнтів і базовий показник CLV . Проте їхній основний недолік – обмежена прогнозов аність, що знижує точність оцінок у мінливому бізнес -середовищі. Прогнозні методи характеризуються вищою точністю та здатністю враховувати індивідуальну поведінку клієнтів. Вони дають змогу своєчасно виявляти р изикові сегменти та інтегрувати результати в розширені моделі CLV для отримання більш реалістичних прогнозів. Водночас ці метод и є складнішими у впровадженні , вимагають якісних даних, а також потребують додаткових інструментів для забезпечення інтерпретова ності. Узагальнені резул ьтати аналізу наведено в табл. 2.4. 38 Таблиця 2.4 – Порівняння класичних і прогнозних методів аналізу поведінки клієнтів"}
{"chunk_id": "2025_M_IUS_Potapenko_AO.pdf:10", "source": "2025_M_IUS_Potapenko_AO.pdf", "text": "що знижує точність оцінок у мінливому бізнес -середовищі. Прогнозні методи характеризуються вищою точністю та здатністю враховувати індивідуальну поведінку клієнтів. Вони дають змогу своєчасно виявляти р изикові сегменти та інтегрувати результати в розширені моделі CLV для отримання більш реалістичних прогнозів. Водночас ці метод и є складнішими у впровадженні , вимагають якісних даних, а також потребують додаткових інструментів для забезпечення інтерпретова ності. Узагальнені резул ьтати аналізу наведено в табл. 2.4. 38 Таблиця 2.4 – Порівняння класичних і прогнозних методів аналізу поведінки клієнтів Критерій Класичні методи Прогнозні методи Простота застосування Висока, легко реалізуються, зрозумілі для менеджерів Нижча, потребують додаткових інструментів і технічних знань Інтерпретація Наочні графіки та зрозумілі показники Обмежена у «чорних скриньках», вимагає пояснюва льних моделей Прогнозованість Низька, описують минулі тенденції Висока, дозволяють прогнозувати відтік і цінність клієнтів Вимоги до даних Середні (історичні транзакції, базова сегментація) Високі (повні транзакційні, поведінкові, цифрові дані) Практична цінність Сегментація, ретроспективний аналіз Утримання клієн тів, персоналізовані с тратегії Отже , класичні методи забезпечують прозорість і простоту інтерпретації, а прогнозні – точність і гнучкість. Водночас обидві групи не є взаємовиключними : їхнє поєднання дасть змогу сформувати більш повну картину поведінки клієнтів. Інтеграція когортного підход у, CLV та методу прогнозу вання відтоку створює основу для розробки комбінованого методу . 39 3 РОЗРОБКА КОМБІНОВАНОГО МЕТОД У АНАЛІЗУ ПОВЕДІНКИ КЛІЄНТІВ 3.1 Вимоги до розроблюваного методу Аналіз існуючих підходів до вивчення поведінки кліє нтів показав, що кожна з груп методів ма є власні сильні сторони, проте має й обмеження, які ускладнюють їхнє застосування в ІС планування діяльності підприємства. Класичні методи (когортний аналіз, базові моделі CLV) відзначаються простотою реалізації та в исоким рівнем інтерпретованості. Вони дозволяють оцінювати динаміку утримання клієнтів і розраховувати довгострокову цінність окремих сегментів. Проте, як показує дослідження [26], точність таких моделей знижується у випадках структурних змін попиту, а інш е дослідження [27] підкреслює, що когортний підхід здебільшого є описовим і не забезпечує належної прогноз ованості. Прогнозні методи, зокрема моделі відтоку на основі машин ного навчання, забезпечують високу точність і дозволяють завчасно ідентифікувати клієнтів із високим ризиком втрати. У роботі [28] показано, що використання багатоканальних даних дає змогу значно підвищити якість прогнозів. Проте автори наголошують на обмеженій інтерпретованості складних алгоритмів , що ускладнює їх у провадження в практичну діяльність компаній. Крім того, дослідники відзначають, що для малих і середніх підприємств характерна нестача якісних даних, через що навіть точні моделі потребують статистичних підходів, здатних працювати зі скороченими вибірками. До таких нал ежить аналіз виживання, який забезпечує можливість врахування часової динаміки та неповноти спостережень і, таким чином, розширює застосовність прогнозних моделей у практиці бізнесу . Отже , маємо ситуацію, коли класичні підходи є зрозумілими та зручними, а ле малопрогностичними, тоді як прогнозні – точні, проте складні та ресурсомісткі. Це створю є розрив між потребами бізнесу та можливостями 40 аналітичних інструментів. У зв’язку з цим виникає практична проблема: підприємствам потрібен метод, який забезпечує од ночасно інтерпретованість для управлінців і достатню точність для прийняття рішень у мінливих умовах . На підставі цього можна сформулювати вимоги до розроблюваного методу. Він повинен: - використовувати фактичні дані підприємства (транзакційні, поведінкові); - забезпечувати аналіз як на рівні груп клієнтів (когорт ), так і на рівні окремих записів; - надавати можливість отримувати результати у зрозумілому для бізнесу вигляді ( через retention -криві, середню тривалість взаємодії та прогнозовану цінність клієнта ); - бути відтворюваним на доступних наборах даних без необхідності використання складних інструментів машинного навчання; - мати можливість інтеграції в а налітичні модулі ІС підприємств а. Метою розробки є створення комбінованого методу, заснованого на когортному а налізі, що доповнюється оцінкою CLV та коригується шляхом прогнозу вання відтоку. Використання комбінації цих методів має забезпечити більш реалістичну оцінку клієнтської бази, підвищити точність прогнозування та водночас залишатися зрозумілою для користува чів ІС для планування діяльності підприємства . 3.2 Основні етапи методу Комбінований метод аналізу поведінки клієнтів поєднує можливості когортного аналізу, розрахунку CLV та прогнозування відтоку. Таке поєднання да сть змогу оцінити клієнтську базу не лише з погляду отриманих у минулому доходів, а й врахувати імовірності майбутньої активності клієнтів, 41 тобто зробити оцінку більш реалістичною. Мета розробки методу полягає у створенні інструменту, який дозволить підприємс тву краще планувати свою діяльність, прогнозувати доходи та розробляти дієві стратегії утримання клієнтів . Метод можна представити у вигляді такої послідовності етапів. 1. Усі клієнти групуються за певною ознакою (наприклад, місяць першої покупки) у когорти. Для кожної сформованої когорти будується retention - крива, яка показує, як змінюється кількість активних клієнтів у часі. Це да сть змогу виявити закономірності утримання к лієнтів і визначити характер поведінки різних груп. 2. Обчислюється базовий показник CLV. Він відображає очікуваний сумарний дохід від кожної когорти протягом обраного періоду прогнозування. 3. Виконується прогнозування ймовірності втрати клієнтів на основ і поведінкових характеристик. Отримана функція виживання відображає, з якою ймовірністю клієнти певної когорти залишатимуться активними протягом певного часу. 4. Базовий показник CLV коригується з урахуванням прогнозованої ймовірності відтоку, завдяки чому отримується ризик -скоригований показник CLV. Запропонований комбінований метод реалізується у вигляді модуля аналізу поведінки клієнтів ІС планування діяльності"}
{"chunk_id": "2025_M_IUS_Potapenko_AO.pdf:11", "source": "2025_M_IUS_Potapenko_AO.pdf", "text": "лієнтів і визначити характер поведінки різних груп. 2. Обчислюється базовий показник CLV. Він відображає очікуваний сумарний дохід від кожної когорти протягом обраного періоду прогнозування. 3. Виконується прогнозування ймовірності втрати клієнтів на основ і поведінкових характеристик. Отримана функція виживання відображає, з якою ймовірністю клієнти певної когорти залишатимуться активними протягом певного часу. 4. Базовий показник CLV коригується з урахуванням прогнозованої ймовірності відтоку, завдяки чому отримується ризик -скоригований показник CLV. Запропонований комбінований метод реалізується у вигляді модуля аналізу поведінки клієнтів ІС планування діяльності підприємства. Для опису логіки роботи модуля застос ован о методологію структурного моделювання Integration Definition for Function Modeling (IDEF0) [30]. Ця нотація використовується для формалізованого зображення функціональної структури системи, опису потоків даних, управлінських впливів і ресурсів, необхідних для виконання процесів . Основним елеме нтом методології IDEF0 є функціональний блок, який відображає окрему операцію або підпроцес. Коже н блок має чотири типи стрілок: - входи (input) – дані або об’єкти, що обробляються у процесі; 42 - керування (control) – правила, параметри, нормативи, які визначают ь, як виконується процес; - механізми (mechanism) – ресурси, засоби та виконавці, що забезпечують виконання процесу; - виходи (output) – результати роботи або продукти, які утворюються після виконання функції. Згідно з методологією IDEF0 , побудов а моделі відбу вається поетапно: - контекстна діаграма ( A0) – подає загальний огляд системи, її межі, основні вхідні та вихідні п отоки; - декомпозиція першо го рівня (A1) – деталізує головний процес, розкриваючи його на підпроцеси, між якими встановлюються логічні зв’язки. У межах роботи за допомогою методології IDEF0 розроблено контекстну діаграму модуля аналізу поведінки клієнтів ІС планування діяльності підприємства, який реалізує запропонований комбінований метод, та декомпозицію її першого рівня – рис. 3.1 –3.2. Рисунок 3.1 – Контекстна діаграма модуля аналізу поведінки клієнтів 43 Рисунок 3 .2 – Декомпозиція першого рівня 44 На рис. 3.2 наведено декомпозицію контекстної діаграми A0, що відображає основні етапи роботи аналітичног о модуля. Кожен із підпроцесів A1–A4 виконує окрему функцію: формування когорт клієнтів, розрахунок базового показника CLV, прогнозування ймовірності відтоку та формування ризи к-скоригованого показника CLV . Між підпроцесами передаються проміжні результати у вигляді когортних даних, поведінкових характеристик і фінансових показників. Спільні компоненти керування та механізми забезпечують функціонування всіх етапів процесу. Для реалізації комбінованого методу використовуються дані клієнтської бази підприємства, що надходять до аналітичного модуля ІС . До складу вхідних потоків належать: - історичні дані про клієнтів – ідентифікатори користувачів та інформація про дату першої покупки ; - транзакційні дані – дата та сума кожної покупки ; - дані клієнтської активності – поведінкові характеристики, сформовані на основі транзакційних записів (частота покупок, середній інтервал між транзакціями та інші ознаки взаємодії клієнта з підприємством) . Зазначені дані надходять з корпоративних джерел чи журналів подій, форм уючи єдину аналітичну вибірку, що використовується на подальших етапах. 3.3 Формальний опис етапів комбінованого методу Архітектура комбінованого методу являє собою, як зазначено вище, чітко визначену послідовність етапів (рис. 3.3 ), кожен з яких відіграє ключову роль у забезпеченні високої якості результатів . 45 Рисунок 3.3 – Основні етапи комбінованого методу аналізу поведінки клієнтів Розглянемо основні етапи запропонованого методу більш детально. На першому етапі комбінованого методу здійснюється формування когорт клієнтів – груп користувачів, об’єднаних за спільною ознакою. Найчастіше когорта визначається за місяцем першої покупки або реєстрації, що дає змогу відстежити динаміку активності клієнтів упродовж часу . Для кожного клієнта в изначається дата першої транзакції за формулою : 𝑓𝑖𝑟𝑠𝑡 _𝑡𝑟𝑎𝑛𝑠𝑎𝑐𝑡𝑖𝑜𝑛 𝑢=min (𝑑𝑎𝑡𝑒 _𝑡𝑟𝑎𝑛𝑠𝑎𝑐𝑡𝑖𝑜𝑛 𝑢), (3.1) де u – унікальний ідентифікатор клієнта (номер облікового запису або ID у базі даних клієнтів) ; date_transaction u – дата здійснення кожної транза кції клієнта ; first_transaction u – дата першої покупки клієнта . На практиці це вигл ядає таким чином. Я кщо клієнт здійснив покупки 12.01.202 5, 02.02.2025 та 15.03.2025, то first_transaction u = 12.01.202 5. Відпові дно, клієнт належить до когорти, що визначається місяцем першої транзакції, за формулою: 46 𝑐𝑜ℎ𝑜𝑟𝑡(𝑢)=month (𝑓𝑖𝑟𝑠𝑡 _𝑡𝑟𝑎𝑛𝑠𝑎𝑐𝑡𝑖𝑜𝑛 𝑢), (3.2) де cohort (u) – функція, що визначає належність клієнта до певної когорти; month( ) – функція, що повертає місяць з дати; first_transaction u – дата першої покупки клієнта . Отже , якщо перша покупка відбулась 12.01.2025, тоді cohort(u) = 01.2025 , тобто клієнт належить до когорти січня 2025 року. Для подальших розрахунків позначимо через c = cohort(u) когорту, до якої належить клієнт u. Для кожної когорти c і періоду t обчислюються наступні показники: - Nc – кількість клієнтів у когорті на момент її формування , тобто кількість користувачів, які вперше здійснили покупку або зареєструвалися в певному періоді ; - Ac(t) – кількість клієнтів когорти, які залишаються активними у періоді t; - Rc(t) – частка утриманих клієнтів , тобто коефіцієнт утримання, у когорті c на момент часу t. Показник R c(t) визначаєт ься за формулою: 𝑅𝑐(𝑡)=𝐴𝑐(𝑡) 𝑁𝑐, 𝑡= 0,1,2,…,𝑇, (3.3) де T – тривалість періоду спостереження , кількість місяців ; Nc – кількість клієнтів у когорті на момент її формування , осіб ; Ac(t) – кількість клієнтів когорти, які за лишаються активними у періоді t , осіб."}
{"chunk_id": "2025_M_IUS_Potapenko_AO.pdf:12", "source": "2025_M_IUS_Potapenko_AO.pdf", "text": "періоді ; - Ac(t) – кількість клієнтів когорти, які залишаються активними у періоді t; - Rc(t) – частка утриманих клієнтів , тобто коефіцієнт утримання, у когорті c на момент часу t. Показник R c(t) визначаєт ься за формулою: 𝑅𝑐(𝑡)=𝐴𝑐(𝑡) 𝑁𝑐, 𝑡= 0,1,2,…,𝑇, (3.3) де T – тривалість періоду спостереження , кількість місяців ; Nc – кількість клієнтів у когорті на момент її формування , осіб ; Ac(t) – кількість клієнтів когорти, які за лишаються активними у періоді t , осіб. Показник R c(t) відображає частку утриманих клієнтів (коефіцієнт утримання) у когорті в момент часу t. Він може задаватися у вигляді частки від 0 до 1 або у відсотках (від 0 % до 100 %). Значення 1 означає, що всі клієнти когорти залишаються активними, тоді як 0 свідчить про повну втрату клієнтів. 47 Після визначення когорт дані агрегуються у вигляді когортної матриці, у якій рядки відповідають когортам, а стовпці – часовим періодам (вік у когорти). Приклад когортної матриці наведено на рис. 3.4. Рисунок 3.4 – Когортна матриця Retention -крива – додатковий спосіб візуалізації когорт, який дає змогу оцінити, наскільки швидко відбувається відтік користувачів після першого контакту, а також порівняти різні когорти між с обою та виявити закономірності. Приклад графіку retention -кривих за даними трьох когорт з рис. 3.4 наведено на рис . 3.5. 48 Рисунок 3.5 – Графік retention -кривих д ля когорт «Січень 2025», «Лютий 2025» та «Б ерезень 2025» На другому етапі комбінованого методу обчислюється базовий показник CLV. Цей показник використовується для оцінювання очікуваного прибутку, який підприємство може отримати від клієнта або групи клієн тів за певний період взаємодії . У межах дослідження розрахунок проводиться на рівні когорт, сформованих за ознакою місяця першо ї покупки. Для кожної ко горти c і періоду t визначаються: - кількість клієнтів Ac(t), які залишаються активними в кожному часовому періоді t; - частка утриманих кліє нтів Rc(t), що розрахована за формулою (3.3) ; - середній дохід mc(t) від одного активного клієнта когорти c у період t, який обчислюється за формулою: 49 𝑚𝑐(𝑡)=𝐼𝑛𝑐𝑜𝑚𝑒 𝑐𝑎𝑐𝑡(𝑡) 𝐴𝑐(𝑡), (3.4) де Income cact(t) – сумарний дохід, отриманий від активних клієнтів когорти c у періоді t, грн; Ac(t) – кількість клієнтів когорти c, що залишаються активними у періоді t, осіб. Базовий показник CLV когорти c визначається за формулою: 𝐶𝐿𝑉 𝑐𝑏𝑎𝑠𝑒=∑𝑅𝑐(𝑡)∗𝑚𝑐(𝑡) (1+𝑑)𝑡𝑇 𝑡=1, (3.5) де T – горизонт прогнозування , кількість періодів ; Rc(t) – частка утриманих клієнтів у когорті c на момент часу t , безрозмірна величина ; mc(t) – середній дохід від одного активного клієнта когорти c у період t , грн/період ; d – ставка дисконтуван ня, що враховує зміну вартості грошей у часі, вимірюється у частках від одиниці або у відсотках за період . Для демонстрації розрахунків розглянемо когорту клієнтів, сформовану у січні 2025 року (рис. 3.4). Припустимо, що середній дохід від одного активного клієнта становить mc(t) = 100 грн/місяць, а ставка дисконту дорівнює d = 0,05 (5% на місяць). Показники утримання для перших п’яти місяців становлять Rc(t) = [0,86; 0,79; 0,73; 0,69; 0,65] . Тоді базовий показник CLV на одного клієнта згідно з формулою (3.5) : 𝐶𝐿𝑉січ𝑏𝑎𝑠𝑒=0,86∗100 (1+0,05)1+0,79∗100 (1+0,05)2+0,73∗100 (1+0,05)3+0,69∗100 (1+0,05)4+ +0,65∗100 (1+0,05)5=324 ,32. (3.6) 50 Отже, очікувана CLV одного клієнта цієї когорти становить близько 324 грн, а для всієї когорти з 1245 клієнтів – приблизно 403, 8 тис. грн . Базовий показник CLV відображає лише історичну та прогнозовану динаміку доходів , не враховуючи ризиків втрати клієнтів. На наступному етапі метод доповнюється компонентом прогнозування ймовірно сті відтоку, що дає змогу скор игувати базову оцінку . На третьому етапі здійснюється прогнозування ймовірності відтоку клієнтів. Для реалізації цього етапу комбінованого методу обрано метод аналіз у виживання , що ґрунтується на використанні моделі пропорційних ризиків Кокса [31]. Вибір цього методу зумовлений специфікою поставленої задачі, а саме необхідністю враховувати не лише факт відтоку клієнта, а й час, через який він відбувається. Більшість традиційних методів прогнозу вання, таких як логістична регресія чи дерева рішень, дозволяють визначити лише ймовірність того, що клієнт припинить взаємодію з компанією, не враховуючи часову динаміку цього процесу . Натомість аналіз виживання спеціально розроблений для задач оцінювання часу до настання події, де важливо моделювати не ли ше сам факт події, а тривалість до її виникнення . У контексті поведінки клієнтів це означає можливість описати , як змінюється ймовірність відтоку у часі. Важливою перевагою survival -аналізу є здатність пр ацювати з цензурованими спостереженнями, тобто з тими клієнтами, які на момент аналізу залишаються активними. У звичайних класифікаційних моделях такі випадки або відкидаються, або спотворюють результати, тоді як survival - підхід дає змогу коректно їх враху вати , зберігаючи повноту вибірки. Серед різних варіантів реалізації survival -аналізу обрано саме модель Кокса, яка поєднує обґрунтовану математичну основу з практичною інтерпретованістю. Вона дає змогу не лише оцінювати загальну функцію виживання клієнтів, а й визначати, як конкретні поведінкові фактори впливають на ризик відтоку . Функція виживання S c(t) описуватиме ймовірні сть того, що клієнт із когорти c залишатиметься активним до мом енту часу t."}
{"chunk_id": "2025_M_IUS_Potapenko_AO.pdf:13", "source": "2025_M_IUS_Potapenko_AO.pdf", "text": "або відкидаються, або спотворюють результати, тоді як survival - підхід дає змогу коректно їх враху вати , зберігаючи повноту вибірки. Серед різних варіантів реалізації survival -аналізу обрано саме модель Кокса, яка поєднує обґрунтовану математичну основу з практичною інтерпретованістю. Вона дає змогу не лише оцінювати загальну функцію виживання клієнтів, а й визначати, як конкретні поведінкові фактори впливають на ризик відтоку . Функція виживання S c(t) описуватиме ймовірні сть того, що клієнт із когорти c залишатиметься активним до мом енту часу t. Це значення надалі 51 використовується для коригування базового показника CLV , даючи змогу врахувати ризик відтоку та зробити оцінку більш реалістичн ою. Для оцінювання ймовірності відтоку використано математичну модель пропорційних ризиків Кокса, яка дозволяє врахувати вплив по ведінкових характеристик клієнтів на ризик припинення взаємодії з підприємством. Функція ризику для клієнта u має вигляд : ℎ𝑢(𝑡)=ℎ0(𝑡)∗𝑒𝛽1∗𝑥𝑢1+𝛽2∗𝑥𝑢2+...+𝛽𝑘∗𝑥𝑢𝑘, (3.7) де h u(t) – інтенсивність відтоку клієнта u у момент час у t, частка/період ; h0(t) – базова інтенсивність ризику відтоку , спільна для всієї вибірки клієнтів ; βi – коефіцієнти моделі , які показують вплив кожного поведінкового фактора на ризик відтоку , безрозмірна величина ; xui – поведінкові характеристики клієнта u (частота покупок, середній чек, тривалість взаємодії тощо), власні одиниці або нормовані в діапазоні [0; 1] ; k – кількість поведінкових факторів, включених у модель ; t – час або порядковий номер періоду спостереження (наприклад, місяць, тиждень, день після першої покупки). Значення коефіцієнта βi інтерпретується так:  якщо β i > 0, то фактор підвищує ризик відтоку;  якщо β i < 0, то фактор знижує ризик відтоку та свідчить про вищу лояльність клієнта. Функція виживання Su(t) показує, з якою ймовірністю клієнт залишиться активним до моменту часу t , і має вигляд: 𝑆𝑢(𝑡)=[𝑆0(𝑡)]𝑒𝛽1∗𝑥𝑢1+𝛽2∗𝑥𝑢2+...+𝛽𝑘∗𝑥𝑢𝑘, (3.8) де Su(t) – функція виживання для клієнта u, що відображає ймовірність залишатися активним до моменту часу t, безрозмірна величина (0 ≤ S u(t) ≤ 1 ); 52 S0(t) – базова функція виживання для «середнього» клієнта, яка оцінюється статистич но за допомогою методів Каплана -Мейєра або Бреслоу [32], безрозмірна величина ( 0 ≤ S0(t) ≤ 1); βi – коефіцієнти моделі, що показують вплив кожного поведінк ового фактора на ризик відтоку , безрозмірна величина ; xui – поведінкові характеристики клієнта u (частота покупок, середній чек, тривалість взаємодії тощо), власні одиниці або нормовані в діапазоні [0; 1] ; k – кількість поведінкових факт орів, включених у модель; t – час або порядковий номер періоду спостереження (наприклад, місяць, тиждень, день після першої покупки). Функція S u(t) є спадною в часі: чим більше значення t, тим менше значення ймовірності утримання клієнта. Це відображає природну тенденцію до зростання ризику відтоку з часом. Експоненційний вираз 𝑒𝛽1∗𝑥𝑢1+𝛽2∗𝑥𝑢2+...+𝛽𝑘∗𝑥𝑢𝑘 модифікує форму базової кривої S0(t) відповідно до індивідуальних характеристик клієнта:  якщо 𝑒𝛽∗𝑥𝑢 > 1, крива спадає швидше – клієнт має підвищений ризик відтоку;  якщо 𝑒𝛽∗𝑥𝑢 < 1, крива спадає повільніше – клієнт є більш лояльним . Розберемо п риклад обчислення індивідуальної ймовірності утримання клієнтів на основі моделі пропорційних ризиків Кокса – табл. 3.1. Таблиця 3.1 – Приклад обчислення індивідуальної ймовірності утримання клієнтів Клієнт x₁, частота покупок x₂, місяці неак - тивності β₁ β₂ eβ₁x₁+β₂x₂ S₀(3) Sᵤ(3) Ймовірність утримання 1 5 1 -0,4 0,6 0,247 0,7 0,916 91,6% 2 4 2 -0,4 0,6 0,670 0,7 0,787 78,73% 53 Продовження таблиці 3.1 Клієнт x₁, частота покупок x₂, місяці неак - тивності β₁ β₂ eβ₁x₁+β₂x₂ S₀(3) Sᵤ(3) Ймовірність утримання 3 4 1 -0,4 0,6 0,368 0,7 0,877 87,70% 4 3 3 -0,4 0,6 1,822 0,7 0,522 52,21% 5 4 5 -0,4 0,6 4,055 0,7 0,235 23,54% Для демонстрації використано два поведінкові фактори: х 1 – частота покупок, яка знижує ризик відтоку, та x 2 – тривалість неактивності, яка, навпаки, його підвищує. Коефіцієнти моделі прийнято рівними : β1 = −0,4 та β 2 = 0,6. Базова функція виживання для «середн ього» клієнта через три місяці становить S 0(3) = 0,7. Для кожного клієнта розраховано значення експоненти 𝑒𝛽1∗𝑥1+𝛽2∗𝑥2, яка визначає індивідуальний рівень ризику, а також відповідну функцію виживання S u(3). Як видно з результатів, наведених у табл. 3.1 , ймовірність утримання зменшується зі зниженням частоти покупок і зі збільшенням періоду неактивності. Наприклад, клієнт №1, який здійснює 5 покупок на місяць і був неактивним лише 1 місяць, має прогноз ймовірності утримання 91,6 %, а клієнт 5, який купує 4 рази, але був неактивним протягом 5 місяців, має значно нижчу ймовірність утримання – 23,54%. Для всієї когорти клієнтів функція виживання обчислюється як середнє значення : 𝑆𝑐(𝑡)=1 𝑁𝑐∑𝑆𝑢(𝑡) 𝑢∈𝑐, (3.9) де Sc(t) – середня функція вижива ння для всієї когорти клієнтів c у момент часу t, безрозмірна величина ( 0 ≤ S с(t) ≤ 1) ; 54 Nc – кількість клієнтів у когорті c , осіб ; Su(t) – індивідуальна функція виживання для кожного клієнта u у когорті c , безрозмірна величина ( 0 ≤ S u(t) ≤ 1) ; t – період спостереження . Формула (3.9 ) відображає перехід від індивідуального прогнозу S u(t) до"}
{"chunk_id": "2025_M_IUS_Potapenko_AO.pdf:14", "source": "2025_M_IUS_Potapenko_AO.pdf", "text": "𝑆𝑐(𝑡)=1 𝑁𝑐∑𝑆𝑢(𝑡) 𝑢∈𝑐, (3.9) де Sc(t) – середня функція вижива ння для всієї когорти клієнтів c у момент часу t, безрозмірна величина ( 0 ≤ S с(t) ≤ 1) ; 54 Nc – кількість клієнтів у когорті c , осіб ; Su(t) – індивідуальна функція виживання для кожного клієнта u у когорті c , безрозмірна величина ( 0 ≤ S u(t) ≤ 1) ; t – період спостереження . Формула (3.9 ) відображає перехід від індивідуального прогнозу S u(t) до агрегованого показника для всієї групи – Sc(t). Вона визначає, яка частка клієнтів певної когорти , у середньому, залишиться активною до моменту часу t. Оскі льки Su(t) є ймовірністю, то й Sc(t) також є безрозмі рною величиною в діапазоні [0; 1]. Використаємо дані з попередньої таблиці для п’яти клієнтів ( табл. 3.1 ). Індивідуальні ймовірності утримання за три місяці становлять: S u(3) = [0,916; 0,787; 0,877; 0,522; 0,235]. Тоді середня функція виживання для когорти розраховується як: 𝑆𝑐(3)=1 5(0,916 +0,787 +0,877 +0,522 +0,235 )=0,667 . (3.10) Отже, середня ймовірність того, що клієнти цієї когорти залишаться активними через три м ісяці, становить приблизно 66,7 %. На че твертому етапі розраховується скоригований показник CLV. На відміну від базового значення CLV , який враховує лише історичні дані про утримання клієнтів R c(t), скоригований показник CLV відобража тиме прогнозовану поведінку клієнтів з урахуванням ризику відтоку. Формула ризик -скоригованого показника CLV ма тиме вигляд: 𝐶𝐿𝑉𝑐𝑎𝑑𝑗=∑𝑆𝑐(𝑡)∗𝑚𝑐(𝑡) (1+𝑑)𝑡𝑇 𝑡=1, (3.11) де Sc(t) – прогнозована ймовірність того, що клієнти когорти c залишатимуться активними у періоді t, безрозмірна величина (0 ≤ S с(t) ≤ 1) ; 55 mc(t) – середній дох ід від одного активного клієнта когорти с у період t, грн/період ; d – ставка дисконтування, що враховує зміну вартості грошей у часі, вимірюється у частках від одиниці або у відсотках за період ; T – горизонт прогнозування , кількість періодів . Для демонстрації застосування ризик -скоригованої моделі розглянемо когорту клієнтів, для якої середня функція виживання через три місяці становить Sc(3) = 0,67 , як було отримано у попередньому прикладі. Припус тимо, що середній місячний дохід від одного активного клієнта дорівнює mc(t) = 100 грн, ставка дисконту d = 0,05 (5% на місяць), а горизонт прогнозування T = 5 місяців. Для демонстрації використано умовний ряд значень Sc(t), що плавно зменшується в часі та відображає тип ову динаміку скорочення активності клієнтів: Sc(t) = [0,87; 0,79; 0,67; 0,62; 0,60 ]. Для кожного періоду t обчислюється очікуваний дохід за формулою (3. 11). Результати обчислень наведено в табл. 3.2. Таблиця 3.2 – Результат обчислення ризик -скоригованого показника CLV t (місяць) 𝑆𝑐(𝑡) (1+𝑑)𝑡 𝑆𝑐(𝑡) ∗ 𝑚𝑐(𝑡) (1+𝑑)𝑡, грн 1 0,87 1,05 82,86 2 0,79 1,10 71,66 3 0,67 1,16 57,88 4 0,62 1,22 51,01 5 0,6 1,28 47,01 Сума 310,41 У результаті отримано CLV cadj ≈ 310 грн, тоді як базова оцінка без урахування відтоку становить CLV cbase ≈ 324 грн. Зменшення значення 56 пояснюється тим, що частка клієнтів має високий ризик відтоку , що зменшує очікувані майбутні прибутки . Такий приклад демонструє, як в рахування функції виживання дає можливість отримати більш реалістичну та економічно обґрунтовану довгострокову цінність клієнтів . Отже, подвійний розрахунок CLV має різне призначення на кожному етапі комбінованого методу. Спочатку обчислюється базовий показник, який відображає історичну дохідність клієнтської бази без урахування ризиків . Далі проводиться коригування показника CLV з інтеграцією функції виживання. Вона використовується як ваговий коефіцієнт, що коригує майбутні грошові потоки відповідно до прогнозованої ймовірності утримання клієнтів. Такий підхід дає змогу отримати більш реалістичну оцінку довгострокової цінності, що враховує не лише обсяг доходу від клієнта , а й ризик його втрати в часі. Такий підхід забезпечує зв’язок між історичною фінансовою оцінкою та прогнозною моделлю поведінки клієнтів. Це дає змогу не лише порівняти отримані результати, а й оцінити, як зміни у поведінці клієнтів впливають на фактичну прибутковість підпр иємства, та визначи ти більш точні стратегії їх утримання . 3.4 Інтеграція методу в ІС та очікувані результати Розроблений комбінований метод аналізу поведінки клієнтів інтегрується в ІС планування діяльності підприємства як окремий аналітичний модуль. Він забезпечує обробку даних про клієнтів, розрахунок показників CLV та оцінювання ризику відтоку. Основною метою інтеграції є підвищення аналітичних можливостей ІС, формування комплексного розуміння клієнтської поведінки та отримання достовірних прогнозів для підтримки управлінських рішень . 57 Як показано на рис. 3.2, модуль отр имує дані з кількох джерел: бази даних клієнтів, транзакційного журналу та системи моніторингу активності. На виході формується набір аналітичних результатів, що включає когортні retention -криві та ризик -скориговані показники CLV. Ці результати можуть вико ристовуватися для побудови візуалізацій, створення аналітичних звітів або формування узагальнених показників у межах ІС . Очікуваний результат інтеграції полягає у розширенні функціональних можливостей ІС: система зможе не лише фіксувати поточні показники діяльності, а й прогнозувати поведінку клієнтів у майбутніх періодах. Застосування комбінованого методу дасть змогу формувати більш точні прогнози доходів, своєчасно ідентифікувати ризикові сегменти клієнтської бази та визначати найбільш прибуткові когорти для подальшої роботи . Інтеграція розглядається на концептуальному рівні та демонструє логічну"}
{"chunk_id": "2025_M_IUS_Potapenko_AO.pdf:15", "source": "2025_M_IUS_Potapenko_AO.pdf", "text": "-скориговані показники CLV. Ці результати можуть вико ристовуватися для побудови візуалізацій, створення аналітичних звітів або формування узагальнених показників у межах ІС . Очікуваний результат інтеграції полягає у розширенні функціональних можливостей ІС: система зможе не лише фіксувати поточні показники діяльності, а й прогнозувати поведінку клієнтів у майбутніх періодах. Застосування комбінованого методу дасть змогу формувати більш точні прогнози доходів, своєчасно ідентифікувати ризикові сегменти клієнтської бази та визначати найбільш прибуткові когорти для подальшої роботи . Інтеграція розглядається на концептуальному рівні та демонструє логічну взаємодію розробленого методу з іншими підсистемами ІС. У наступному розділі наведено приклад практичної реалізації комбінованого методу та проведено аналіз отрим аних результатів . 58 4 ЕКСПЕРИМЕНТАЛЬНА ПЕРЕ ВІРКА ПРАЦЕЗДАТНОСТІ РОЗРОБЛЕНОГО КОМБІНО ВАНОГО МЕТОДУ АНАЛІЗ У ПОВЕДІНКИ КЛІЄНТІВ 4.1 Приклад застосування розробленого комбінованого методу Для демонстрації практичного застосування розробленого комбінованого методу аналізу поведінки клієнтів використано транзакційні дані умовної компанії . У наборі даних містяться : - унікальний ідентифікатор клієнта; - дата здійснення транзакції; - сума покупки. Такий формат даних є типовим для CRM - та e -commerce -систем і забезпечує можливість проведення повноцінного когортного та поведінков ого аналіз у, необхідного для реалізації комбінованого методу. Відповідно до методики когортного аналізу, описан ої в розділі 3, для кожного клієнта було визначено дату першої покупки за формулою (3.1). Серед сформованих когорт для подальшого аналізу обрано когорту лютого 2024 року. Такий вибір обґрунтований такими чинниками : - когорта має оптимальний розмір , що дозволяє отримувати ст абільні статистичні оцінки; - період спостереження охоплює майже повний рік, що дає достатній горизонт для розрах унку показників R(t), m(t), CLV base та побудови survival - моделі; - поведінка клієнтів не містить виражених сезонних коливань , що підвищує репрезентативн ість отриманих результатів . На наступному етапі комбінованого методу здійснюється аналіз динаміки утримання клієнтів. За формулою (3.3) розраховано частку утриманих клієнтів когорти. Як було зазначено вище, показник R(t) виражається у частка х і змінюється в діапазоні від 0 до 1 (або від 0% до 100%). 59 Результати обчислень наведено в табл. 4.1. Таблиця 4.1 – Активні кл ієнти та частка утримання R(t) когорти Місяць віку когорти Кількість активних клієнтів Частка утриманих клієнтів 2024 -02 163 1,00 2024 -03 54 0,33 2024 -04 75 0,46 2024 -05 59 0,36 2024 -06 68 0,42 2024 -07 63 0,39 2024 -08 56 0,34 2024 -09 61 0,37 2024 -10 56 0,34 2024 -11 67 0,41 2024 -12 73 0,45 Для порівняння динаміки утримання між різними групами клієнтів було побудовано retention -криві для трьох різних когорт: січня, лютого та березня 2024 року (рис. 4.1). Згідно з рис. 4.1 , для всіх когорт характерним є різкий спад активності після першого міс яця. Це є типовою поведінковою моделлю: більшість клієнтів демонструють найвищу активність у момент першої взаємодії з компанією , після чого інтенсивність покупок поступово знижується. Починаючи з другого місяця значення утриман ня стабілізуються в інтервал і 0,30–0,45, що свідчить про формування групи постійних клієнтів, які продовжують взаємодію з компанією протягом кількох місяців поспіль. Порівняння трьох когорт показує, що характер динаміки утримання є подібним: після початкового спаду криві мають близькі траєкторії, без суттєвих розбіжностей між когортами . Це дає підстави стве рджувати , що 60 сезонні чинники або зовнішні умови не мали значного впливу на поведінку клієнтів протягом аналізованого періоду. Отже, побудовані retention -криві підтверджують с табільність поведінкової моделі клієнтів та можуть бути використан і як в хідні дані для подальших розрахунків середнього доходу m(t), базового показника CLV та побудови survival -моделі . Рисунок 4.1 – Графік retention -кривих для когорт «2024 -01», «2024 -02» та «2024 -03» На другому етапі визначається показник середнього доходу на одного активного клієнта за формулою (3.4) та обчислюється базовий показник CLV за формулою (3.5). У межах дослідження ставку дисконту прийнято d = 10%. Результа ти розрахунків наведено в табл. 4.2. 61 Таблиця 4.2 – Результати розрахунків базового показника CLV Місяць віку когорти t Кількість активних клієнтів Частка R(t) Дохід m(t) Множник 1 (1+d)𝑡 Базовий показник CLV 2024 -02 1 163 1,00 658,82 0,91 598,92 2024 -03 2 54 0,33 662,57 0,83 181,41 2024 -04 3 75 0,46 693,04 0,75 239,58 2024 -05 4 59 0,36 589,46 0,68 145,73 2024 -06 5 68 0,42 685,53 0,62 177,58 2024 -07 6 63 0,39 687,23 0,56 149,93 2024 -08 7 56 0,34 661,74 0,51 116,67 2024 -09 8 61 0,37 692,73 0,47 120,94 2024 -10 9 56 0,34 640,84 0,42 93,37 2024 -11 10 67 0,41 641,32 0,39 101,63 2024 -12 11 73 0,45 712,31 0,35 111,81 Сумарний базовий показник CLV когорти лютого 2024 року: 𝐶𝐿𝑉2024 −02𝑏𝑎𝑠𝑒=∑𝐶𝐿𝑉 𝑇𝑇 𝑡=1=2037 ,58 грн. (4.1) На третьому етапі будується survival -модель , яка використовує наступні поведінкові змінні: - frequency – частота транзакцій клієнта; - avg_purchase_norm – нормований середній чек; - avg_interval – середній інтервал між покупками. Значення коефіцієнтів моделі прийняті такі : - βfrequency = -0,05; - βavg_purchase_norm = -0,12; 62 - βavg_interval = 0,04. Значення коефіцієнтів обрано відповідно до очікуваного впливу поведі нкових змінних на"}
{"chunk_id": "2025_M_IUS_Potapenko_AO.pdf:16", "source": "2025_M_IUS_Potapenko_AO.pdf", "text": "0,45 712,31 0,35 111,81 Сумарний базовий показник CLV когорти лютого 2024 року: 𝐶𝐿𝑉2024 −02𝑏𝑎𝑠𝑒=∑𝐶𝐿𝑉 𝑇𝑇 𝑡=1=2037 ,58 грн. (4.1) На третьому етапі будується survival -модель , яка використовує наступні поведінкові змінні: - frequency – частота транзакцій клієнта; - avg_purchase_norm – нормований середній чек; - avg_interval – середній інтервал між покупками. Значення коефіцієнтів моделі прийняті такі : - βfrequency = -0,05; - βavg_purchase_norm = -0,12; 62 - βavg_interval = 0,04. Значення коефіцієнтів обрано відповідно до очікуваного впливу поведі нкових змінних на ризик відтоку: - знак мінуса перед частотою покупок (β frequency = -0,05) означає , що активні клієнти мають нижчу й мовірність припинення взаємодії; - вищий середній чек також зменшує ризик втрати клієнта (βavg_purchase_norm = -0,12); - позитивний коефіцієнт при середньому інтервалі між покупками (βavg_interval = 0,04) означає, що великі паузи між транзакціями є індикатором зростання ризику відтоку. Індивідуальний показник інтенсивності ризику відтоку обчислюється за формулою : 𝛼𝑢=𝑒𝛽frequency ∗𝑥1+𝛽avg _purchase _norm ∗𝑥2+𝛽avg _interval ∗𝑥3. (4.2) Survival -функція клієнта обчислюється за формулою: 𝑆𝑢(𝑡)=𝑆0𝛼𝑢, 𝑆0=0,80. (4.3) Середнє значення survival -функції для когорти становить S c = 0,44. Це означає, що клієнт когорти буде залишатися активним у найближчому періоді з імовірністю 44%. На останньому етапі обчислюється ризик -скоригований показник CLV за формулою (3.11). У табл. 4.3 наведено порівняння значень базового та ризик -скоригованого показників CLV . 63 Таблиця 4.3 – Порівняння отриманих р езультатів обчислень показників CLV Місяц ь віку когорти Базовий показник CLV Ризик -скоригований показник CLV 2024 -02 598,93 261,34 2024 -03 181,41 239,73 2024 -04 239,58 226,58 2024 -05 145,73 174,73 2024 -06 177,58 185,28 2024 -07 149,93 167,76 2024 -08 116,67 147,12 2024 -09 120,94 141,93 2024 -10 93,37 117,33 2024 -11 101,63 109,03 2024 -12 111,81 108,68 Графічне порівняння отриманих показників CLV наведено на рис. 4.2. Рисунок 4.2 – Порівняння показників CLV 64 Сумарний ризик -скоригований показник CLV когорти лютого 2024 року: 𝐶𝐿𝑉2024 −02𝑎𝑑𝑗=∑𝐶𝐿𝑉 𝑇𝑇 𝑡=1=1880 ,9 грн. (4.4) 4.2 Аналіз отриманих результатів Використання survival -аналізу дозволило доповнити когортний підхід оцінкою ризику тривалого відтоку. Отримане значення функції виживання – 0,44 свідчить про те , що майже половина клієнтів має потенціал залишатися активними, навіть якщо вони не здійснили покупку в певному місяці. Поведінкові фактори підтвердили очікувану логіку впливу на ризик відтоку : - клієнти з високою частотою покупок демонструють нижчий ризик відтоку; - вищий середній чек корелює зі стабільн ішою активністю та лояльністю ; - тривалі інтервали між транзакціями є надійним індикатором ризику , що знижу є прогнозовану цінність клієнта. Результати обчислень продемонстрували, що скоригований за ризиком показник CLV є нижчим за базовий на близько 7 –8%. Це очікуваний результат, оскільки базов ий CLV не враховує індивідуальної ймовірності відтоку клієнтів , тоді як survival -корекція усуває цю неповноту та роб ить прогноз більш реалістичним. В окремих періодах скоригований показник CLV перевищує базовий. Це може бути наслідком того, що аналіз виживання оцінює потенційну активність клієнта, тоді як когортний підхід базується лише на факті здійсненої покупки. 65 Survival -модель «пом’якшує» падіння активності у періоди, де R(t) є низьким, і дає плавніш у динаміку. Сумарне значення CLV зменшується, що цілком обґрунтовано з економічної точки зору : ризик -скоригований показник CLV відображає дійсні довгострокові перспективи клієнтів, враховуючи неоднорідність їхньої поведінки та ймовірність відтоку . З метою комплексної оцінки запропонованого комбінованого методу було здійснено розрахунок сумарних показників CLV для всіх клієнтських когорт 2024 року . Результати п орівняння навед ено в табл. 4.4. Таблиця 4.4 – Результат п орівняння сумарних показників CLV Когорта клієнтів Сумарний базовий показник CLV Сумарний ризик - скоригований показник CLV 2024 -01 2085,27 грн 2044,65 грн 2024 -02 2037,58 грн 1880,9 грн 2024 -03 1839,48 грн 1737,56 грн 2024 -04 1729,99 грн 1561,57 грн 2024 -05 1704,78 грн 1400,93 грн 2024 -06 1736,88 грн 1841,59 грн 2024 -07 1945,56 грн 1771,08 грн 2024 -08 868,52 грн 1022,68 грн 2024 -09 1374,21 грн 1183,99 грн 2024 -10 539,99 грн 472,74 грн 2024 -11 1171,69 грн 833,62 грн Результати демонструють, що співвідношення між базовим та ризик - скоригованим показниками CLV залежить від поведінкових особливостей кожної когорти. У когортах, де клієнти демонструють нерегулярні інтервали між 66 покупками, нестабільну активність або підвищений ризик в ідтоку, скоригований показник є нижчим за базовий. Це свідчить про те, що survival - модель коригує завищені очікування, притаманні класичному когортному підходу, та зменшує прогнозовану довгострокову цінність таких когорт відповідно до їх реальної поведінко вої динаміки. Водночас у деяких когорт ризик -скоригований показник CLV перевищує базовий. Це характерно для клі єнтів зі стабільною активністю, короткими інтервалами між транзакціями або високим середнім чеком. У таких випадках survival -модель підтверджує п ідвищений потенціал клієнтів і демонструє, що класичний підхід може недооцінювати їхню цінність . Таким чином, порівняння двох моделей дає змогу : - оцінити поведінкові особливості когорт і ступінь стабільності їхньої поведінки ; - виявити найбільш перспективні с егменти клієнтів; - коригувати управлінські рішення з урахуванням реальних поведінкових ризиків. У подальших дослідженнях доцільно розширити набір поведінкових характеристик клієнтів та перевірити вплив інших survival"}
{"chunk_id": "2025_M_IUS_Potapenko_AO.pdf:17", "source": "2025_M_IUS_Potapenko_AO.pdf", "text": "зі стабільною активністю, короткими інтервалами між транзакціями або високим середнім чеком. У таких випадках survival -модель підтверджує п ідвищений потенціал клієнтів і демонструє, що класичний підхід може недооцінювати їхню цінність . Таким чином, порівняння двох моделей дає змогу : - оцінити поведінкові особливості когорт і ступінь стабільності їхньої поведінки ; - виявити найбільш перспективні с егменти клієнтів; - коригувати управлінські рішення з урахуванням реальних поведінкових ризиків. У подальших дослідженнях доцільно розширити набір поведінкових характеристик клієнтів та перевірити вплив інших survival -моделей на точність прогнозування довгострокової цінності. Перспективним напрямом є також інтеграція отриманих показників у рекомендаційні системи чи модулі персоналізації для формування індивідуальних пропозицій і цілеспрямованих стратегій утримання клієнтів. Це дозволить поглибити аналіз , підвищити адаптивність бізнес -процесів та забезпечити стійкість прогнозів у різних умовах і для вибірок різного розміру . 67 ВИСНОВКИ У ході виконання магістерської кваліфікаційної роботи було здійснено комплексне дослідження методів аналізу поведінки клієнтів та визначено їх придатність до використання в ІС для планування ді яльності підприємства. Встановлено, що сучасні підприємства, зокрема , у сфері електронної комерції, стикаються з низкою особливосте й щодо поведінки клієнтів, які ускладнюють процес прогнозування та потребують поєднання різних аналітичних підходів. У процесі дослідження було здійснено ґрунтовний огляд наявних підходів до аналізу клієнтської поведінки та визначено їх функціональні можли вості. На основі порівняння класичних і прогнозних методів встановлено, що жоден із них окремо не забезпечує повного уявлення про довгострокову взаємодію клієнтів із підприємством. Тому було проведено аналіз когортного підходу, моделей оцінювання CLV та ме тодів прогнозування відтоку з використанням аналізу виживання. У результаті обґрунтовано доцільність поєднання цих підходів у межах комбінованого методу. Було розроблено комбінований метод аналізу поведінки клієнтів, заснований на застосуванні когортного а налізу, базового показника CLV та survival -моделі. Детально описано етапи роботи методу, а також побудовано його функціональну структуру у нотації IDEF0, що підтверджує можливість інтеграції методу в аналітичні модулі ІС. Перевірено працездатність запропонованого комбінованого методу з використанням транзакційних даних умовної компанії . Було сформовано когорти клієнтів, визначено динаміку їхнього утримання, розраховано базові значення CLV та побудовано survival -модель з урахуванням поведінкових хара ктеристик. Порівняння базового та скоригованого показників CLV продемонструвало , що їх співвідношення залежить від стабільності та особливостей поведінкової динаміки клієнтів у межах кожної когорти. Такий 68 підхід дав змогу отримати більш узгоджену оцінку до вгострокової взаємодії клієнтів із підприємством і виявити групи з підвищеним або зниженим потенціалом. Таким чином , результати експериментальних досліджень підтвердили працездатність та ефективність розробленого комбінованого методу . Отримані в роботі результати можуть бути використані під час розробки ІС для планування діяльності підприємства. Запропонований комбінований метод дає змогу формувати прогнозні показники, досліджувати особливості поведінки клієнтів, враховувати імовірність їхнього відтоку. Це створює підґрунтя для побудови гнучких моделей взаємодії з клієнтами, уточнення управлінських рішень і формування стратегій, які враховують реальні тенденції розвитку клієнтської бази. Подальші дослідження можуть бути спрямовані на розширення набору поведі нкових параметрів, удосконалення прогнозних моделей відтоку та інтеграцію отриманих результатів у системи персоналізації або рекомендаційні модулі. Це дасть змогу підвищити адаптивність аналітичних рішень та проводити більш глибоке моделювання поведінки кл ієнтів у різних галузях. Результати дослідження пройшли апробацію на міжнародних наукових конференціях «Science and Information Technologies in the Modern World» (Афіни, 2025) [ 33] та «The Future of Science, Technology and Econom y» (Софія, 2025) [ 34]. Робо ту виконано відповідно до вимог щодо розробки та оформлення кваліфікаційних робіт [35], а також із дотриманням положень державних стандартів ДСТУ 3008:2015 [36] і ДСТУ 8302:2015 [37] . 69 ПЕРЕЛІК ДЖЕРЕЛ ПОСИЛА ННЯ 1. Urbany J. , Dapena -Baron M. The pursuit of customer centricity // AMS review. 2025. URL: https://doi.org/10.1007/s13162 -024-00288 -4 (date of access : 24.11.2025) . 2. Момотков І. С. Нейромаркетинг в основі маркетингових рішень: нове бачення поведінки клієнтів // Українськи й економічний часопис. 2025 . No 9. С. 84 –89. URL: https://doi.org/10.32782/2786 -8273/2025 -9-14 (дата звернення: 24 .11.2025) . 3. Potapenko A., Shekhovtsova V. Protection of personal data of consumers in the analysis of dema nd for the company's products . Modern pedagogical technologies a nd innovative methods : materials of the 3d International scientific and practical conf. , Seville, 25-28 Feb. 2025. Seville, 2025. P. 53 –56. 4. Як розрахувати когортний аналіз // Data -life-ua. URL: https://data -life- ua.com/analyst/yak -rozrakhuvaty -kohortnyy -analiz/ (дата звернення: 25 .11.2025) . 5. Somavarapu S. Cohort analysis for user behavior trends in marketplace platforms // International journal of research in modern engineering and emerging technology . 2025. No 1. P. 149 –173. 6. Cohort analysis: 4 use cases for product managers // Smartlook blog. URL: https://www.smartlook.com/blog/cohort -analysis/ (date of access : 25.11.2025) . 7. Ahn J . A survey on churn analysis in various business domains // IEEE access. 2020. No 8. P. 220816 –2208 39. URL: https://doi.org/10.1109/access. 2020.3042657 (date of access : 25.11.2025) . 8. Земко П. А. Застосування логістичної регресії для прогнозування відтоку клієнті в телекомунікаційної компанії. Радіоелектроніка та молодь у ХХІ столітті: матеріали 25 -го Між нар. молодіж н. форума, м. Харків, 20-22 квіт."}
{"chunk_id": "2025_M_IUS_Potapenko_AO.pdf:18", "source": "2025_M_IUS_Potapenko_AO.pdf", "text": "cases for product managers // Smartlook blog. URL: https://www.smartlook.com/blog/cohort -analysis/ (date of access : 25.11.2025) . 7. Ahn J . A survey on churn analysis in various business domains // IEEE access. 2020. No 8. P. 220816 –2208 39. URL: https://doi.org/10.1109/access. 2020.3042657 (date of access : 25.11.2025) . 8. Земко П. А. Застосування логістичної регресії для прогнозування відтоку клієнті в телекомунікаційної компанії. Радіоелектроніка та молодь у ХХІ столітті: матеріали 25 -го Між нар. молодіж н. форума, м. Харків, 20-22 квіт. 2021 р. Харків , 2021. Т. 7. С . 53–54. 9. Geiler L., Affeldt S., Nadif M. A survey on machine learning methods for 70 churn prediction // International journal of data science and analytics . 2022. No 14. P. 217 –242. URL: https://doi.org/10.1007/s41060 -022-00312 -5 (date of access : 25.11.2025) . 10. Ashtari H. XGBoost vs random forest vs gradient boosting // Spiceworks inc. URL: https://www.spiceworks.com/tech/artificial -intelligence/articles/xgboost - vs-random -forest -vs-gradient-boosting/ (date of access : 26.11.2025) . 11. Бовчалюк С ., Гайдай Я . Аналіз методу опорних векторів у порівнянні з традиційними методам и передбачення ринкових рухів // Збірник наукових праць. 2024. No 77. С. 89–92. URL: https://doi.org/10.26906/sunz.2024.3.089 (дата звернення: 2 6.11.2025) . 12. Edwine N., Wang W. Detecting the risk of customer churn in telecom sector: a comparative study // Mathematical problems in engineering . 2022. No 1. P. 1–16. URL: https://doi.org/10.1155/2022/8534739 (date of access : 26.11.2025). 13. Dusmatov B. The subscription economy : transforming business model s // Journal of analysis and inventions . 2024. No 2. P. 71–76. 14. Uwah V. Impact of subscription revenue model on 21st century businesses // Journal of science education and humanities . 2023. No 7. P. 132 –142. 15. Kowalkowski C., Ulaga W. Subscription offers in business -to-business markets: conceptualization, taxonomy and framework for growth // Industrial marketing management . 2024. No 117. P. 440–456. URL: https://doi.org/10.1016/j.indmarman.2024.01.014 (date of access : 26.11.2025). 16. Тимошенко К. Роль стратегічного управління в адаптації бізнес - моделей електронної ко мерції до динамічних змін ринку // Економіка та суспільство . 2025. No 74. С. 637 –642. URL: https://doi.org/10.32782/2524 - 0072/2025 -74-94 (дата звернення: 26.11.2025). 17. Dogan O. Business analytics in customer lifeti me value: an overview analysis // WIREs data mining and knowledge discovery . 2024. No 15. P . 1–18. URL: https://doi.org/10.1002/widm.1571 (date of access : 27.11.2025). 18. Qismat T., Feng Y. Comparis on of classical RFM models and m achine learning models in CLV prediction // Master's thesis. Oslo. 2023 . 71 19. Bauer J., Jannach D. Improved customer lifetime value predic tion with sequence -to-sequence learning and feature -based models // ACM transactions on knowledge discovery from data . 2021. No 5. P. 1–37. URL: https://doi.org/10.1145/3441444 (date of access : 27.11.2025). 20. Akter J., Roy A. Artificial intelligence -driven customer lifetime value (CLV) forecasting: integrating RFM analysis with machine learning f or strategic customer retention // Journal of computer science and technology studies . 2025. No 1. P. 249–257. URL: https://doi.org/10.32996/jcsts.2025.7.1.18 (date of access : 28.11.2025). 21. Герчаківський О. Аналіз впливу маркетингових комунікацій на споживачів в ритейлі: переваги та недо ліки інтегр ованої маркетингової стратегії // Академічні візії . 2023. No 17. С. 1–9. URL: https://doi.org/10.5281/zenodo.7759321 (дата звернення: 28 .11.2025). 22. Darvidou K. Omnichannel marketing in the digita l age: creating consistent, personalized and connecte d customer experiences // Technium business and management . 2023. No 10 . P. 34–54 (date of access : 29.11.2025). 23. Sinha R., Arbour D. Bayesian mod eling of marketing attr ibution // JSM proceedings . 2022. URL: https://doi.org/10.48550/arXiv.2205.15965 (date of access : 29.11.2025). 24. Abrardi L., Cambini C., Rondi L. Artificial intelligence, firms and consumer behavior: a survey // Journal of economic s urveys . 2021. No 4. P . 969 – 991. URL: https://doi.org/10.1111/joes.12455 (date of access : 01.12 .2025). 25. Siti Z., Hazik M. Customer predictive analytics using artificial intelligence // The Singapore economic review . 2025 . No 4. P. 1009–1020 . URL: https://doi.org/10.1142/s0217590820480021 (date of access : 01.12 .2025). 26. Tudoran A., Hjerrild C., Thomasen S. Understanding consum er behavior during and after a pandemic: i mplications for customer l ifetime value prediction models // Journal of business research . 2024. No 174. P. 114527 –1145 40. URL: https://doi.org/10.1016/j.jbusres.2024.114527 (date of access : 02.12 .2025). 27. Gurunathan A., Lakshmi K. Exploring the perceptions of generations x, y 72 and z about online platforms and digital marketing a ctivities : a focus -group discussion based study // International journal of professional business review . 2023. No 5. P. 1–34. URL: https://doi.org/10.26668/businessreview/2023.v8i5.2122 (date of access : 03.12 .2025). 28. Adekunle B., Balogun E., Ogunsola K. Improving customer rete ntion through machine learning: a predictive approach to churn preve ntion and engagement strategies // International journal of scientific research in computer science, engineering and information technology . 2023. No 4. P. 507 –523. URL: https://ijsrcseit.com/CSEIT2311254"}
{"chunk_id": "2025_M_IUS_Potapenko_AO.pdf:19", "source": "2025_M_IUS_Potapenko_AO.pdf", "text": "platforms and digital marketing a ctivities : a focus -group discussion based study // International journal of professional business review . 2023. No 5. P. 1–34. URL: https://doi.org/10.26668/businessreview/2023.v8i5.2122 (date of access : 03.12 .2025). 28. Adekunle B., Balogun E., Ogunsola K. Improving customer rete ntion through machine learning: a predictive approach to churn preve ntion and engagement strategies // International journal of scientific research in computer science, engineering and information technology . 2023. No 4. P. 507 –523. URL: https://ijsrcseit.com/CSEIT2311254 (date of access : 03.12.2025) . 29. Kalishina D. Algorithmic customer churn predict ion and targeted intervention: o ptimizing customer lifetime value in data -sparse SME environments // World journal of adv anced research and reviews . 2025. No 1. P. 593 –603. URL: https://doi.org/10.30574/wjarr.2025.26.1.1045 (date of access : 03.12.2025). 30. IDEF // Maxym Zosуm . URL: https://www.maxzos im.com/idef/ (date of access : 04.12.2025). 31. Cox proportional hazards model // GeeksforGeeks . URL: https://www.geeksforgeeks.org/data -science/cox -proportional -hazards -model/ (date of access : 05.12.2025). 32. Understanding customer attrition through survival analysis : Kaplan M eier & Cox proportional // Medium. URL: https://medium.com/@agudaidris5/ understanding -customer -attrition -through -survival -analysis -kaplan -meier -cox- regression -on-in-45e3d871a82d (date of access : 06.12.2025). 33. Потапенко А . О., Петров К . Е. Дослідження та використання методів аналізу поведінки клієнтів для планування діяльності підприємства. Наука та інформаційні технології у сучасному світі: матеріали Міжн ар. наук .-практ . конф ., м. Афіни, 21 -23 трав . 2025 р. Афіни, 2025. С. 295 –297. 34. Потапе нко А. О., Петров К. Е. Комбінований метод аналізу поведінки клієнтів у системах планування діяльності підприємства . Майбутнє науки, технологій та економіки: матеріали Міжн ар. наук .-практ . конф ., м. Софія, 29 -31 жовт . 2025 р. Софія, 2025. С. 206–210. 73 35. Метод ичні вказівки щодо розробки та оформлення кваліфікаційної роботи другого (магістерського) рівня вищої освіти за освітньо -професійною програмою «Інформаційні управляючі системи та технології» спеціальності 122 Комп’ютерні науки / Упоряд.: К.Е. Петров, В.М. Левикін, С.Ф. Чалий, М.В. Євланов, В.І. Саєнко, Д.К. Міхнов, А.В. Міхнова, О.В. Чала. ХНУРЕ: Харків, 2024. 24 с. 36. ДСТУ 3008:2015. Інформація та документація. Звіти у сфері науки і техніки. Структура та правила оформлювання. Чинний від 2017 -07-01. – Київ: ДП «УкрНДНЦ», 2016. – 31 с. 37. ДСТУ 8302:2015. Інформація та документація. Бібліографічне посилання. Загальні положення та правила складання. Чинний від 2016 -07-01. – Вид. офіц. Київ : УкрНДНЦ, 2016. 16 с ."}
{"chunk_id": "2025_M_IUS_Shkredova_YeYa.pdf:0", "source": "2025_M_IUS_Shkredova_YeYa.pdf", "text": "1 Міністерство освіти і науки України Харківський національний університет радіоелектроніки Навчально -науковий центр заочної форми навчання (повна назва) Кафедра Інформаційних управляючих систем (повна назва) КВАЛІФІКАЦІЙНА РОБОТА Пояснювальна записка рівень вищої освіти другий (магістерський) Дослідження методів аналізу збитків при розробці інформаційної (тема) системи виробництва виробів із нержавіючої сталі Виконав: здобувач 2 року навчання , групи ІУСТзм -24-1 Єлизавета ШКРЕДОВА (власне ім’я , прізвище ) Спеціальність 122 К омп’ютерні науки (код і повна назва спеціальності) Тип програми освітньо -професійна (освітньо -професійна або освітньо -наукова) Освітня програма Інформаційні управляючі системи та технології (повна назва освітньої програми) Керівник: проф. каф. ІУС Ірина ПАНФЬОРОВА (посада, власне ім’я, прізвище ) Допускається до захисту Завідувач кафедри ІУС Костянтин ПЕТРОВ (підпис) (власне ім’я , прізвище ) 2025 р. 2 Харківський національний університет радіоелектроніки ЗАТВЕРДЖУЮ: Зав. кафедри (підпис) « 24 » листопада 20 25 р. ЗАВДАННЯ НА КВАЛІФІКАЦІЙНУ РОБОТУ здобуваче ві Шкре довій Єлизаветі Ярославовні (прізвище, ім’я, по батькові) 1. Тема роботи Дослідження методів аналізу збитків при розробці інформаційної системи виробництва виробів із нержавіючої сталі затверджена наказом університету від « 24 » листопада 2025 р. № 201Стз 2. Термін подання здобувачем роботи до екзаменаційної комісії « 17 » грудня 2025 р. 3. Вихідні дані до роботи методи аналізу збитків, правила для написання рекомендацій, дані про збитки від простоїв, дані про збитки від браку, дані про збитки від втрати матеріалів, дані про збитки через логістичні проблеми, дані про збитки через перевиробництво, каталог виробів, матеріали передатестаційної практики, наукові публікації та інтернет -джерела з тематики аналізу збитків на виробництві. 4. Перелік питань, що потрібно опрацювати в роботі опис бізнес -процесу аналізу збитків на виробництві виробів із нержавіючої сталі, опис підприємства, опис недоліків існуючого бізнес -процесу, опис ІС аналізу збитків, опис методів аналізу збитків, формулювання задачі дослідження, аналіз методів вирішення задачі аналізу збитків, розробка комбінованого методу аналізу збитків, експериментальна перевірка працездатності комбінованого методу аналізу збитків, оцінювання точності запропонованого рішен ня, формування висновків. Навчально -науковий центр заочної форми навчання Кафедра Інформаційних управляючих систем Рівень вищої освіти другий (магістерський) Спеціальність 122 К омп’ютерні науки (код і повна назва) Тип програми освітньо -професійна (освітньо -професійна або освітньо -наукова) Освітня програма Інформаційні управляючі системи та технології (повна назва) 3 3 КАЛЕНДАРНИЙ ПЛАН № Назва етапів роботи Термін и виконання етапів роботи Примітка 1 Дослідження бізнес -процесу аналізу збитків на виробництві виробів із нержавіючої сталі 24.11.2025 – 25.11.2025 Виконано 2 Опис ІС аналізу збитків 26.11.2025 – 27.12.2025 Виконано 3 Опис методів аналізу збитків для виробництва виробів із нержавіючої сталі 28.11.2025 – 29.11.2025 Виконано 4 Постанов ка задачі дослідження 30.11.2025 – 01.12.2025 Виконано 5 Обґрунтування вибору методів аналізу збитків 02.12.2025 – 03.12.2025 Виконано 6 Розробка комбінованого методу для вирішення задачі аналізу збитків 04.12.2025 – 05.12.2025 Виконано 7 Проєктування та опис підсистеми аналізу збитків при розробці ІС виробництва виробів із нержавіючої сталі 06.12.2025 – 07.12.2025 Виконано 8 Обґрунтування вибору технічного забезпечення підсистеми аналізу збитків 08.12.2025 –09.12.2025 Виконано 9 Експериментальна перевірка працездатності комбінованого методу аналізу збитків та оцінювання точності запропонованого рішення 10.12.2025 – 11.12.2025 Виконано 10 Оформлення пояснювальної записки 12.12.2025 – 13.12.2025 Виконано 11 Оформлення додатків 14.12.2025 – 15.12.2025 Виконано 12 Попередній захист 16.12.2025 Виконано Дата видачі завдання 24 листопада 2025 р. Здобувач (підпис) Керівник роботи проф. каф. ІУС Ірина ПАНФЬОРОВА (підпис) (посада, власне ім’я , прізвище ) 4 РЕФЕРАТ Пояснювальна записка кваліфікаційної роботи: 107 с., 20 рис., 17 табл., 3 дод., 31 джерела. АНАЛІЗ ЗБИТКІВ , ЗБИТКИ НА ВИРОБНИЦТВІ, ІНФОРМАЦІЙНА СИСТЕМА ПІДПРИЄМСТВА, КОГОРТНИЙ АНАЛІЗ , МЕТОДИ АНАЛІЗУ, МЕТОД ПАРЕТО, ПРИЧИНИ ЗБИТКІВ . Об’єктом дослідження кваліфікаційної роботи є процес аналізу збитків на виробництві виробів із нержавіючої сталі . Предметом дослідження є методи аналізу збитків , які застосовуються для зменшенн я фінансових витрат і прийняття управлінських рішень щодо управління збитк ами. Метою кваліфікаційної роботи є дослідження методів аналізу збитків для розробк и комбінованого методу аналізу збитків , що дозволить комплексно аналізувати різні типи збитків, виявляти причини їх виникнення для прийняття управлінсь ких рішень та мінімізувати вплив збитків на фінансові втрати підприємст ва. Для дослідження процесу аналізу збитків проаналізовано основні методи аналізу збитків та визначено комбінований метод аналізу збитків з урахуванням особливостей виробництва виробів із нержавіючої сталі . У роботі розглянуто опис підприємства, бізнес -процес підприємства, що займається аналізом збитків , та недоліки цього процесу . Проаналізовано інформаційні системи, що використовують для аналізу збитків , мет оди аналізу , сформульовано за дачу для комбінованого методу аналізу збитків та надано теоретичний опис . Здійснено практичну реалізацію поставленої задачі, спроєктовано підсистему аналізу збитків та експериментально перевірено запропоноване рішення аналізу збитків . 5 5 ABSTRACT Master’s thesis: 107 pages, 20 figures, 17 tables, 3 appendices, 31 sources. ANALYSIS OF LOSSES, ANALYSIS METHODS, CAUSES OF LOSSES , COHORT ANALYSIS , ENTERPRISE INFORMATION SYSTEM, PARETO METHOD , PRODUCTION LOSSES . The object of the research of this qualification work is the process of loss analysis in the production of stainless -steel products. The subject of the research is the methods of loss analysis used to reduce financial costs"}
{"chunk_id": "2025_M_IUS_Shkredova_YeYa.pdf:1", "source": "2025_M_IUS_Shkredova_YeYa.pdf", "text": "перевірено запропоноване рішення аналізу збитків . 5 5 ABSTRACT Master’s thesis: 107 pages, 20 figures, 17 tables, 3 appendices, 31 sources. ANALYSIS OF LOSSES, ANALYSIS METHODS, CAUSES OF LOSSES , COHORT ANALYSIS , ENTERPRISE INFORMATION SYSTEM, PARETO METHOD , PRODUCTION LOSSES . The object of the research of this qualification work is the process of loss analysis in the production of stainless -steel products. The subject of the research is the methods of loss analysis used to reduce financial costs and support managerial decision -making regarding loss management. The qualification work aims to research the methods of loss analysis in order to develop a combined loss analysis method that yields a comprehensive analysis of various types of losses, identify the causes of their occurrence for management decision -making, and minimize the impact of losses on the financial losses of the enterprise. To investigate the loss analysis process, the primary loss analysis methods were examined, and a combined loss analysis method was identified that accounts for the specific featu res of stainless -steel product manufacturing. The work includes a description of the enterprise, the business process related to loss analysis, and the shortcomings of this process. Information systems used for loss analysis and relevant analysis methods w ere examined. The task for the combined loss analysis method was formulated, and its theoretical description was provided. A practical implementation of the proposed solution was performed, a loss analysis subsystem was designed, and the proposed approach was experimentally validated. 6 6 ЗМІСТ С. Скорочення та умовні познаки ................................ ................................ ...... 8 Вступ ................................ ................................ ................................ ................. 9 1 Опис та аналіз існуючи х методів аналізу збитків на виробництві та формулювання задачі дослідження ................................ ............................... 11 1.1 Дослідження процесу аналізу збитків на виробни цтві виробів із нержавіючої сталі «KREDO» ................................ ................................ ..... 11 1.2 Опис проблеми вибору методу аналізу збитків на виробництві ...... 20 1.3 Аналіз існуючих ІС аналізу збитків на виробництві ......................... 22 1.4 Огляд існуючих методів аналізу збитків для виробництва виробів із нержавіючої сталі ................................ ................................ ...... 25 1.5 Висновки та формування задачі дослідження ................................ ... 27 2 Дослідження методів аналізу збитків при розробці ІС вироб ництва виробів із нержавіючої сталі ................................ ................................ .......... 29 2.1 Аналіз методів вирішення задачі аналізу збитків .............................. 29 2.2 Розробка комбінованого методу для вирішення задачі аналізу збитків ................................ ................................ ................................ .......... 32 2.3 Висновки до другого розділу ................................ ............................... 35 3 Інформаційна технологія дослідження методів аналіз у збитків при розробці ІС виробництва виробів із нержавіючої сталі .............................. 37 3.1 Розробка підсистеми «Аналіз збитків на виробництві вироб ів із нержавіючої сталі» з використанням комбінованого методу ................ 37 3.2 Особливості впровадження та експлуатації підсистеми «Аналіз збитків на виробництві виробів із нержавіючої сталі» ........................... 39 3.3 Опис алгоритму роботи підсистеми «Аналіз збитків на виробництві виробів із нержавіючої сталі» ................................ .............. 40 7 7 4 Експериментальна перевірка розробленого комбінованого методу та оцінка точності запропонованого рішення ................................ ................... 44 4.1 Обґрунтування вибору платформи програмного забезпечення ....... 44 4.2 Опис вимог д о програмного забезпечення ................................ ......... 45 4.3 Експериментальна перевірка отриманих результатів ....................... 47 4.4 Висновки до четвертого розділу ................................ .......................... 63 Висновки ................................ ................................ ................................ .......... 64 Перелік джерел посил ання ................................ ................................ ............. 67 Додаток А Документ «Правила для написання рекомендацій» ................ 71 Додаток Б Програмний код реалізації комбінованого методу аналізу збитків ................................ ................................ ................................ ............... 72 Додаток В Графічний матеріал кваліфікаційної роботи ........................... 76 8 8 СКОРОЧЕННЯ ТА УМОВНІ ПОЗНАКИ БД – база даних ІС – інформаційна система ПЗ  програмне забезпечення СУБД  система управління базами даних ERP – Enterprise Resource Planning OEE – Overall Equipment Effectiveness QMS – Quality Management Systems SADT – Structured Analysis and Design Technique SAP – System Analysis Program Development SVM – Support Vector Machine UML – Unified Modeling Language 9 9 ВСТУП Аналіз збитків на виробництві є одним із найважливіших елементів забезпечення ефективно ї роботи сучасного промислового підприємства. Збитки, що виникають у процесі виготовлення продукції, можуть мати різне походження  від нераціонального використання матеріалів і порушень у технологічному процесі до простоїв облад нання або людських помилок. Вчасне виявлення збитків на виробництві та точне визначення їх причин є критичними для зменшення фінансових втрат і підвищення якості продукції . Особливої актуальності ця проблема набуває для підприємств, що працюють із дорогими матеріалами, зокрема з нерж авіючою сталлю. Будь - який брак, виробничі відходи , логістичні проблеми, простої або неефективне використання сировини призводять до значних збитків . У таких умовах важливо не лише фіксувати збитки, а й аналізувати їх причин и, щоб вчасно вживати заходи для їх усунення і запобігати повтор у аналогічних ситуацій у майбутньому. Це дозвол ить підприємству не лише зменшити фінансові витрати, а й підвищити загальну продуктив ність і конкурентоспроможність підприємства . Для реалізації цього завдання необхідне використ ання сучасних методів аналізу збитків, що дозволяють обробляти великі обсяги виробничих даних, виявляти закономірності та над авати управлінські рекомендації . Кожен метод аналізу збитків"}
{"chunk_id": "2025_M_IUS_Shkredova_YeYa.pdf:2", "source": "2025_M_IUS_Shkredova_YeYa.pdf", "text": "збитків . У таких умовах важливо не лише фіксувати збитки, а й аналізувати їх причин и, щоб вчасно вживати заходи для їх усунення і запобігати повтор у аналогічних ситуацій у майбутньому. Це дозвол ить підприємству не лише зменшити фінансові витрати, а й підвищити загальну продуктив ність і конкурентоспроможність підприємства . Для реалізації цього завдання необхідне використ ання сучасних методів аналізу збитків, що дозволяють обробляти великі обсяги виробничих даних, виявляти закономірності та над авати управлінські рекомендації . Кожен метод аналізу збитків має свої переваги та обмеження, а тому вибір найбільш доцільного метод у для аналізу збитків є надзвичайно важливим завданням . Основна проблема полягає в тому, що у виробничому процесі можуть виникати різні типи збитків, кожен із яких потребує застосування окремого методу аналізу збитків . Оскільки у ніверсального методу аналізу збитків , який однаково ефективно працював би для всіх типів збитків , не існує , актуальним є розроб ка комбінованого методу аналізу 10 10 збитків , який враховує специфіку виробництва та підходить для кожного типу збитків . Метою дослідження є дослідження м етодів аналізу збитків для розробк и комбінованого методу аналізу збитків, що дозволить комплексно аналізувати різні типи збитків, виявляти причини їх виникнення для прийняття управлінських рішень та мінімізувати вплив збитків на фінансові втрати підприємст ва. Запропонований підхід передбачає розробку комбінован ого метод у аналізу збитків для формува ння відомості з результатами аналізу збитків за період , за типом збитків і за модел лю виробів і відомості з рекомендаціями щодо управління збитками . Автоматизація процесу аналізу збитків не лише скорочує витрати часу та зусиль фахівців, а й зменшує ймовірність помилок, спричинених суб’єктивністю людського аналізу. Це сприяє впровадженню прозорої підсистеми контролю та покращенню загального управління в иробництвом. Крім того, така підсистема може бути інтегрована у вже існуючі інфо рмаційні системи (ІС) підприємств , підвищуючи їхню функціона льність і адаптивність до змін. Результати цього дослідження можуть стати основою для створення універсального рішен ня з аналізу збитків, яке буде застосовним не лише у виробництві виробів із нержавіючої сталі, а й в інших галузях промисловості, де питання мінімізації збитків має ключове значення для забезпечення стабільного розвитку підприємства. 11 11 1 ОПИС ТА АНАЛІЗ ІСНУЮЧИХ МЕТОДІВ АНАЛІЗУ ЗБИТКІВ НА ВИРОБНИЦТВІ ТА ФОРМУЛЮВАННЯ ЗАДАЧІ ДОСЛІДЖЕННЯ 1.1 Дослідження процесу аналізу збитків на виробництві виробів із нержавіючої сталі «KREDO» Сучасне виробниче підприємство  це складна система, у якій поєднуються десятки паралельних процесів: від постачання сировини, обробки матеріалів і технічного контролю до зберігання готової продукції та логістики. На кожному з цих етапів можуть виникати збої, що призводять до фінансових втрат, і які мають назву «збитки» . У технічній термінології «збитки»  це втрати активів або грошей, обумовлені чиїмись протиправними діями. Збитки виникають унаслідок неефективної організації процесів, технічних несправностей , людських помилок, зовнішніх обставин або управлінських недоліків. У підприємництві під цим терміном часто розуміють витрати підприємства, що п еревищують фактичні доходи [1]. У сучасних умовах ринку, де посилюється конкуренція та щороку зростають вимоги до ефективності виробництва та мінімізації витрат, особливої уваги набуває аналіз збитків, що виникають у виробничій діяльності підприємств. Це має особливе значення для галузей, які працюють з високовартісними матеріалами, такими як нержавіюча сталь. Виробництво виробів із нержавіючої сталі є технологічно складним і багатоетапним процесом, який вимагає точності, дотримання стандартів якості та ефективної організації кожної операції [2]. Висока ціна матеріалів, вимоги до виробничих процесів зумовлюють те, що навіть короткий простій, дод атковий прохід шліфування або невдалий розкрій миттєво перетворюються на фінансові втрати. На кожному етапі виробництва  від постачання сировини до виготовлення готової продукції  можуть виникати різні типи збитків, які впливають на прибутковість виробництва. 12 12 У науковій літературі [3, 4] існує велика кількість підходів до класифікації збитків, які виникають у діяльності підприємств. Фундаментальний підхід до класифікації виробничих збитків у виробництві бере свій початок від роботи Сейічі Накадзіми , який визначив набір збитків, широко відомих як «шість великих збитків», що складають основу для метрики Overall Equipment Effectiveness (OEE ) [5]. «Шість великих збитків» включають незаплановані простої через несправність або аварії обладнання, планові з упинки через налаштування та планове технічне обслуговування обладнання, незначні зупинки та холостий хід, повільний цикл роботи обладнання через брудне або зношене обладнання, погане змащення обладнання, неякісні матеріали, дефекти виробничого процесу та зниження виходу продукції, що впливає на показники якості. Ці збитки групуються у три основні категорії, що відповідають компонентам OEE: доступність, продуктивність та якість. Подальші дослідження вказали на обмеженість класифікації шести основних типів з битків у відображенні всіх можливих збитків, що виникають у сучасних виробничих умовах. У статті [ 6] наголошується на включенні додаткових факторів, таких як час очікування вхідних матеріалів, зупинки виробничого потоку, людські помилки, помилки планування та проблеми з якістю, які традиційно не охоплюються шістьма основними категоріями. Отже, сьогодні не існує єдино ї класифікації виробничих збитків, яка повною мірою охоплюва ла різноманітність збитків на підприємстві. У різних дослідженнях та стандартах ви користовуються відмінні критерії класифікації, орієнтовані на конкретні галузі, типи продукції або бізнес - моделі. У контексті виробництва виробів із нержавіючої сталі збитки мають"}
{"chunk_id": "2025_M_IUS_Shkredova_YeYa.pdf:3", "source": "2025_M_IUS_Shkredova_YeYa.pdf", "text": "У статті [ 6] наголошується на включенні додаткових факторів, таких як час очікування вхідних матеріалів, зупинки виробничого потоку, людські помилки, помилки планування та проблеми з якістю, які традиційно не охоплюються шістьма основними категоріями. Отже, сьогодні не існує єдино ї класифікації виробничих збитків, яка повною мірою охоплюва ла різноманітність збитків на підприємстві. У різних дослідженнях та стандартах ви користовуються відмінні критерії класифікації, орієнтовані на конкретні галузі, типи продукції або бізнес - моделі. У контексті виробництва виробів із нержавіючої сталі збитки мають свою специфіку, обумовлену високою вартістю сировини, багатоступеневими техн ологічними процесами та підвищеними вимогами до якості. Зважаючи на це , у межах дослідження кваліфікаційної роботи пропонується використовувати таку класифікацію типів збитків, яка адаптована до особливостей підприємства, яке спеціалізується на 13 13 виготовленні продукції з нержавіючої сталі, та використ овується для подальшого аналізу збитків. В результаті дослідження існуючих класифікацій типів збитків виокремлено п’ять основних типів збитків, які найчастіше зустрічаються на виробництві виробів із н ержавіючої сталі та мають суттєвий вплив на фінансові втрати підприємства. До таких типів збитків було віднесено:  збитки від простою [ 7], які виникають унаслідок зупинки виробництва через несподівані несправності обладнання, перебої в електропостачанні, відсутність персоналу (через хворобу або інші обставини), а також затримки у постачанні сировини та через очікування попереднього етапу на виробництві;  збитки від втрат матеріалів, що обумовлені псуванням сировини, крадіжками, неправильним зберіганням або транспортуванням готової продукції, форс -мажорними обставинами [8], нераціональним використанням залишків або помилками у виробничих етапах (через людський фактор або через несправність обладнання);  збитки від браку [ 7, 9], які поділяють на технічно н еминучий невиправний брак та непередбачений внутрішній брак, що може виникнути через працівників підприємства (некваліфіковані дії персоналу, неуважність), постачальників матеріалів, унаслідок виробничих факторів (неточної роботи обладнання), форс -мажорних обставин;  збитки через логістичні проблеми, які пов’язані з порушеннями у ланцюгу постачання сировини, збоями у транспортуванні готової продукції, неефективним маршрутом перевезення або затримками на митниці;  збитки через перевиробництво [ 10], що виникають через неточне прогнозування попиту або потреби замовника або несвоєчасне виконання замовлення, через що продукція виявилася на складі. Документаційні збитки, що виникають унаслідок використання застарілої технічної документації, невідповід ності креслень фактичним параметрам продукції, помилок у розрахунках , та енерговтрати, що 14 14 зумовлені надмірним споживанням електроенергії через застаріле обладнання або людський фактор , не було розглянуто в кваліфікаційній роботі, бо ймовірність виникнення цих збитків є малою. Своєчасне виявлення збитків і аналіз їх причин дозволяє підприємству вчасно вживати заходів для їх усунення, зменшувати фінансові витрати та покращувати показники роботи. Одним із підприємств, яке активно впроваджує елементи автомати зованого аналізу збитків при розробці ІС виробництв а виробів із нержавіючої сталі , є компанія «KREDO». Підприємство спеціалізується на виготовленні професійних виробів із нержавіючої сталі , таких як сходи та сходові конструкції, перила для балконів і терас , меблі, рушникосушарки, дистилятори та інше обладнання. Організаційна структура компанії охоплює 8 відділів, а саме адміністрацію, відділ кадрів, відділ виробництва, відділ постачання та логістики, господарський відділ, відділ якості, відділ фінансів і відділ маркетингу та продажів. Адміністрація підприємства відповідає за стратегічне управління, ухвалення управлінських рішень, а також координацію роботи між усіма відділами. Відділ кадрів здійснює підбір, оформлення та супровід працівників, а також слідк ує за дотриманням норм трудового законодавства. Виробничий відділ безпосередньо відповідає за виготовлення продукції та веде облік збитків від простою, втрат матеріалів, браку та тих, що виникають через перевиробництво . Відділ постачання та логістики займ ається закупівлею сировини й комплектуючих, а також організовує транспортування матеріалів і готової продукції. Тут фіксуються логістичні збитки, затримки поставок або пошкодження під час транспортування. Господарський відділ забезпечує технічне обслуговув ання та функціонування інфраструктури підприємства. Відділ якості проводить вхідний і вихідний контроль продукції та реєструє випадки б раку . Після внесення даних начальниками відділів про збитки до бази даних (БД) підсистеми «Облік збитків» ІС підприємств а 15 15 фінансовий відділ здійснює розрахунок вартості збитків, формує підсумкові відомості про збитки за типами , а також бере участь у плануванні бюджету з урахуванням збитків . Відділ маркетингу та продажів опрацьовує зворотній зв’язок від клієнтів, який може б ути джерелом інформації про приховані або пізно виявлені дефекти. На підприємстві «KREDO» процес фіксації та аналізу збитків розподіллено між кількома відділами: відповідальні особи  начальники відділу виробництва, постачання та логістики, а також відділу якості  вносять дані про факти збитків до БД підсистеми «Облік збитків» ІС підприємства. Після збору даних фінансовий аналітик (фінансист) виконує аналіз даних про збитки , обирає не обхідний метод аналізу, проводить аналіз збитків і їх причин та формує відомості про збитк и, які надалі використовуються керівництвом для прийняття рішень щодо усунення причин збитків. Роль цього фахівця є ключовою, оскільки саме він на основі даних, внесе них начальниками в ідділів виробництва, постачання, логістики і якості, формує висновки щодо збитків і їх собівартості . Усі дані надходять до фінансиста у структурованому вигляді  за партіями виробів , з деталізацією моделей виробів, типу збитків, їх причин и, дати та вартості. На основі цієї інформації фінансист за допомогою програми Microsoft Excel застосовує різні методи"}
{"chunk_id": "2025_M_IUS_Shkredova_YeYa.pdf:4", "source": "2025_M_IUS_Shkredova_YeYa.pdf", "text": "збитк и, які надалі використовуються керівництвом для прийняття рішень щодо усунення причин збитків. Роль цього фахівця є ключовою, оскільки саме він на основі даних, внесе них начальниками в ідділів виробництва, постачання, логістики і якості, формує висновки щодо збитків і їх собівартості . Усі дані надходять до фінансиста у структурованому вигляді  за партіями виробів , з деталізацією моделей виробів, типу збитків, їх причин и, дати та вартості. На основі цієї інформації фінансист за допомогою програми Microsoft Excel застосовує різні методи аналізу для кожного типу збитків, що дозволяє врахувати специфіку їх виникнення та наслідків. Для збитків від простою та логістичних втр ат фінансист застосовує метод Парето [11], оскільки в цих випадках важливо швидко визначити, яка частка інцидентів є най витратн ішою. Метод дозволяє виділити критичні джерела проблем  певні ділянки виробництва, де найчастіше виникають затримки або конкретні етапи транспортування, на яких систематично відбуваються зриви постачань. При аналізі збитків від браку та вт рат матеріалів використовується когортний аналіз [12]. У цьому випадку фінансист групує дані за моделями виробів або за конкретними виробничими процесами, щоб простежити закономірності у виникненні 16 16 дефектів або пошкоджень. Такий підхід дозволяє виявити певний тип обладнання , пов’язаний з певним типом браку , або періоди наванта ження на склад, які призводять до втрат матеріалів. Для збитків через перевиробництво використовується аналіз динамічних рядів [13], що дозволяє оцінити обсяги накопичення готової продукції на складах та виявити періоди з найбільшою невідповідністю між попитом і фактичними виробничими обсягами. Після завершення аналізу збитків фінансист формує відомості з результатами аналізу збитків , що містять оцінку фінансових втрат через збитків за конкретний період , основні причини збитків для кожного типу збитків і для конкретної моделі виробів . На основі отриманих результатів аналізу збитків він переходить до етапу формування відомості з рекомендаціями . Для цього фінансист формує документ, у якому прописує рекомендації щодо управління збитк ами, використовуючи результати аналізу збитків. Рекомендації прописуються згідно з документом підприємства «Правила для написання рекомендацій» (додаток А ). Для бізнес -процесу «Аналіз збитків на виробництві виробів із нержавіючої сталі », що виконується на пі дприємстві «KREDO», створено візуальну модель за методологією структурного аналізу і проєктування Structured Analysis and Design Technique (SADT ) [14]. Контекстна модель бізнес -процесу представлена на рисунку 1.1 . Для реалізації бізнес -процесу «Аналіз збитків на виробництві виробів із нержавіючої сталі» враховано набір ключових вхідних даних, що забезпечують коректність і повноту аналізу. До них належать:  тип збитків, який дозволяє зосередити аналіз на певному типі збитків (наприклад, простої, брак то що);  період аналізу, що задає часові межі, в рамках яких здійснюється аналіз;  модель виробів, для якої треба визначити типи збитків та їх причини. 17 17 Рисунок 1.1 – Бізнес -процес «Аналіз збитків на виробництві виробів із нержавіючої сталі» (контекстна діаграма) Завершення процесу передбачає формування відомості з :  результатами аналізу збитків за період , в якій вказано звітн ій період, дату формування документа, користувача підсистеми , типи збитків, дати виникнення збитк ів, причин и типів збитків , вартість збитку і відсоткову частку , номери партій, що постраждали, відповідальних осіб, текстовий опис збитку та результати аналізу ;  результатами аналізу збитків за типом збитків , в якій містяться дані про тип збитків , звітн ій період, дату формування документа , користувача підсистеми , причини збитків і дати їх виникнення , відсоткові показники та вартість збитків, номери партій, що постраждали , відповідальних осіб, а також текстовий опис збитку та результати аналізу;  результатами аналізу збитків за моделлю виробів , в якій вказано дані про модель виробів , дату формування документа, користувача підсистеми, типи та причини збитків, дати виникнення збитку, відсоткові показники збитків , вартість збитків, показник постраждалих виробів, відповідальних осіб, текстовий опис збитку та результати аналізу ;  рекомендаціями щодо управління збитк ами, яка містить 18 18 рекомендації , сформованих за результатами аналізу збитків, і є основою для подальших управлінських рішень . На рисунку 1.2 наведено діаграму декомпозиці ї першого рівня бізнес - процесу «Аналіз збитків на виробництві виробів із нержавіючої сталі» . Такий підхід, коли для кожного типу збитків фінансист застосовує окремий метод аналізу, здається логічним і ефективним, адже дозволяє використовувати методи, адаптовані до специфіки кожного типу збитків. Однак на практиці він має низку суттєвих недоліків, що обмежують його універсальність і знижують якість та точність роботи з даними про збитки. По-перше, використання різних методів для кожного типу збитків ускладнює уніфікацію процесу аналізу. Фінансисту доводиться постійно перемикатися між різними алгоритмами, форматами обробки даних і підходами до візуалізації результатів. Це збільшує часові витрати та підвищує ризик виникнення помилок через відсутність єдин ого стандарту. По-друге, такий підхід не завжди дозволяє якісно обробляти комплексні випадки, коли один інцидент охоплює кілька типів збитків одночасно. Наприклад, затримка транспортування (логістичні проблеми ) може призвести до простою виробничого процесу , що формально належить до іншого типу збитків. Окремий аналіз таких ситуацій за різними методами призводить до фрагментації інформації та ускладнює формування цілісної картини щодо збитків на виробництві. Також, обираючи метод аналізу, фінансист може сфок усуватися"}
{"chunk_id": "2025_M_IUS_Shkredova_YeYa.pdf:5", "source": "2025_M_IUS_Shkredova_YeYa.pdf", "text": "Це збільшує часові витрати та підвищує ризик виникнення помилок через відсутність єдин ого стандарту. По-друге, такий підхід не завжди дозволяє якісно обробляти комплексні випадки, коли один інцидент охоплює кілька типів збитків одночасно. Наприклад, затримка транспортування (логістичні проблеми ) може призвести до простою виробничого процесу , що формально належить до іншого типу збитків. Окремий аналіз таких ситуацій за різними методами призводить до фрагментації інформації та ускладнює формування цілісної картини щодо збитків на виробництві. Також, обираючи метод аналізу, фінансист може сфок усуватися на певному типі збитків, який видається йому найбільш значущим, але згодом з’ясовується, що цей тип збитків складає лише незначну частку у загальному обсязі збитків. Це призводить до неефективного використання трудових ресурсів і затримує прийнят тя управлінських рішень, спрямованих на усунення основних причин збитків. . 19 19 Рисунок 1.2 – Бізнес -процес «Аналіз збитків на виробництві виробів із нержавіючої сталі» (декомпозиція першого рівня) 20 По-третє, різнорідність методів створює додаткові витрати часу на навчання персоналу. Щоб упевнено працювати з Microsoft Excel, фінансист має володіти широким спектром навичок  від побудови діаграм Парето д о глибинного когортного аналізу або аналізу динамічних рядів . Для невеликих підприємств це може бути надмірним і ресурсовитратним . Крім того, такий підхід не дозволяє автоматизувати аналіз збитків у повному обсязі. Кожен метод вимагає своєї логіки обробки даних, що ускладнює створення єдиної програмної реалізації та інтеграції до ІС підприємства. Як наслідок, підприємство залишається залежним від ручної роботи фінансиста та його індивідуального досвіду. Саме тому виникає потреба у створенні комбінованого методу аналізу збитків, який поєдну є сильні сторони існуючих методів, але водночас ма є єдину структуру обробки даних. Такий метод дозволить застосовувати його до всіх типів збитків, забезпечуючи єдиний алгоритм групування та аналізу збитків , а також можлив ість інтеграції з ІС підприємства , що розроблюється . У результаті фінансист отримає універсальний метод , що скорочує час аналізу, зменшує вплив людського фактору та підвищує точність прийняття управлінських рішень. 1.2 Опис проблеми вибору методу аналізу збитків на виробництві На виробництві виробів із нержавіючої сталі аналіз збитків є одним із ключових етапів контролю роботи виробництва та прийняття управлінських рішень. Відділ фінансів, який відповідає за облік та аналіз збитків, отримує дані від керівників підрозділів (виробничого, логістичного і постачання, контролю якості), що фіксують випадки збитків , дефектів або відхилень від плану у БД підсистеми «Облік збитків» ІС підприємства . Наразі вибір методу аналізу здійснює ться вручну  на підставі досвіду фінансиста, 21 специфіки ситуації та типу збитків . Такий підхід ґрунтується на індивідуальній оцінці й передбачає ви користання найпростіших методів . Однак у процесі аналізу збитків виникає проблема вибору методу аналізу збитк ів, бо існує ве лика кількість різних методів , кожен з яких має свої переваги та недоліки залежно від типу збитків , структури й повноти даних, особливостей технологічного процесу та організації виробництва. У реальних умовах підприємства збитки можуть бути пов’язані з різними моделями виробів, кваліфікацією та діями окремих працівників, етапами технологічного ланцюжка (зварювання, складання, сортування, контроль якості тощо), а також із різними типами збитків. Це призводить до того, що вибір методу аналізу з биткі в, який забезпечить найточніші та найстабільніші результати в конкретній ситуації, стає складним завданням, особливо коли необхідно опрацьовувати великі масиви даних із багатьма ознаками та взаємозв’язками. Ручний вибір методу аналізу збитків має низку недоліків:  суб’єктивність прийняття рішень, бо результати аналізу залежать від інтуїції та досвіду фінансиста, що може призво дити до викривлень у висновках;  трудомісткість процесу через те, що для кожного типу збитк ів фінансист у необхідно вручну порівнювати декілька варіантів і приймати рішення;  зниження точності при узагальненому підході , бо використання одного методу аналізу для всіх випадків може дати прийнятні результати в одній ситуації, але бути неефективним для іншої, наприклад, аналіз пр остоїв і браку потребує різних методів аналізу . У зв’язку з цим постає завдання розробки комбінованого методу аналізу збитків , який враховував специфіку конкретного виробництва, типи збитків, структуру наявних даних та можливості їх подальшого опрацювання . Автоматизований аналіз збитків на виробництві дозволить знизити ризик помилок, пов’язаних із людським фактором , значно 22 прискорити процес формування відомостей за рахунок автоматичного опрацювання великих масивів даних, підвищити якість результатів аналізу завдяки застосуванню комбінованого методу аналізу збитків, адаптованого до конкретних умов виробництва, та надати керівництву об’єктивні та обґрунтовані рекомендації щодо управління збитками . 1.3 Аналіз існуючих ІС аналізу збитків на виробництв і У сучасних умовах впровадження цифрових технологій у промислов і підприємств а дедалі більшого значення набувають ІС, що забезпечують підтримку управлінських рішень на основі аналізу даних. Одним із напрямів застосування таких систем є аналіз збитків, що виникають у процесі виробництва. Враховуючи складність технологій виготовлення виробів із нержавіючої сталі, багатофа кторність причин збитків і високу вартість сировини, аналіз збитків є критично важливою умовою для зменшення фінансових витрат підприємства. Існує низка програмних продуктів, які частково або повністю реалізують функції збирання, фіксації, обліку та аналіз у збитків. Серед таких систем можна вид ілити Quality Management Systems (QMS ) [15, 16], System Analysis Program Development (SAP) Enterprise Resource Planning (ERP)"}
{"chunk_id": "2025_M_IUS_Shkredova_YeYa.pdf:6", "source": "2025_M_IUS_Shkredova_YeYa.pdf", "text": "Одним із напрямів застосування таких систем є аналіз збитків, що виникають у процесі виробництва. Враховуючи складність технологій виготовлення виробів із нержавіючої сталі, багатофа кторність причин збитків і високу вартість сировини, аналіз збитків є критично важливою умовою для зменшення фінансових витрат підприємства. Існує низка програмних продуктів, які частково або повністю реалізують функції збирання, фіксації, обліку та аналіз у збитків. Серед таких систем можна вид ілити Quality Management Systems (QMS ) [15, 16], System Analysis Program Development (SAP) Enterprise Resource Planning (ERP) [17], Siemens Teamcenter та інші системи, що містять функціонал для обробки інформації про збитки , контроль якості та підтримку прийняття рішень на основі аналізу збитків. В таблиці 1.1 наведено порівняння ІС QMS , SAP ERP та Siemens Teamcenter за критеріями  опис системи, наявність автоматизації аналізу, переваги та недоліки . 23 Таблиця 1.1 – Порівняння ІС аналізу збитків Назва системи Критерії порівняння Характеристика 1 2 3 QMS Опис системи QMS  це система управління якістю, яка забезпечує моніторинг невідповідностей, контроль дефектів, управління аудитами та запобіжними заходами. Наявність автоматизації аналізу QMS підтримує інструменти для аналізу причин дефектів (Fishbone, 5Why), генерацію звітів, контрольні карти та моніторинг показників якості. Переваги QMS надає деталізований контроль якості, наочність у візуалізації причинно -наслідкових зв’язків. Недоліки Недоліками QMS є обмежена інтеграція з фінансовими модулями та відсутність врахування економічної оцінки збитків. SAP ERP Опис системи SAP ERP  це програмне забезпечення (ПЗ) для управління бізнес -процесами підприємства, що охоплює фінансовий облік, управління виробництвом, складом, постачанням, а також модулі для моніторингу та аналізу збитків. Наявність автоматизації аналізу SAP ERP забезпечує частков е автоматичне збирання та обробляння даних про виробничі збитки, формування звіти за типами, причинами та місцями виникнення збитків, інтегрування результатів з фінансовими показниками. Переваги До переваг SAP ERP можна віднести інтеграцію з усіма бізнес -процесами, модульність і можливість масштабування. Недоліки Недоліками SAP ERP є висока вартість впровадження та обслуговування, значні вимоги до кваліфікації персоналу, складність налаштування під специфічні процеси. Siemens Teamcenter Опис системи Siemens Teamcenter  програмна платформа для управління повним життєвим циклом продукції, яка включає модулі для управління якістю, документацією, процесами та виробничими даними, а також інструменти для контролю дефектів і відстеження їх причин. 24 Кінець таблиці 1.1 1 2 3 Siemens Teamcenter Наявність автоматизації аналізу Siemens Teamcenter підтримує автоматичне збирання даних із виробничого обладнання, модулі для аналізу невідповідностей, побудову діаграм та інтеграцію з системами автоматизованого проєктування та комп’ютерного інженерного аналізу. Переваги Перевагами Siemens Teamcenter є інтеграція з інженерними системами, висока точність даних, підтримка складних виробничих процесів, можливість проведення аналізу ще на етапі проєктування. Недо ліки Недоліками Siemens Teamcenter є висока вартість ліцензії та впровадження, складність налаштування для невеликих підприємств. Порівняльна таблиця 1.1 надає комплексний огляд трьох провідних ІС, що використовуються для аналізу збитків на виробництві: QMS, SAP ERP та Siemens Teamcenter. Вибір найкращого рішення для аналізу збитків на виробництві залежить від масштабів підприємства, його бюджету, вимог до аналітики, інтеграції та гнучкості. SAP ERP є універсальним рішенням для великих і середніх підприємств, яке забезпечує повну інтеграцію бізнес - процесів. QMS ефективна у випадках, коли акцент робиться на якості продукції та відповідності стандартам, однак не вирішує завдань глибокого аналізу збитків. У свою чергу, Siemens Teamcenter демонструє найвищий рівень інтеграції та точності, проте потребує значних ресурсів для впровадження та обслуговування. Незважаючи на переваги кож ної з систем, жодна з них не врахову є специфіку різних типів збитків : простоїв, браку, втрат матеріалів , логістичних проблем і перевиробництва . Крім того, існуючі рішення не враховують детальні виробничі особливості, наприклад  маркування партій, зв’язок із конкретними етапами виробництва або відповідальними особами. У зв’язку з цим розробка власного комбінованого методу є 25 доцільною та обґрунтованою. Такий підхід дозволить врахувати як виробничу специфіку підприємства, так і потребу в гнучкому, адаптивному підході до аналізу збитків. 1.4 Огляд існуючих методів аналізу збитків для виробництва виробів із нержавіючої сталі На сучасних виробничих підприємствах управління витратами є неможливим без системного аналізу збитків. Такий аналіз дозволяє виявити проблемні ділянки виробничого процесу, зменшити кількість дефектів, поліпшити логістичні ланцюги та підвищити відповідальність працівників. Проте існує велика кількість методів , що використовуються для аналізу збитків, кожен із яких орієнтований на певний тип збитків або має обмеження у застосуванні в контексті конкретного виробництва. Серед найбільш поширених методів аналізу збитків можна виділити ABC -аналіз, метод Парето та ко гортний аналіз. Кожен із них має переваги, але й обмеження, які знижують точність аналізу збитків у реальних умовах. ABC -аналіз застосовується для класифікації об’єктів за їхнім внеском у загальний результат [1 8, 19]. У контексті аналізу збитків він дозвол яє розподілити всі типи збитків на три групи, а саме А  найзначущі збитки, що формують основну частину загальних втрат, B  середньої важливості, C  незначні, але численні. Хоча метод дає змогу швидко визначити пріоритетні збитки для зниження фінансових втрат, його недоліком є високий рівень узагальнення. ABC -аналіз не показує причин виникнення збитків і не враховує взаємозв’язки між типами. У результаті підприємство може помилково зосередити ресурси на усуненні не найвитратніших збитках. Метод Парето ґру нтується"}
{"chunk_id": "2025_M_IUS_Shkredova_YeYa.pdf:7", "source": "2025_M_IUS_Shkredova_YeYa.pdf", "text": "аналізу збитків він дозвол яє розподілити всі типи збитків на три групи, а саме А  найзначущі збитки, що формують основну частину загальних втрат, B  середньої важливості, C  незначні, але численні. Хоча метод дає змогу швидко визначити пріоритетні збитки для зниження фінансових втрат, його недоліком є високий рівень узагальнення. ABC -аналіз не показує причин виникнення збитків і не враховує взаємозв’язки між типами. У результаті підприємство може помилково зосередити ресурси на усуненні не найвитратніших збитках. Метод Парето ґру нтується на відомому принципі «80/20 », відповідно 26 до якого близько 80 % наслідків зумовлюються 20 % причин [20]. У контексті збитків на виробництві цей метод використовується для визначення найвитратніших типів збитків, які формують основну частину загальн их фінансових втрат підприємства. Основна перевага методу Парето  це його наочність і простота застосування. Результати можна представити у вигляді діаграми Парето, що дозволяє легко визначити найвитратніші збитки . Обмеження методу полягає в тому, що він показує лише розподіл за значимістю збитків , не розкриваючи глибинних причин виникнення збитків. На відміну від попередніх методів к огортний аналіз застосовується для вивчення поведінки груп даних, які об’єднуються за певною ознакою (когортою) [12, 21]. У випадку виробництва виробів із нержавіючої сталі такими когортами можуть бути моделі виробів, причини збитків, відповідальні працівники або постачальники . Метод дозволяє виявляти закономірності: наприклад, що певний тип браку виникає переважно у конкретній зміні або що втрати матеріалів зростають при роботі з конкретним постачальником. Водночас , когортний аналіз не завжди забезпечує швидке визначення найвитратніших типів збитків, оскільки фокусується більше на їхній структурі та динаміці, ніж на фінансових втратах. Його недоліком є складність інтерпретації через велик у кількість когорт. Отже, кожен із розглянутих методів забезпечує лише часткове розв’язання задачі аналізу збитків і не дає змоги комплексно охопити всі аспекти формування збитків на виробництві. ABC -аналіз та метод Парето ефективні для попереднього ранжування збитків за рівнем їх ньої значущості та виділення найвитратніших типів збитків, однак вони майже не враховують причинно -наслідкові зв’язки й специфіку технологічних процесів. Натомість когортний аналіз дозволяє глибше дослідити причини збитків, пов’язати їх із конкретними моде лями виробів, працівниками, постачальниками чи робочими змінами, але потребує значних аналітичних зусиль і не забезпечує ідентифікацію найвитратніших типів збитків . 27 У реальних умовах виробництва, де збитки утворюються під впливом багатьох факторів , застосу вання одного методу аналізу збитків призводить до неякісних результатів аналізу. Усі розглянуті методи розв’язують окремі задачі, але не забезпечують комплексного підходу до аналізу збитків на виробництві виробів із нержавіючої сталі. Це підкреслює необхід ність розробки комбінованого методу аналізу збитків , який інтегруватиме переваги існуючих методів, адаптуватиме їх під специфіку типів збитків та дозволить формувати уніфіковані відомості в межах розроблюваної ІС підприємства. 1.5 Висновки та ф ормування задачі дослідження Проведений у першому розділі аналіз виробництва виробів із нержавіючої сталі та огляд існуючих ІС аналізу збитків і методів аналізу збитків показав, що на підприємствах відсутній єдиний підхід, здатний комплексно враховувати особливості різних типів збитків. Існуючі рішення часто орієнтовані на один або декілька типів збитків або надають результати, що не враховують багатофакто рну природу причин їх виникнення. Додатковою складністю є необхідність роботи з даними моделей виробів, які містять різні за структурою відомості  від характеристик моделі виробів до детальної інформації про етапи виробництва та відповідальних осіб. Об’єк том дослідження є процес аналізу збитків на виробництві виробів із нержавіючої сталі. Предметом дослідження є методи аналізу збитків із викорис танням даних виробничих партій. Мета роботи полягає у дослідженн і методів аналізу збитків для розробк и комбінованого методу аналізу збитків, що дозволить комплексно 28 аналізувати різні типи збитків, виявляти причини їх виникнення для прийняття управлінських рішень та мінімізувати вплив збитків на фінансові втрати підприємст ва. Комбінований метод дозволить пі дприємству своєчасно ідентифікувати критичні зони ризику виникнення збитків, мінімізувати їхній вплив на фінансові втрати підприємства та забезпечити обґрунтоване прийняття управлінських рішень щодо усунення причин збитків. Для досягнення мети у кваліфікац ійній роботі необхідно виконати такі завдання дослідження : – аналіз існуючих методів вирішення задачі аналізу збитків; – розробка комбінованого методу вирішення задачі аналізу збитків; – розроб ка практичн ої реалізац ії вирішення задачі аналізу збитків ; – прове дення експериментальн ої перевірк и отриманих результатів дослідження та порівня ння запропонован ого комбінованого методу анал ізу збитків з існуючим. 29 2 ДОСЛІДЖЕННЯ МЕТОД ІВ АНАЛІЗУ ЗБИТКІВ ПРИ РОЗРОБЦІ ІС ВИРОБНИЦТВА ВИР ОБІВ ІЗ НЕРЖАВІЮЧОЇ СТАЛІ 2.1 Аналіз методів вирішення задачі аналізу збитків Аналіз збитків на виробництві є одним із ключових завдань для забезпечення конкурентоспроможності підприємства. Для дослідження основних типів збитків і причин їх виникнення використовуються різні методи аналізу . Серед основних методів аналізу збитків можна виділити метод Парето та когортний аналіз, які в ідзначаються універсальністю у різних сферах, зокрема у промисловості, економіці та бізнес -аналітиці. Метод Парето ґрунтується на відкритті італійськ ого еко номіста Вільфредо Парето . Він встановив, що приблизно 80 % земель в Італії належали лише 20 % населення [22]. Американський спеціаліст з управління якістю Джозеф Джуран узагальнив цю ідею у сфері менеджменту, ввівши термін «правило 80/20» [23]."}
{"chunk_id": "2025_M_IUS_Shkredova_YeYa.pdf:8", "source": "2025_M_IUS_Shkredova_YeYa.pdf", "text": "і причин їх виникнення використовуються різні методи аналізу . Серед основних методів аналізу збитків можна виділити метод Парето та когортний аналіз, які в ідзначаються універсальністю у різних сферах, зокрема у промисловості, економіці та бізнес -аналітиці. Метод Парето ґрунтується на відкритті італійськ ого еко номіста Вільфредо Парето . Він встановив, що приблизно 80 % земель в Італії належали лише 20 % населення [22]. Американський спеціаліст з управління якістю Джозеф Джуран узагальнив цю ідею у сфері менеджменту, ввівши термін «правило 80/20» [23]. Він показав, що у виробництві приблизно 80 % дефектів зумовлені 20 % причин. На виробництві основна мета методу Парета полягає у виявленні ключових типів збитків, на які необхідно спрямувати увагу керівництва для прийняття управлінських рішень . На першо му е тапі методу Парето дані сортуються за вартістю збитків за спаданням. Після чого для кожного типу збитків розраховується його частка у загальній сумі збитків за формулою: 𝑆𝑗=𝐶𝑗 ∑ 𝐶𝑗𝑘 𝑗=1×100 , (2.1) де Sj  частка j-го типу збитків у загальн ій сумі збитків, %; Сj  вартість j-го типу збитків , грн; k  кількість типів збитків. Після обчислення часток кожного типу збитків розраховується їх ня 30 кумулятивна частка , що дозволяє визначити сукупний вплив основних типів збитків . Для цього використовується формула кумулятивної частки: 𝑃𝑗=𝑃𝑗−1+𝑆𝑗,𝑗=1,2,…,𝑘, 𝑃0=0, (2.2) де Pj  кумулятивна частка j-го типу збитків, %; Sj  частка j-го типу збитків , %; k  кількість типів збитків . Наступним етапом застосовується правило відбору, що базується на пороговому значенні ( 80%). Правило відбору найвитратніши х типів збитків полягає в тому, що до переліку включаються всі типи збитків, поки їхня кумулятивна частка не досягне встановленого довірчого інтервалу порогового значення [L, T]. Оптимальним індексом j* є індекс , при якому кумулятивна частка Pj* потрапляє в довірчий інтервал порогового значення [L, T] і при цьому є мінімальною (Pj* перетнуло нижню межу L). Таким чином, до переліку основних типів збитків включаються всі типи збитків від першого до j*-го. Це дозволяє виділити основні типи збитків, які справляють найбільший вплив на фінансов і втрат и підприємства. Правил о відбору найвитратніших типів збитків представлено формулою : 𝑗∗=𝑎𝑟𝑔 𝑚𝑖𝑛 { 𝑃𝑗∣𝐿≤𝑃𝑗≤𝑇}, (2.3) де j* індекс найвитратн іших типів збитків ; Pj  кумулятивна частка j-го типу збитків, %; L  нижня межа порогового значення, %; T  верхня межа порогового значення, %. Результати методу візуалізуються у вигляді діаграми Парето, що показує розподіл збитків від найвитратн іших до менш витра тних і демонструє відповідну кумулятивну криву збитків . Обмеження методу 31 полягає в тому, що він показує лише основні типи збитків , але не розкриває внутрішніх причин збитків. Когортний аналіз є методом, що знайшов широке застосування у різних сферах  від медиц ини та демографії до бізнес -аналітики та промисловості. У бізнес -аналітиці метод був адаптований для дослідження поведінки клієнтів у часі [21]. В подальшому його почали застосовувати й у промисловості, де когортами можуть виступати партії виробів , групи обладнання або виробничі процеси . Суть когортного аналізу полягає у поділі загальної вибірки даних на окремі групи (когорти) за спільною ознакою . У технічному описі алгоритму когортного аналізу [24] для виробництва виробів із нержавіючої стал і можн а виділити кілька етапів:  визнач ення критерію формування когорт;  обчислення розміру кожної когорти;  визначення періоду звітності;  розрахунок метрик для кожної когорти . На етапі визначення критерію формування когорт визначається, за якою ознакою будуть об’єднані дані. У випадку аналізу збитків ознаками можуть бути причин и збитк ів (несправність обладнання , відсутність персоналу , помилк и у виробн ичому етапі тощо ), номер партії виробів або моделі виробів . Наприклад, якщо когорта визначається за моделлю виробів , то всі збитки, що стосуються цієї моделі , об’єднуються в одну когорту . Обчи слення розміру кожної когорти  це етап, коли д ля кожної когорти визначається її кількість або обсяг. У випадку з виробництвом це може бути кіль кість інцидентів збитків , що відбулися в межах когорти, або загальна вартість цих збитків. Наприклад, когорта « несправність обладнання » може містити 3 випадк и із загальною вартістю 7 тисяч гривень . На н аступному етапі визначається період звітності, за який здійснюється аналіз когорт. Це може бути місяць, квартал , рік або інший період залежно від потреб підприємства. Визначення періоду дає змогу 32 простежити динаміку появи збитків у часі . Наприклад, повторюється певна причина збитків упродовж кількох місяців або вона є поодиноким випадком. Для виробництва виробів із нержавіючої сталі це особливо важливо, адже певні проблеми можуть мати сезонний характер (наприклад, перебої з постачанням у зимовий пері од) або бути пов’язаними з виробничими етапами . Останнім етапом є розрахунок метрик для кожної когорти. До таких метрик можуть належати сумарна вартість збитків у межах когорти, частка збитків ко горти та інше . Наприклад, якщо аналізується когорта «Рушникос ушарка Змійовик ө 33 », то визначає ться сумарна вартість збитків по цій моделі виробів , розраховує ться частка причин збитків (у межах когорти за формулою 2.1). Когортний"}
{"chunk_id": "2025_M_IUS_Shkredova_YeYa.pdf:9", "source": "2025_M_IUS_Shkredova_YeYa.pdf", "text": "певні проблеми можуть мати сезонний характер (наприклад, перебої з постачанням у зимовий пері од) або бути пов’язаними з виробничими етапами . Останнім етапом є розрахунок метрик для кожної когорти. До таких метрик можуть належати сумарна вартість збитків у межах когорти, частка збитків ко горти та інше . Наприклад, якщо аналізується когорта «Рушникос ушарка Змійовик ө 33 », то визначає ться сумарна вартість збитків по цій моделі виробів , розраховує ться частка причин збитків (у межах когорти за формулою 2.1). Когортний аналіз на виробництві виробів із нержавіючої сталі дозволяє не лише групувати дані за ключовими ознаками, але й виявляти закономірності та порівнювати вплив різних збитків на фінансові втрати підприємства. Це створює підґрунтя для ухвалення управлінських рішень, спрямованих на зменшення повторюваних збиткі в і підвищення стабільності виробничих процесів. Отже , метод Парето забезпечує виявлення найкритичн іших типів збитків, а когортний аналіз дозволяє дослідити їх причини . Проте використання цих методів окремо не забезпечує комплексного підходу для аналізу збитків , що підкреслює доцільність розробки комбінованого методу. 2.2 Розробка комбінованого методу для вирішення задачі аналізу збитків Комбінований метод  це метод, що передбачає інтеграцію кількох окремих методів аналізу в єдиний метод для досягнення більш 33 комплексного та об’єктивного результату аналізу. Необхідність у комбінованому методі зумовлена тим, що окремі методи аналізу збитків дають лише часткове уявлення про ситуацію. Комбінований метод інтегрує різні метод и аналізу у єдин ий метод і забезпечує комплексний підхід д ля аналізу збитків. Основна перевага комбінованого методу полягає у тому, що він дає можливість виділити найкритичніші типи збитків і деталізувати причини їх виникнення. Р езультати аналізу збитків комбінованим методо м перетворюються на практичні рекомендації для прийняття управлінських рішень . На першому етапі дані про збитки спочатку групуються за типами збитків, після чого сортуються у порядку спадання вартості збитк ів за формулою: 𝐶𝑗≥𝐶𝑗+1,𝑗=1,2,…,𝑘, (2.4) де C j – вартість j-го типу збитків, грн; k – кількість типів збитків. Далі розраховується частка кожного типу збитків за формулою 2.1. Потім дані відсортовуються за бінарним показником пріоритету Zj кожного типу збитків. Бінарний показник пріоритету Zj має значення 1 для керованих збитків та 0 для некерованих або форс -мажорних збитків . Таким чином, якщо частки Sj двох або більше типів збитків є однаковими, пріоритет надається типу збитків з вищим пок азником Zj. Це гарантує, що типи збитків, на які підприємство може вплинути, раніше увійдуть до кумулятивної частки. Після відсортування даних розраховується кумулятивна частка кожного типу збитків , що дозволяє визначити сукупний вплив основних типів збитків. Для цього використовується формула кумулятивної частки 2.2. Далі застосовується правило відбору найв итратн іших типів 34 збитків за формулою 2.3 та довірчого інтервалу порогового значення [70%; 90%]. Для методу Парето було використано не фіксоване значення 80%, а довірчий інтервал від 70% до 90 %, оскільки співвідношення «80/20» є емпіричним і може змінюватися залежно від специфіки процесів на підприємстві. Такий інтервал дозволяє точніше відобразити реальний розподіл збитків у конкретних умовах виробн ицтва [25, 26]. На другому етапі для кожного з найвитратніших типів збитків , які були визначенні на попередньому етапі, застосовується когортний аналіз. У межах цієї роботи когортний аналіз модифіковано для аналізу причин найвитратніших типів збитків і для аналізу збитків для конкретної моделі виробів. У першому випадку к огортами виступають причини збитків конкретного типу збитків . Усі збитки з однаковою причиною об’єднуються в одну когорту. Для кожної причини підраховується кількість інц идентів та їх з агальна вартість . Період звітності зазначається підприємством. Метриками для когорт при аналізі причин вист упають дати виникнення збитк ів, частка причини збитк ів від загальної вартості конкретного типу збитк ів. При розрахунках частк и причин збитк ів використовується формул а 2.1. Такий підхід дозволяє виявити приховані закономірності, наприклад, що більшість простоїв виникають через несправності конкретного обладнання. У другому випадку когортами виступають моделі виробів. Усі дані про збитки групуються за моделями виробів , що дозволяє визначити основні типи та причини збитків для кожної моделі виробів . Для кожної моделі визначається кількість інцидентів причин збитків та їхня сукупна вартість. Моделі виробів можуть аналізуватися у розрізі міся ців або кварталів, що дозволяє визначати найбільш проблемні проміжки часу. Метриками для когорт при аналізі моделі виробів є дати виникнення збитку та частка причин збитк ів від загальної вартості всіх типів збитків для конкретної моделі виробів. 35 Завершальним етапом є формування рекомендацій за результат ами аналізу збитків комбінованим метод ом. Принцип формування рекомендацій побудован о за правилом «якщо -то»: для кожного виявленого випадку збитків визначається відповідна управлінська рекомендація. Таким чином, комбінований метод працює за принципом послідовної інтегра ції:  етап 1 є застосування метод у Парето для визначення найвитратніших типів збитків;  етап 2 є використання модифікованого когортного аналізу для деталізації причин збитків для найвитратніших типів збитків і для оцінювання збитків для конкретної моделі виробів; Додатково відбувається обробка результатів аналізу збитків на основі правил з таблиці рішень (Decision Table ) для форму вання рекомендаці й щодо управління збитками . Запропо нований комбінований метод усуває основні недоліки окремих підходів і дозволяє створити уніфіковану процедуру"}
{"chunk_id": "2025_M_IUS_Shkredova_YeYa.pdf:10", "source": "2025_M_IUS_Shkredova_YeYa.pdf", "text": "за принципом послідовної інтегра ції:  етап 1 є застосування метод у Парето для визначення найвитратніших типів збитків;  етап 2 є використання модифікованого когортного аналізу для деталізації причин збитків для найвитратніших типів збитків і для оцінювання збитків для конкретної моделі виробів; Додатково відбувається обробка результатів аналізу збитків на основі правил з таблиці рішень (Decision Table ) для форму вання рекомендаці й щодо управління збитками . Запропо нований комбінований метод усуває основні недоліки окремих підходів і дозволяє створити уніфіковану процедуру аналізу збитків для всіх їх ніх типів. Інноваційним є поєднання результатів методу Парето з когортним аналізом для формування рекомендацій. 2.3 Висновки до другого розділу У ході роботи було проведено огляд методів , що застосовуються для вирішення задачі аналізу збитків на виробництві виробів із нержавіючої сталі. Розглянуто метод Парето та когортний аналіз, які продемонстрували точність у виявл енні основних типів збитків, визначенні їх ніх причин і трансформації результатів у практичні управлінські рекомендації . Метод Парето дозволив в ідокремити найвитратніш і типи збитків, що формують значну частку загальних збитків . Когортний аналіз у 36 модифікова ному вигляді був використаний для деталізації причин збитків і аналізу збитків за моделюю виробів . Водночас результати дослідження показали, що використання кожного методу окремо має свої обмеження , тому жоден із методів не забезпечує комплексн ого підходу до аналізу збитків. Запропонований комбінований метод вирішує проблему застосування кожного методу окремо завдяки інтеграції розглянутих методів у єдиний метод . Він забезпечує уніфікацію обробки даних, підвищує точність результатів аналізу збитків , зменшує залежність від суб’єктивності фінансиста та надає можливість формування відомостей, що є основою для прийняття управлінських рішень. Таким чином, результати другого розділу підтверджують доцільність розробки комбінованого методу. 37 3 ІНФОРМАЦІЙНА ТЕХНОЛОГІЯ ДОСЛІДЖЕН НЯ МЕТОДІВ АНАЛІЗУ ЗБИТКІВ ПРИ РОЗРОБЦІ ІС ВИРОБНИЦТВА ВИРОБІВ ІЗ НЕРЖАВІЮЧОЇ СТАЛІ 3.1 Розробка підсистеми «Аналіз збитків на виробництві виробів із нержавіючої сталі» з використанням комбінованого методу Для підприємства «KREDO» розробка підсистеми «Аналіз збитків на виробництві виробів із нержавіючої сталі» є важливим етапом у підвищенні точності та якості аналізу збитків та вдосконаленні бізнес -процесів підприємства. Існуючий процес аналізу збитків, що б азується на ручних розрахунках фінансиста за допомогою електронних таблиць Microsoft Excel, є трудомістким, фрагментарним і залежним від суб’єктивного підходу. Тому виникає потреба у розробці підсистеми , яка автоматизує розрахунки за допомогою комбінованог о методу та надасть готові управлінські рекомендації для керівництва. Підсистема «Аналіз збитків на виробництві виробів із нержавіючої сталі» складається з 4 ключових модулів, кожен із яких виконує важливу роль у забезпеченні комплексного аналізу збитків. Перелік модулів та їхній опис наведено у таблиці 3.1. Таблиця 3.1 – Опис модулів підсистеми «Аналіз збитків на виробництві виробів із нержавіючої сталі» Назва модуля Опис функціоналу модуля 1 2 Модуль збору та обробки даних Забезпечу ється імпорт даних із БД підсистеми «Облік збитків» ІС підприємства та їх перевірку на коректність. Модуль аналізу збитків Реалізу ється метод Парето та когортний аналіз, формує проміжні результати аналізу збитків (діаграми, таблиці, відсоткові співвідношення). 38 Кінець таблиці 3.1 1 2 Модуль рекомендацій Використовуючи правила рекомендацій з Decision Table у Excel, автоматично формуються текстові рекомендації для керівництва за результатами аналізу. Модуль виведення результатів Візуалізація результатів у графіках та таблицях, формування відо мостей для користувачів. Першим модулем є модуль збору та обробки даних, який відповідає за імпорт усіх необхідних даних із БД підсистеми «Облік збитків» ІС підприємства. На цьому етапі здійснюється перевірка коректності даних, усунення можливих пропусків і помилок, а також попереднє структурування інформації за моделями виробів та типами збитків. Висока якість вхідних даних є критичною умовою для подальшог о аналізу. Другим модулем підсистеми виступає модуль комбінованого методу, який реалізує поєднання методу Парето та когортного аналізу. На основі методу Парето відбираються найвитратніш і типи збитків, після чого за допомогою когортного аналізу здійснюється деталізація причин їх виник нення та визначення збитків для моделі виробів. Вихідними результатами цього модуля є аналітичні таблиці, діаграм и та відсоткові співвідношення. Третім модулем підсистеми є модуль рекомендацій , який функціонує на основі підходу правил «якщо -то». У Decision Table у Excel зберігаються правила рекомендацій для типов их випадків збитків. Наприклад, якщо причиною простою є несправність конкретного обладнання, модуль пропонує провести його технічне обслуговування або модернізацію. Завдяки цьому модуль автоматично формує набір практичних рекомендацій, що додає результа там аналізу прикладну цінність. Останнім модулем підсистеми «Аналіз збитків на виробництві виробів із нержавіючої сталі» є модуль виведення результатів, який відповідає за візуалізацію даних і формування відомостей . Він забе зпечує 39 побудову діаграм Парето, таблиць , а також створює відомості для керівництва у форматі PDF. Таким чином, користувач отримує візуальне представлення результатів аналізу і готові документи для при йняття управлінських рішень. 3.2 Особливості впровадження та експлуатації підсистеми «Аналіз збитків на виробництві виробів із нержавіючої сталі» Впровадження підсистеми «Аналіз збитків на виробництві виробів із нержавіючої сталі» вимагає ретельного планування, узгодження з розроблюваною ІС підприємства та врахування специфіки виробничих процесів. Оскільки підсистема інтегрується з ІС підприємства «KREDO», ключовим завданням на етапі впровадження є забезпечення коректного обміну да ними між підсистемою «Облік збитків» ІС підприємства та модулем збору"}
{"chunk_id": "2025_M_IUS_Shkredova_YeYa.pdf:11", "source": "2025_M_IUS_Shkredova_YeYa.pdf", "text": "Таким чином, користувач отримує візуальне представлення результатів аналізу і готові документи для при йняття управлінських рішень. 3.2 Особливості впровадження та експлуатації підсистеми «Аналіз збитків на виробництві виробів із нержавіючої сталі» Впровадження підсистеми «Аналіз збитків на виробництві виробів із нержавіючої сталі» вимагає ретельного планування, узгодження з розроблюваною ІС підприємства та врахування специфіки виробничих процесів. Оскільки підсистема інтегрується з ІС підприємства «KREDO», ключовим завданням на етапі впровадження є забезпечення коректного обміну да ними між підсистемою «Облік збитків» ІС підприємства та модулем збору й обробки даних. Для цього необхідно налаштувати інтерфейси доступу до даних у режимі «тільки для читання» з метою збереження ці лісності та безпеки інформації. Однією з особливостей є не обхідність попередньої підготовки та стандартизації даних, що вносяться начальниками відділів виробництва, постачання та логістики, а також відділу контролю якості. Для коректної роботи підсистеми «Аналіз збитків на виробництві виробів із нержавіючої сталі » потрібно уніфікувати формати введення: визначити обов’язкові поля (тип збитку, дата збитку, причина збитку, вартість, номери постраждалих партій) та забезпечити уніка льність ідентифікаторів моделей виробів . Особливу увагу слід приділити модулю рекомендац ій, для якого необхідне коректне формування правил у Decision Table , яка буде містити типові сценарії реагування на різні причини збитків. Регулярна актуалізація цих правил і контроль їх узгодженості забезпечують точність рекомендацій. 40 Важливою особливістю є також гнучкість у використанні  підсистема «Аналіз збитків на виробництві виробів із нержавіючої сталі» може функціонувати як окремий модуль для фінансового аналітика, так і бути інтегрована у загальну ІС підприємства. У першому випадку результати аналізу формуються у вигляді відомостей та PDF -звітів, у другому – вони автоматично переда ються до інших модулів, наприклад, ІС планування виробництва чи управлі ння запасами. На етапі експлуатації необхідно враховувати й питання безпеки та захисту даних. У сі відомості , що формуються підсистемою , повинні зберігатися у захищеному середовищі з обмеженим доступом. Особливо це стосується відомостей із деталізацією за працівниками або відповідальними особами, що може належати до катег орії конфіденційної інформаці ї. Таким чином, впровадження та експлуатація підсистеми «Аналіз збитків на виробництві виробів із нержавіючої сталі» потребує комплексного підходу: технічного налаштування, стандартизації вхідних даних, формування та постійної підтримки бази правил, інтегр ації з розроблюваною ІС підприємства та організації безпечного зберігання відомостей . 3.3 Опис алгоритму роботи підсистеми «Аналіз збитків на виробництві виробів із нержавіючої сталі» Алгоритм роботи підсистеми «Аналіз збитків на виробництві виробів із нержавіючої сталі» забезпечує автоматизацію всіх етапів обробки даних, їх аналізу та формування управлінських рекомендацій для підприємства «KREDO». Процес виконується у кілька послідовних кроків, які включають збір та обробку даних, застосування комбінованого методу, формування рекомендацій та генерацію відомостей. 41 Алгоритм починається зі збору даних із БД підсистеми «Облік збитків» ІС підприємства. Користувач через інтерфейс підсистеми обирає період аналізу, тип збитків або модель виробів для аналізу збитків. На цьому етапі підсистема здійснює перевірку введених даних на коректність. У разі виявлення помилок користувач отримує повідомлення про некоректне введення. Якщо дані введено правильно, підсистема формує зап ит до підсистеми «Облік збитків» ІС підприємства. Далі виконується перевірка наявності даних у БД підсистеми «Облік збитків» . Якщо відповідні дані відсутні, процес завершується повідомленням про неможливість виконання аналізу. Якщо дані наявні, підсистема переходить до головного етапу  проведення аналізу збитків комбінованим методом, який базується на поєднанні методу Парето та когортного аналізу. На цьому етапі здійснюється поділ аналітичного процесу на три паралельні гілки:  аналіз збитків за період;  аналіз збитків за типом збитків;  аналіз збитків за моделлю виробів. Кожна гілка передбачає виконання етапу «Формування рекомендацій відповідно до результатів аналізу ». Окремо для гілки «Аналіз збитків за період» та «Аналіз збитків за типом збитків» викон ується етап «Формування рекомендацій щодо управління збитками». Після формування рекомендацій виконується генерація відомостей. У результаті роботи підсистеми «Аналіз збитків на виробництві виробів із нержавіючої сталі» формується чотири типи вихідних доку ментів: «Відомість з результатами аналізу збитків за період», «Відомість з результатами аналізу збитків за типом збитків», «Відомість з р езультатами аналізу збитків за моделлю виробів» та «Відомість з рекомендаціями щодо управління збитками». Таким чином, підсистема «Аналіз збитків на виробництві виробів із нержавіючої сталі» забезпечує повну автоматизацію процесу аналізу, від збору даних до формування рекомендацій та генерації відомостей , що 42 дозволяє підприємству виявляти найвитратніш і типи збитків, їхні п ричини та форму є практичних рекомендацій. Це мінімізує часові витрати на обробку даних, знижує ризик людських помилок і забезпечує керівництво обґрунтован им аналізом для прийняття управлінських рішень . Діаграма активності (Activity Diagram) у нотації Unifi ed Modeling Language (UML ) використовується для моделювання послідовності дій та процесів у підсистемах [27]. Вона дає змогу наочно відобразити логіку виконання бізнес -процесів, починаючи з моменту ініціації події і завершуючи д осягненням кінцевого результ ату. Діаграму активності підсистеми «Аналіз збитків на виробництві виробів із нержавіючої сталі» представлено на рис унку 3.1. Робота підсистеми починається з того, що користувач задає параметри аналізу: обирає період, тип збитків або модель виробів. Після цього виконується перевірка коректності введених даних. Якщо дані некоректні, процес завершується (користувач має повторити вв едення). Якщо дані коректні, підсистема переходить"}
{"chunk_id": "2025_M_IUS_Shkredova_YeYa.pdf:12", "source": "2025_M_IUS_Shkredova_YeYa.pdf", "text": "процесів у підсистемах [27]. Вона дає змогу наочно відобразити логіку виконання бізнес -процесів, починаючи з моменту ініціації події і завершуючи д осягненням кінцевого результ ату. Діаграму активності підсистеми «Аналіз збитків на виробництві виробів із нержавіючої сталі» представлено на рис унку 3.1. Робота підсистеми починається з того, що користувач задає параметри аналізу: обирає період, тип збитків або модель виробів. Після цього виконується перевірка коректності введених даних. Якщо дані некоректні, процес завершується (користувач має повторити вв едення). Якщо дані коректні, підсистема переходить до взаємодії з БД підсистем и «Облік збитків» ІС підприємства. У підсистемі «Облік збитків» здійснюється збір даних про збитки з БД та перевірка їх наявності. Якщо потрібних даних немає, процес також заверш ується. Якщо дані наявні, вони передаються в підсистему аналізу, де виконується аналі з збитків комбінованим методом. Далі процес розгалужується на три напрямки , а саме аналіз збитків за періодом , за типом збитків і за моделлю виробів . Результати аналізу за періодом і за типом збитків використовуються не лише для побудови відповідних відомостей, але й для формування узагальненої відомості з рекомендаціями щодо управління збитками, яка відобража є пріоритетні напрями для прийняття управлінських рішень. Аналіз за моделлю виробів завершується формуванням відомості за моделями та рекомендацій щодо оптимізації виробництва окремих виробів. Після формування всіх необхідних відомостей і рекомендацій робота підсистеми завершується, а згенеровані результати можуть бути 43 використані керівництвом підприємства для прийняття управлінських рішень . Рисунок 3.1 – Діаграма активності підсистеми «Аналіз збитків на виробництві виробів із нержавіючої сталі» 44 4 ЕКСПЕРИМЕНТАЛЬНА П ЕРЕВІРКА РОЗРОБЛЕНОГ О КОМБІНОВАНОГО МЕТОДУ ТА ОЦІН ЮВАННЯ ТОЧНОСТІ ЗАПРОПОНОВАНОГО РІШЕ ННЯ 4.1 Обґрунтування вибору платформи програмного забезпечення Вибір платформи ПЗ є одним із ключових етапів розробки підсистеми «Аналіз збитків на виробництві виробів із нержавіючої сталі» , оскільки від нього залеж ать продуктивність, надійність та інтеграційна сумісність з розроблюваною ІС підприємства . Для реаліз ації комбінованого метод аналізу збитків (метод Парето, когортний аналіз) та використання модул ю рекомендацій , підсистема «Аналіз збитків на виробницт ві виробів із нержавіючої сталі» потребує стабільного середовища, здатного працювати з великими обсягами даних, виконувати статистичні розрахунки, будувати аналітичні графіки та формувати відомості у зручному для користувача форматі. Для реалізації підсист еми «Аналіз збитків на виробництві виробів із нержавіючої сталі» було обрано мову програмування Python [28], яка є універсальним середовищем програмування з відкритим кодом і підтримкою великої кількості біблі отек для аналітики, статистики та візуалізації. Її використання забезпечує простоту інтеграції з ІС підприємства, гнучкість у реалізації аналітичних алгоритмів та низькі витрати на розробку й супровід. До основних переваг використання мови програмування Python належить її універсальність і широкий набі р інструментів для реалізації аналізу збитків. Для обробки великих обсягів даних, що надходять з БД підсистеми «Облік збитків» ІС підприємства, використовуються бібліотеки pandas та NumPy, які забезпечують швидку фільтрацію та групування даних , обчислення часток і кумулятивних показників, які необхідн і для реалізації комбінованого методу аналізу збитків . Для візуалізації даних 45 застосовуються бібліотеки matplotlib та seaborn, які дають змогу побудувати діаграми Парето. Важливим етапом у підсистемі «Аналіз зб итків на виробництві виробів із нержавіючої сталі» є формування відомостей . Для генерації автоматичних PDF -звітів використовується бібліотека ReportLab, яка дозволяє інтегрувати графічні елементи, таблиці та текстові висновки в єдиний документ. Це значно п олегшує процес підготовки управлінських відомостей і робить результати аналізу доступними для перегляду. Модуль рекомендацій підсистеми побудований на принципах rule - based систем і реалізується з використанням бібліотеки experta, яка забезпечує підтримку л огіки правил типу «якщо -то». Усі правила рекомендацій зберігаються в таблиці Microsoft Excel, яка виконує роль Decision Table. Для зчитування даних із цих таблиць використовується бібліотека openpyxl, що дозволяє легко оновлювати набір правил без зміни програмного коду, забезпечуючи простоту масштабування та підтримки. Для зберігання результатів аналізу та рекомендацій обрано систему управління базами даних (СУБД) MySQL, яка має високу продуктивність, підтримує багатокористувацький доступ і добре інтегруєть ся з Python через бібліотеки SQLAlchemy або mysql.connector. Це забезпечує централізоване управління даними, підвищує безпеку та зручність у подальшій експлуатації підсистеми . Програмний код реалізації комбінованого методу аналізу збитків наведено у додатк у Б. 4.2 Опис вимог до програмного забезпечення Розробка підсистеми «Аналіз збитків на виробництві виробів із нержавіючої сталі» потребує чіткого визначення вимог до ПЗ, оскільки саме 46 вони формують основу для її подальшої реалізації, тестування та впровадження. Усі вимоги умовно поділяються на дві категорії  функціональні та нефункціональні, які спільно забезпечують коректність, продуктивність і зручність роботи ІС. Функціональні вимоги описують, що саме має виконувати ІС. Основна її функція полягає в виконані повного циклу аналізу збитків  від збору вихідних даних до формування відомостей та рекомендацій. підсистемі «Аналіз збитків на виробництві виробів із нержавіючої сталі» повинна автоматично імпортувати дані з БД підсистеми «Облік збитків» ІС підприємства, де містяться записи про типи збитків, їх причини, вартості, номери партій, що постраждали, дати виникнення збитків, відповідальних осіб, а також текстовий опис збитку. Після отримання даних підсистема застосовує комбінований метод аналізу. Пі сля виконання аналізу збитків підсистема переходить до етапу формування рекомендацій. Модуль рекомендацій зчитує правила з Decision"}
{"chunk_id": "2025_M_IUS_Shkredova_YeYa.pdf:13", "source": "2025_M_IUS_Shkredova_YeYa.pdf", "text": "аналізу збитків  від збору вихідних даних до формування відомостей та рекомендацій. підсистемі «Аналіз збитків на виробництві виробів із нержавіючої сталі» повинна автоматично імпортувати дані з БД підсистеми «Облік збитків» ІС підприємства, де містяться записи про типи збитків, їх причини, вартості, номери партій, що постраждали, дати виникнення збитків, відповідальних осіб, а також текстовий опис збитку. Після отримання даних підсистема застосовує комбінований метод аналізу. Пі сля виконання аналізу збитків підсистема переходить до етапу формування рекомендацій. Модуль рекомендацій зчитує правила з Decision Table у форматі Excel та автоматично генерує текстові висновки для керівництва. Це дає змогу скоротити час на формування ріш ень і підвищити об’єктивність управлінських рекомендацій . Далі підсистема автоматично створює відомості у форматі PDF, які містять результати аналізу, графічні матеріали та сформовані рекомендації. Нефункціональні вимоги до підсистеми «Аналіз збитків на виробництві виробів із нержавіючої сталі» визначають її якість і надійність. Підсистема повинна бути швидкою, стабільною та безпечною. Вона має забезпечувати обробку великих обсягів даних у короткий проміжок часу, не перевищуючи 10 секунд для середнього запи ту. Безпека гарантується аутентифікацією користувачів. Важливою вимогою є сумісність підсистеми з різними операційними середовищами (Windows, Linux, macOS) та можливість інтеграції з іншими модулями ІС підприємства. Оскільки підсистема може використовувати ся кількома користувачами одночасно, передбачено зберігання даних у СУБД MySQL із 47 можливістю резервного копіювання. Для підтримки й оновлення програмного коду використовується система контролю версій GitHub, що забезпечує надійне зберігання та зручність ко мандної розробки. Таким чином, розроблене ПЗ має забезпечувати комплексний аналіз збитків, високу швидкість обробки даних, стабільність і безпеку роботи. 4.3 Експериментальна перевірка отриманих результатів Експериментальна перевірка є завершальним етапом розроблення підсистеми «Аналіз збитків на виробництві виробів із нержавіючої сталі» і має на меті перевірити працездатність створеного програмного рішення, коректність реалізації комбінованого методу , точніс ть отриманих результатів аналізу, а також правильн ість формування відомостей . Для демонстрації функціональності підсистеми було проведено моделювання бізнес -процесу аналізу збитків на прикладі даних виробництва виробів із нержавіючої сталі. Результатом роб оти підсистеми є відомості, які містять дані для аналізу, таблиці з результатами аналізу , графічн і діаграм и та автоматично сформован і рекомендацій, що дають змогу користувачеві приймати управлінські рішення . На рисунку 4.1 подано приклад інтерфейсу підсистеми «Аналіз збитків на виробництві виробів із нержавіючої сталі» , який дозволяє вносити параметри для аналізу збитків і формувати відомості з результат ами аналізу збитків та рекомендаціями . На рисунку 4.2 наведено приклад сформованого PDF -документа «Відомість з результатами аналізу збитків за період» , який містить інформацію про період звітності, дата формування відомості, користувача, таблицю з даними про збитки за період, діаграм у Парето та рекомендації . 48 Рисунок 4.1  Інтерфейс підсистеми «Аналіз збитків на виробництві виробів із нержавіючої сталі» 49 Рисунок 4.2  Вихідний документ «Відомість з результ атами аналізу збитків за період » 50 Рисунок 4.2, аркуш 2 51 На рисунку 4.3 зображено приклад сформованого PDF -документа «Відомість з результатами аналізу збитків за типом збитків «Брак », який містить інформацію про тип збитку, період звітності, дата формування відомості, користувача, таблицю з даними про збитки за типом збитків , діаграму Парето та рекомендації. Приклад сформованого PDF -документа «Відомість з результатами аналізу збитків за моделлю виробів «Квадростиль » представлено на рисунку 4.4. В ньому відображено інформацію про модель виробів, період звітності, дат у формування відомост і, користувача, таблицю з даними про збитки за моделлю виробів , діаграму Парето та рекомендації. Останнім вихідним документом є «Відомість з рекомендаціями щодо управління збитками», як ий містить рекомендації щодо управління збитками і є основою для подаль ших управлінських рішень. Приклад сформованого PDF -документа «Відомість з рекомендаціями щодо управління збитками» надано на рисунку 4. 5. 52 Рисунок 4.3  Вихідний документ « Відомість з результатами аналізу збитків за типом збитків «Брак » 53 Рисунок 4.3, аркуш 2 54 Рисунок 4.4  Вихідний документ «Відомість з результатами аналізу збитків за моделлю виробів «Квадростиль » 55 Рисунок 4.4, аркуш 2 56 Рисунок 4.5  Вихідний документ «Відомість з рекомендаціями щодо управління збитками» 57 Для перевірки точності розробленого комбінованого методу аналізу збитків проведено експериментальн у перевірку на основі узагальнених виробничих даних підприємства, що займається виготовленням виробів із нержавіючої сталі. Метою експериментальної перевірки є оцінити, наскільки точно комбінований метод аналізу збитків дозволяє виявляти найвитратніші збитки та встановлювати закономірності їх виникнення у порівнянні з існуючим методом аналізу збитків. Під час експеримент у враховували ся такі показники як дата виникнення збитку , його тип, причина, вартість, дані про партії виробів та відповідальних осіб. Усі збитки було класифіковано за запропонованою класифікацією збитків. У таблиці 4.1 наведено приклад вхідних даних про збитки на виро бництві виробів із нержавіючої сталі, використаних для демонстрації роботи комбінованого методу аналізу збитків. Таблиця 4.1  Вхідн і дані про збитки на виробництві № Дата виникнення Тип збитку Причина збитку Вартість збитку Номер партії Відповідальна особа 1 05.09.2025 Брак Несправність обладнання 7 200 грн. № 9465 Мартинюк А.В. 2 12.09.2025 Простої Відсутність персоналу 4 900 грн.  Сидоренко Т.Ю. 3 18.09.2025 Втрати матеріалів Псування сировини 4 720 грн."}
{"chunk_id": "2025_M_IUS_Shkredova_YeYa.pdf:14", "source": "2025_M_IUS_Shkredova_YeYa.pdf", "text": "збитків. У таблиці 4.1 наведено приклад вхідних даних про збитки на виро бництві виробів із нержавіючої сталі, використаних для демонстрації роботи комбінованого методу аналізу збитків. Таблиця 4.1  Вхідн і дані про збитки на виробництві № Дата виникнення Тип збитку Причина збитку Вартість збитку Номер партії Відповідальна особа 1 05.09.2025 Брак Несправність обладнання 7 200 грн. № 9465 Мартинюк А.В. 2 12.09.2025 Простої Відсутність персоналу 4 900 грн.  Сидоренко Т.Ю. 3 18.09.2025 Втрати матеріалів Псування сировини 4 720 грн. № 9471  4 25.09.2025 Брак Некваліфіковані дії персоналу 3 400 грн. № 9474 Матюшков О.В. 5 03.10.2025 Логістичні проблеми Затримка у постачанні сировини 2 800 грн.  ТОВ «Santan» Степанова І.П. 6 17.10.2025 Брак Несправність обладнання 2 600 грн. № 9479 Мартинюк А.В. 7 28.10.2025 Втрати матеріалів Нераціональне використання залишків 1 500 грн. № 9482 Микитенко П.Л. 58 58 На першому етапі комбінованого методу аналізу збитків дані про збитки групуються за типами збитків і сортуються у порядку спаданн я вартості збитків за формулою 2.4 . Далі до відсортованих даних про збитки застосовується метод Парето, який дозволяє визначити, які саме типи збитків формують найбільшу частину фінансових втрат на виробництві. Розрахунок частки кожного типу збитків здійснюється за формулою 2 .1. Далі дані пр о збитки відсортовуються за бінарним показником пріоритету Zj кожного типу збитків, щоб типи збитків, на які підприємство може вплинути, раніше увійшли до кумулятивної частки. Щоб визначити сукупний вплив основних типів збитків на виробництво, розраховуєть ся кумулятивна частка кожного типу збитків за формулою 2.2. Далі застосовується правило відбору, яке базується на пороговому значенні довірчого інтервалу. Пороговим значенням довірчого інтервалу обрано 70% до 90 %, оскільки такий інтервал дозволяє точніше відобразити розподіл збитків на виробництві виробів із нержавіючої сталі. Результати виконання першого етапу комбінованого методу представлено у таблиці 4.2 та на рисунку 4.6 відображено діаграму Парето . Таблиця 4.2  Найвитратніші типи збитків на виробництві № Тип збитку Причина збитку Вартість збитку Номер партії Відповідальна особа Частка збитку Кумул. частка 1 Брак Несправність обладнання, некваліфіковані дії персоналу 13 200 грн. № 9465, № 9474, № 9479 Мартинюк А.В., Матюшков О.В. 48.67% 48.67% 2 Втрати матеріалів Псування сировини, нераціональне використання залишків 6 220 грн. № 9471, № 9482 Микитенко П.Л. 22.93% 71.60% 3 Простої Відсутність персоналу 4 900 грн.  Сидоренко Т.Ю. 18.07% 89.67% 4 Логістичні проблеми Затримка у постачанні сировини 2 800 грн.  ТОВ «Santan» Степанова І.П. 10.33% 100.00% 59 59 Рисунок 4. 6 Діаграма Парето найвитратніших типи збитків на виробництві Далі застосовується правило відбору, за яким, шляхом послідовного додавання, тип збитку «Брак» перший увійшов до переліку найвитратніших типів збитків, але не досяг порогового значення встановленого довірчого інтервалу. Наступним і останнім типом збитків став тип «Втрати матеріалів», бо його кумулятивна частка (71.60%) уперше перет нула нижню межу порогового значення встановленого довірчого інтервалу від 70% до 90%. На другому етапі комбінованого методу аналізу збитків для кожного з найвитратніших типів збитків, які були визначенні на попередньому етапі, застосовується когортний аналіз. Для кожного з найвитратніших типів збитків когортами виступають причини збитків. Усі збитки з однаковою причиною об’єднуються в одну когорту і для кожної з причин збитку розраховується кількість інцидентів та їх загальна вартість. Метриками для когорт при аналізі причин збитків є дати виникнення збитків та частка причини збитків від загальної вартості конкретного типу збитків. Розрахунок частки причин збитків здійснюється за формулою 2 .1. 60 60 Результати виконання другого етапу комбінованого методу аналі зу збитків представлено у таблиці 4.3 для типу збитків «Брак» та у таблиці 4.4 для типу збитків «Втрати матеріалів». Таблиця 4.3  Причини збитків для типу збитків «Брак» № Дата виникнення Причина збитку Вартість збитку Частка збитку Кількість інцидентів 1 05.09.2025, 17.10.2025 Несправність обладнання 9 800 грн. 74.24% 2 2 25.09.2025 Некваліфіковані дії персоналу 3 400 грн. 25.76% 1 Таблиця 4.4  Причини збитків для типу збитків «Втрати матеріалів» № Дата виникнення Причина збитку Вартість збитку Частка збитку Кількість інцидентів 1 18.09.2025 Псування сировини 4 720 грн. 75.88% 1 2 28.10.2025 Нераціональне використання залишків 1 500 грн. 24.12% 1 Результати проведеного аналізу збитків комбінованим методом показали, що основними джерелами збитків на виробництві виробів із нержавіючої сталі є брак та втрати матеріалів, на які припадає понад 70 % усіх фінансових втрат підприємства. Проведений когортний аналіз дозволив деталізувати причини цих збитків і визначити закономірності їх виникнення, що дає змогу сформулювати рекомендації щодо їх усунення за допомогою Decision Table. Приклад рекомендації за результатами аналізу збитків комбінованим методом представлено у вигляді таблиці 4.5. 61 61 Таблиця 4.5  Рекомендації для н айвитратніш их типів збитків Тип збитку Причина збитку Рекомендація Брак Несправність обладнання Провести планове технічне обслуговування зварювального устаткування; контролювати параметри роботи обладнання; впровадити журнал технічного моніторингу для фіксації відхилень. Некваліфіковані дії персоналу Організувати додаткове навчання працівників; упровадити систему періодичної атестації операторів; посилити внутрішній контроль якості на етапі зварювання та полірування. Втрати матеріалів Псування сировини Оптимізувати умови складування (вологість, температура, механічне навантаження); забезпечити контроль середовища зберігання та використання захисної тари. Нераціональне використання залишків Впровадити"}
{"chunk_id": "2025_M_IUS_Shkredova_YeYa.pdf:15", "source": "2025_M_IUS_Shkredova_YeYa.pdf", "text": "61 Таблиця 4.5  Рекомендації для н айвитратніш их типів збитків Тип збитку Причина збитку Рекомендація Брак Несправність обладнання Провести планове технічне обслуговування зварювального устаткування; контролювати параметри роботи обладнання; впровадити журнал технічного моніторингу для фіксації відхилень. Некваліфіковані дії персоналу Організувати додаткове навчання працівників; упровадити систему періодичної атестації операторів; посилити внутрішній контроль якості на етапі зварювання та полірування. Втрати матеріалів Псування сировини Оптимізувати умови складування (вологість, температура, механічне навантаження); забезпечити контроль середовища зберігання та використання захисної тари. Нераціональне використання залишків Впровадити електронний облік залишків; створити базу повторного використання відходів виробництва; забезпечити планування розкрою для мінімізації відходів. Проведене дослідження підтвердило точність комбінованого методу аналізу збитків, що поєднує метод Парето та когортний аналіз. Застосування цього методу аналізу збитків дозволило отримати детальне та комплексне уявлення про збитки на виробництві. На відміну від традиційних методів, комбінований метод аналізу збитків охоплює не лише кількісні характеристика збитків, а й якісні чинники, що визначають причин и їх виникнення, повторюваність і керованість. Результати аналізу збитків показали, що більшість фінансових втрат припадають на декілька типів збитків, що підтверджує точність принципу Парето для аналізу збитків на виробництві. Проте використання лише цьог о методу не дає змоги зрозуміти причини виникнення збитків і частоту їх повторення, що знижує можливість прийняття управлінських рішень. Додавання когортного аналізу на другому етапі комбінованого методу усунуло цей недолік. Завдяки когортному аналізу вдал ося визначити закономірності появи збитків, простежити залежність між технічними збоями, людським фактором та умовами зберігання матеріалів. Це 62 62 дозволило перейти від простого опису збитків до детального аналізу причин виникнення цих збитків. Важливим резул ьтатом дослідження стало формування рекомендацій за таблиц ею рішень (Decision Table), що дозволяє автоматизувати процес ухвалення управлінських рішень. Такий інструмент може бути інтегрований у ІС підприємства для надання рекомендацій щодо запобігання повт орним інцидентам виникнення збитків. Для оцін ювання точності та якості результатів аналізу збитків запропонованого комбінованого методу аналізу збитків проведено ABC - аналіз тих самих даних, що використовувалися пі д час попереднього дослідження. ABC -аналіз ґрунтується на принципі розподілу об’єктів за ступенем їхнього впливу на загальний результат, де група А – найзначущі збитки, що формують 70 –80 % загальних збитків на виробництві, група B – середньої важливості, 15 –25 %, група C – незначні, але численні збитки, до 10 %. На основі розрахованих часток збитків (табл. 4.2) було отримано такі результати розподілу, а саме група A  «Брак» (48,67 %) і «Втрати матеріалів» (22,93 %), група B  «Простої» (18,07 %), група C  «Логістичні проблеми». Порівнян ня результатів комбінованим методом аналізу збитків з ABC -аналізом показує, що за результатами ABC -аналізу підприємству необхідно зосередитися на усуненні збитків групи A, що співпадає з висновками комбінованого методу. Проте між двома методами є суттєва різниця. ABC -аналіз лише класифікує типи збитків за рівнем їхнього впливу, не враховуючи причин збитків, можливість керування цими збитками та частоту виникнення цих збитків. Цей метод є швидким інструментом для ранжування, але не відповідає на питання , чому виникають збитки на виробництві . Запропонований комбінований метод об’єднує кількісний підхід (через метод Парето) із якісним (через когортний аналіз), що дозволяє не лише виявити найвитратніші типи збитків, а й 63 63 детально дослідити причини їх виникнення , повторюваність, керованість збитків і розробити практичні рекомендації. 4.4 Висновки до четвертого розділу У результаті проведеного дослідження було експериментально перевірено комбінований метод аналізу збитків на виробництві виробів із нержавіючої сталі, який поєднує метод Парето та когортний аналіз. Такий підхід забезпечує поєднання оцінки фінансового впливу збитків на виробництво із якісним аналізом причин їх виникнення, що дозволяє отримати комплексні результати аналізу збитків на виробництві виробів із нержавіючої сталі. Розроблений комбінований метод аналізу збитків дає змогу визначити основні типи збиткі в, які формують більшіст ь фінансових втрат підприємства, виявити повторювані закономірності у виникненн і збитків та їх ключові причини, сформувати рекомендації за таблиц ею рішень (Decision Table), яка забезпечує практичне використання результатів аналізу д ля управлінських рішень. Порівняння комбінованого методу з ABC -аналізом показало, що запропонований комбінований метод аналізу збитків має вищу аналітичну цінність. Якщо ABC -аналіз обмежується класифікацією збитків за рівнем їхньої важливості, то комбінова ний метод аналізу збитків забезпечує глибший причинно -наслідковий аналіз, враховуючи як фінансову, так і процесну складову. Упровадження комбінованого методу в ІС підприємства дозволить підвищити точність аналізу збитків, зменшити вплив людського фактору п ри аналізі збитків та скоротити час формування рекомендацій. Таким чином, запропонований метод аналізу збитків на виробництві є універсальним інструментом для аналізу збитків, а його використання сприятиме підвищенню якості прийняття управлінських рішень. 64 64 ВИСНОВКИ У ході виконання магістерської кваліфікаційної роботи проведено дослідження процесу аналізу збитків на виробництві виробів із нержавіючої сталі та методів аналізу збитків, які застосовуються для зменшення фінансових витрат і підтримки прийняття управлінських рішень. Об’єктом дослідження був процес аналізу збитків на виробництві виробів із нержавіючої сталі, а предметом – методи аналізу збитків . На основі проведеного дослідження методів аналізу збитків розроблено комбінований метод аналізу збитків, який враховує специфіку виробництва та дозволя є отримати точні результати для кожного типу збитків. У першому розділі роботи проаналізовано діяльність підприємства «KREDO», що"}
{"chunk_id": "2025_M_IUS_Shkredova_YeYa.pdf:16", "source": "2025_M_IUS_Shkredova_YeYa.pdf", "text": "проведено дослідження процесу аналізу збитків на виробництві виробів із нержавіючої сталі та методів аналізу збитків, які застосовуються для зменшення фінансових витрат і підтримки прийняття управлінських рішень. Об’єктом дослідження був процес аналізу збитків на виробництві виробів із нержавіючої сталі, а предметом – методи аналізу збитків . На основі проведеного дослідження методів аналізу збитків розроблено комбінований метод аналізу збитків, який враховує специфіку виробництва та дозволя є отримати точні результати для кожного типу збитків. У першому розділі роботи проаналізовано діяльність підприємства «KREDO», що спеціалізується на виготовленні виробів із нержавіючої сталі, та досліджено бізнес -процес «Аналіз збитків на виробництві виробів із нержавіючої сталі» . На основі огляду наукових джерел про різні типи збитків запропоновано єдину класифікацію типів збитків , адаптовану до специфіки виробни цтва виробів із нержавіючої сталі. У другому розділі розглянуто та проаналізовано методи, що застосовуються для вирішення задачі аналізу збитків на виробництві виробів із нержавіючої сталі. Зокрема, досліджено можливості методу Парето, когортного аналізу , аналізу динамічних рядів та ABC -аналізу щодо аналізу різних типів збитків. На основі порівняння переваг і обмежень існуючих методів аналізу збитків обґрунтовано доцільність використання комбінованого методу аналізу збитків, що поєднує метод Парето з кого ртним аналізом. У третьому розділі розроблено інформаційну технологію реалізації комбінованого методу аналізу збитків у складі підсистеми «Аналіз збитків на виробництві виробів із нержавіючої сталі» при розробці ІС виробництва виробів із нержавіючої сталі . Описано функціональну структуру 65 65 підсистеми, побудовану за методологією SADT, обґрунтовано вибір програмних засобів , а саме мови програмування Python та бібліотек pandas, NumPy, matplotlib, seaborn, ReportLab, experta, openpyxl, а також СУБД MySQL для збе рігання результатів аналізу збитків . Реалізовано використання таблиці рішень (Decision Table) у вигляді електронної таблиці Microsoft Excel для формування рекомендацій за правилами типу «якщо - то», що забезпечує можливість оновлення правил без зміни програм ного коду. Описано алгоритм роботи підсистеми «Аналіз збитків на виробництві виробів із нержавіючої сталі» , який охоплює завантаження даних із БД від системи «Облік збитків» ІС підприємства , виконання комбінованого методу аналізу збитків, формування відомо стей і автоматичну генерацію PDF - документів з результатами аналізу та рекомендаціями. У четвертому розділі проведено експериментальну перевірку розробленого комбінованого методу аналізу збитків на основі реальних даних про збитки підприємства «KREDO». Результати комбінованого методу було порівняно з результатами ABC -аналізу: обидва підходи вказали на ті самі пріоритетні напрямки скорочення збитків, проте ABC -аналіз забезпечив лише ранжування типів збитків, тоді як комбінований метод додатково виявив при чини збитків, їхню керованість і частоту виникнення, що є важливим для форм ування практичних рекомендацій. Отримані результати підтвердили, що розроблений комбінований метод аналізу збитків є якісним інструментом для підтримки управлінських рішень на підпр иємстві, яке виготовляє вироби із нержавіючої сталі. Його використання дозволяє одночасно оцінювати фінансовий вплив збитків і досліджувати причини їх виникнення, виділяти найкритичніші типи збитків та виявляти повторювані закономірності у даних за партіям и виробів. Інтеграція методу до підсистеми «Аналіз збитків на виробництві виробів із нержавіючої сталі» створює можливості для автоматизації формування відомостей , стандартизації процесу формування рекомендацій і підвищення прозорості аналізу збитків . 66 66 Резу льтати кваліфікаційної роботи прийнято до опублікування у журнал категорії Б «Information Technology : Computer Science , Software Engineering and Cyber Security » в статті «Розробка комбінованого методу аналізу збитків на виробництві виробів із нержавіючої с талі», що підтверджує наукову й практичну значущість отриманих результатів. Магістерську кваліфікаційну роботу виконано згідно з вимогами методичних вказівок щодо розробки та оформлення кваліфікаційної роботи другого (магістерського) рівня вищої освіти за освітньо -професійною програмою «Інформаційні управляючі системи та технології» [29] та з дотриманням положень державних стандартів ДСТУ 3008:2015 [30] і ДСТУ 8302:2015 [31]. 67 ПЕРЕЛІК ДЖЕРЕЛ ПОСИЛ АННЯ 1. Що таке збитки // Smart Tender URL: https://smarttender.biz/terminy/view/zbytky/ (дата звернення: 24.11.2025). 2. Di Schino A. Manufacturing and Applications of Stainless Steels // Metals. 2020. Vol. 10, No 3. p. 327. URL: https://doi.org/10.3390/met10030327 (дата звернення : 24.11.2025). 3. Garza -Reyes J. A., Rocha -Lona L., Kumar V. A conceptual framework for the implementation of quality management systems // Total Quality Management & Business Excellence. 2015 . Vol. 26, No 11-13. p. 1298 –1310. URL: https://doi.org/10.1080/14783363.2014.929254 (дата звер нення : 24.11.2025). 4. Сердюк Б. М., Лещук А. А. Теоретичні основи класифікації збитків // Електронний журнал. 2010. No 10. URL: http://www.economy.nayka.com.ua/?op=1&z=363 (дата звернення: 25.11.2025). 5. Six Big Losses // Vorne Industries URL: https://www.vorne.com/ learn/tools/six -big-losses/six -big-losses -executive -summary.pdf (дата звернення : 25.11.2025). 6. Bokrantz J., Skoogh A., Ylipää T., Stah re J. Handling of production disturbances in the manufacturing industry // Journal of Manufacturing Technology Management. 2016. Vol. 27, No 8. p . 1054 –1075. URL: https://doi.org/10.1108/JMTM -02-2016 -0023 (дата звернення : 27.11.2025). 7. Царевська Т. Виробничі втрати // Податки & бухоблік. 2019. No 99. URL: https://i.factor.ua/ukr/journals/nibu/2019/d ecember/issue -99/article - 105872.html (дата звернення: 2 7.11.2025). 8. Міщенко К. Списання пошкоджених та знищених запасів під час воєнного стану // FactorAcademy . 2023. URL: https://factor.academy/blog/ 68 68 spisannya -poshkodzhenix -ta-znishhenix -zapasiv -pid-chas-voyennogo - stanu/#%D0%9F%D0%94%D0%92 (дата звернення: 2 8.11.2025). 9. Zhang R., Zhao N., Fu L.,"}
{"chunk_id": "2025_M_IUS_Shkredova_YeYa.pdf:17", "source": "2025_M_IUS_Shkredova_YeYa.pdf", "text": "Manufacturing Technology Management. 2016. Vol. 27, No 8. p . 1054 –1075. URL: https://doi.org/10.1108/JMTM -02-2016 -0023 (дата звернення : 27.11.2025). 7. Царевська Т. Виробничі втрати // Податки & бухоблік. 2019. No 99. URL: https://i.factor.ua/ukr/journals/nibu/2019/d ecember/issue -99/article - 105872.html (дата звернення: 2 7.11.2025). 8. Міщенко К. Списання пошкоджених та знищених запасів під час воєнного стану // FactorAcademy . 2023. URL: https://factor.academy/blog/ 68 68 spisannya -poshkodzhenix -ta-znishhenix -zapasiv -pid-chas-voyennogo - stanu/#%D0%9F%D0%94%D0%92 (дата звернення: 2 8.11.2025). 9. Zhang R., Zhao N., Fu L., Bai X., Cai J. Recognizing defects in stainless steel welds based on multi -domain feature e xpression and self - optimization // Journal of Intelligent Manufacturing. 2023. Vol. 34. p . 1293 – 1309. URL: https://doi.o rg/10.1007/s10845 -021-01849 -1 (дата звернення : 28.11.2025). 10. Yashvi. The causes & dangers of over -production // Qalara. 2024. URL: https://www.qalara.com/blog/the -dangers -of-overprod uction/ (дата звернення : 29.11.2025). 11. Васильченко С. Що таке принцип Парето та як його застосувати в роботі та житті // Happy Monday. URL: https://happymonday.ua/shho -take- pryntsyp -pareto (дата звернення: 30.11.2025). 12. Добрянська В. В., Івасенко О. А., Чижевська М. Б., Скрильник А. С. // Маркетингові дослідження: навчальний посібник. Полтава: ПолтНТУ, 2024. с. 68 -69. URL: https://reposit.nupp.edu.ua/ handle/PoltNTU/18593 (дата звернення : 30.11.2025) 13. Білобородько О. І., Ємел’яненко Т. Г. Аналіз динамічних рядів // Навчальний посібник . Дніпро : РВВ ДНУ, 2014. с. 80. URL: http://repository.dnu.dp.ua:1100/upload/bb40e3dc379f6e0736d9d88c27a31188 Biloborodko_TimeSeries.pdf (дата звернення : 30.11.2025) 14. Кондіус І. Мето дологія SADT // Електронний посібник. Луцьк: ЛНТУ, 2022. URL: https://elib.lntu.edu.ua/sites/default/files/ elib_upload/%D0%9A%D0%BE%D0%BD%D0%B4%D1%96%D1%83%D1% 81%202%20%D0%B3%D0%BE%D1%82%D0%BE%D0%B2%D0%B2%D0 %B0/page8.html (дата звернення: 01.12.2025). 15. Garengo P., Biazzo S. From ISO quality standards to an integrated management system: an implementation process in SME // Total Quality Management & Business Excellence. 2012. Vol. 24, No 3–4. p. 310335. URL: https://doi.org/10.1080/14783363.2012.704282 (дата звернення : 01.12.2025). 69 69 16. Solomon N. P., Bester A., Moll C. M. Diffusion of a quality management system : a case study // The South African Journal o f Industrial Engineering. 2017 . Vol. 28, No 2. p. 149–164. URL: https://doi.org/10.7166/28 - 2-1762 (дата звернення : 02.12.2025). 17. Natarajan D. ISO 9001 Quality Management Systems // Management and Industrial Engineering. 2017 . p. 160. URL: https://link.springer.com/book/10.1007/978 -3-319-54383 -3 (дата звернення : 02.12.2025). 18. Flores B. E., Clay Whybark D. Multiple Criteria ABC Analysis // International Journal of Operations & Production Management. 1986. Vol. 6, No 3. p. 3846. URL: https://doi.org/10.1108/eb054765 (дата звернення : 03.12.2025). 19. Abdolazimi1 O., Shishebori D., Goodarzian F., Ghasemi P., Appolloni A. Designing a new mathematical model based on ABC analysis for inventory control problem: A real case study // RAIRO - Operations Research. 2021. Vol. 55, No 4. p. 23092335. URL: https://doi.org/10.1051/ro/2021104 (дата звернення : 03.12.2025). 20. Pyzdek T. Pareto Analysis // The Lean Healthcare Handbook . 2021. p. 157164. URL: https://doi.org/10.1007/978 -3-030-69901 -7_14 (дата звернення : 04.12.2025). 21. Fukuda K. A cohort analysis of household vehicle expenditure in the U.S. and Japan: A possibility of generational marketing // Marketing Letters. 2010. Vol. 21. p. 53–64. URL: https://doi.org/10.1007/s11002 -009-9077 -2 (дата звернення : 04.12.2025). 22. Dunford R., S u Q., Tamang E. , Wintour A. The Pareto Principle // The Plymouth Student Scientist. 2014. Vol. 7, No 1. p. 140148. URL: https://doi.org/10.24382/swfr -wr17 (дата звернення : 05.12.2025). 23. Pareto principle // Wikipedia. URL: http://en.wikipedia.org/wiki/Pareto principle (дата звернення : 05.12.2025). 24. Біловол М. Що таке когортний аналіз? Як отримати результат? // 70 70 Brander. 2024. URL: https://brander.ua/blog/shcho -take-kohortnyy -analiz -yak- otrymaty -rezultat (дата звернення: 06.12.2025). 25. Castelli P., De Ruvo A., Bucciacchio A., D'Alterio N., Cammà C., \\ Di Pasquale A., Radomski N. Harmonization of supervised machine learning practices for efficient source attribution of Listeria monocytogenes based on genomic data. // BMC Gen omics . 2023. Vol. 24, No 560. p. 1–19. URL: https://doi.org/10.1186/s12864 -023-09667 -w (дата звернення : 07.12.2025). 26. Singh D. The Pareto Principle: Leveraging the 80/20 Rule to Drive Busin ess and Data Science Innovation // Medium. 2025. URL: https://medium.com/ai -enthusiast/the -pareto -principle -leveraging -the-80-20- rule-to-drive -business -and-data-science -innovation -ed7ccd1001f4 (дата звернення : 08.12.2025). 27. Каграманова Ю. Як будувати UML -діаграми // DOU. 2022. URL: https://dou.ua/forums/topic/4057 5/ (дата звернення: 09.12.2025). 28. Python documentation // Python. URL: https://docs.python.org/3/ (дата звернення : 10.12.2025). 29. Методичні вказівки щодо розробки та оформлення кваліфікаційної роботи другого (магістерського) рівня вищої освіти за освітньо -професійною програмою «Інформаційні управляючі системи та технології» спеціальності 122 Комп’ютерні науки / Упоряд.: К.Е. Петров, В.М. Левикін, С.Ф. Чалий, М.В. Євланов, В.І. Саєнко, Д.К. Міхнов, А.В. Міхнова, О.В. Чала. Харків : ХНУРЕ , 2024. с. 24. 30. ДСТУ 3008:2015. Інформація та документація. Звіти у сфері науки і техніки. Структура та правила оформлювання. Чинний від 2017 -07-01. Київ : ДП «УкрНДНЦ», 2016. с. 31. 31. ДСТУ 8302:2015. Інформація та документація. Бібліографічне посилання. Загальні положення та правила складання. Чинний від 2016 -07-01. – Вид. офіц. Київ : УкрНДНЦ, 2016. с. 16."}
{"chunk_id": "2025_M_IUS_Suhorukov_DA.pdf:0", "source": "2025_M_IUS_Suhorukov_DA.pdf", "text": "Міністерство освіти і науки України Харківський національний університет радіоелектроніки Факультет Комп ’ютерних наук (повна назва ) Кафедра Інформаційних управляючих систем (повна назва ) КВАЛІФІКАЦІЙНА РОБОТА Пояснювальна записка рівень вищої освіти другий (магістерський ) Дослідження методів формування запитів до великих мовних моделей з використанням векторної бази даних (тема ) Виконав : здобувач 2 року навчання , групи ІУСТм -24-1 Сухоруков Данило Артурович (прізвище , ім’я, по батькові ) Спеціальність 122 Комп ’ютерні науки (код і повна назва спеціальності ) Тип програми освітньо -професійна (освітньо -професійна або освітньо -наукова ) Освітня програма Інформаційні управляючі системи та технології (повна назва освітньої програми ) Керівник : проф . каф. ІУС Чалий С.Ф. (посада , прізвище , ініціали ) Допускається до захисту Зав. кафедри ІУС Петров К.Е. (підпис ) (прізвище , ініціали ) 2025 р. 2 Харківський національний університет радіоелектроніки ЗАТВЕРДЖУЮ : Зав. кафедри (підпис ) “ 24 ” листопада 20 25 р. ЗАВДАННЯ НА КВАЛІФІКАЦІЙНУ РОБОТУ здобувачеві Сухорукову Данилу Артуровичу (прізвище , ім’я, по батькові ) 1. Тема роботи Дослідження методів формування запитів до великих мовних моделей з використанням векторної бази даних затверджена наказом по університету від “ 24 ” листопада 2025 р. № 1055Ст 2. Термін подання здобувачем роботи до екзаменаційної комісії “ 15 ” листопада 2025 р. 3. Вихідні дані до роботи науково -технічні публікації ; джерела інтернету , науково - технічна література , що стосується теми кваліфікаційної роботи 4. Перелік питань , що потрібно опрацювати у роботі Аналіз властивостей великих мовних моделей ; аналіз методів векторного пошуку та формування контексту до великих мовних моделей ; аналіз існуючих методів удосконалення запитів ; постановка задачі дослідження ; підхід до вирішення задачі ; розробка удосконаленого методу формування запитів ; опис інформаційної технології ; імплементація інформаційної технології ; опис програмного модуля ; опис експериментальних даних . Факультет Комп ’ютерних наук Кафедра Інформаційних управляючих систем Рівень вищої освіти другий (магістерський ) Спеціальність 122 Комп ’ютерні науки (код і повна назва ) Тип програми освітньо -професійна (освітньо -професійна або освітньо -наукова ) Освітня програма Інформаційні управляючі системи та технології (повна назва ) 3 КАЛЕНДАРНИЙ ПЛАН № Назва етапів роботиТермін виконання етапів роботиПримітка 1Аналіз властивостей великих мовних моделей 24.11.2025 Виконано 2Аналіз методів векторного пошуку та формування 26.11.2025 Виконано контексту до великих мовних моделей 3Аналіз існуючих методів удосконалення запитів 28.11.2025 Виконано 4Постановка задачі дослідження 30.11.2025 Виконано 5Підхід до вирішення задачі 01.12.2025 Виконано 6Розробка удосконаленого методу формування 03.12.2025 Виконано запитів 7Опис інформаційної технології 06.12.2025 Виконано 8Імплементація інформаційної технології 07.12.2025 Виконано 9Опис програмного модуля 08.12.2025 Виконано 10Опис експериментальних даних 09.12.2025 Виконано 11Оформлення пояснювальної записки 10.12.2025 Виконано 12Здача роботи для перевірки на нормоконтроль 11.12.2025 Виконано 13Підготовка презентації 14.12.2025 Виконано 14Попередній захист 15.12.2025 Виконано 15Захист кваліфікаційної роботи в екзаменаційній 17.12.2025 Виконано комісії Дата видачі завдання 24 листопада 2025р. Здобувач (підпис ) Керівник роботи проф . каф. ІУС Чалий С.Ф. (підпис ) (посада , прізвище , ініціали ) (і ) 4 РЕФЕРАТ Пояснювальна записка кваліфікаційної роботи : 120 с., 13 рис., 12 табл ., 2 дод., 38 джерел . ВЕКТОРНА БАЗА ДАНИХ , ВЕЛИКІ МОВНІ МОДЕЛІ , ІНФОРМАЦІЙНА ТЕХНОЛОГІЯ , КОМПРЕСІЯ КОНТЕКСТУ , ПОВТОРНЕ РАНЖУВАННЯ , СЕМАНТИЧНИЙ ПОШУК , ФОРМУВАННЯ ЗАПИТІВ , RETRIEVAL-AUGMENTED GENERATION. У роботі виконано аналіз сучасного стану об’єкта дослідження . Оглянуто існуючі варіанти задачі формування запитів до великої мовної моделі . Запропоновано модифікацію метода формування контексту до запиту користувача шляхом використання підсумків попередніх діалогів . Об’єктом дослідження кваліфікаційної роботи є процес формування запитів до великих мовних моделей . Предметом дослідження є методи побудови запитів із використанням векторного контексту . Метою роботи є розробка метода формування запитів до LLM з автоматичною інтеграцією релевантного контексту з векторної бази для підвищення якості відповідей . Для досягнення даної методи необхідно вирішити наступні питання : – аналіз методів формування запитів та RAG-підходів ; – розробка методу інтеграції результату векторного пошуку з попередніх бесід у запит користувача до великої мовної моделі ; – розробка інформаційної технології формування запитів до великих мовних моделей ; – експериментальна перевірка розробленого методу . 5 ABSTRACT Master’s thesis: 120 pages, 13 figures, 12 tables, 2 appendices, 38 sources. CONTEXT COMPRESSION, IN FORMATION TECHNOLOGY, LARGE LANGUAGE MODELS, PROMPT ENGINEERING, RE-RANKING, RETRIEVAL-AUGMENTED GENERATION, SEMANTIC SEARCH, VECTOR DATABASE. The paper analyzes the current state of the research objec t. Existing options for forming queries to a large language model are reviewed. A modification of the method for forming context for a user query by using the results of previous dialogues is proposed. The object of the thesis is the process of forming queries to large language models. The subject of the study is methods of constructing queries using vector context. The goal of the work is to develop a method for forming queries to LLM with automatic integration of relevant context from a vector database to improve the quality of responses. To achieve this, the following issues must be addressed: – analysis of query formation methods and RAG approaches; – development of a method for"}
{"chunk_id": "2025_M_IUS_Suhorukov_DA.pdf:1", "source": "2025_M_IUS_Suhorukov_DA.pdf", "text": "the thesis is the process of forming queries to large language models. The subject of the study is methods of constructing queries using vector context. The goal of the work is to develop a method for forming queries to LLM with automatic integration of relevant context from a vector database to improve the quality of responses. To achieve this, the following issues must be addressed: – analysis of query formation methods and RAG approaches; – development of a method for integrating vector search results from previous conversations into a user query to a large language model; – development of information technology for forming queries to large language models; – experimental verification of the developed method. 6 ЗМІСТ С. Скорочення та умовні познаки ………………………………………….. 7 Вступ ...................... ...................... ................................ ................................ 8 1 Аналіз предметної області та постановка задачі .................. ............... 9 1.1 Аналіз властивостей великих мовних моделей ............................ 9 1.2 Аналіз методів векторного пошуку та формування контексту до великих мовних моделей .................................. .............. ........... 19 1.3 Аналіз існуючих методів удосконалення запитів ........................ 34 1.4 Постановка задачі дослідження .................................................... 46 2 Теоретичні основи та розробка методу ................................................ 50 2.1 Підхід до вирішення задачі ............................................................ 50 2.2 Розробка удосконаленого методу формування запитів ............... 56 3 Розробка інформаційної технології формування запитів до великих мовних моделей .................................. .................................... 61 3.1 Опис інформаційної технології ..................................................... 61 3.2 Реалізація інформаційної технології ........................... .................. 63 4 Програмна реалізація та експериментальна перевірка ....................... 72 4.1 Опис програмного модуля ............................................................. 72 4.2 Опис експериментальних даних ................................ .................... 77 Висновки ..................................................................................................... 84 Перелік джерел посилання ......................................................................... 85 Додаток А Лістинг програми …………………………………………..... 89 Додаток Б Графічний матеріал кваліфікаційної роботи ……………….. 98 7 СКОРОЧЕННЯ ТА УМОВНІ ПОЗНАКИ БД – база даних СУБД – система управління базами даних API – application programming interface BERT – bidirectional encoder representations from transformers BM25 – best match 25 FAISS – facebook artificial in telligence similarity search GPT – generative pre-trained transformer HTML – hypertext markup language IR – information retrieval LLM – large language model MRR – mean reciprocal rank RAG – retrieval-augmented generation SBERT – sentence bidirectional encoder representations from transformers TF-IDF – term frequency-inverse document frequency 8 ВСТУП Сучасний етап розвитку інформаційних технологій характеризується стрімким зростанням обсягів неструктурованих текстових даних , які накопичуються в корпоративних сховищах та домен -орієнтованих інформаційних системах . Класичні пошукові технології , орієнтовані переважно на прості статистичні моделі , виявляються недостатньо ефективними для побудови відповідей природною мовою . Поява та швидкий розвиток великих мовних моделей (Large Language Models, LLM) змінили уявлення про можливості автоматизованої обробки тексту , проте практичне використання таких моделей у реальних прикладних системах виявило низку обмежень , пов’язаних із розміром контекстного вікна . Актуальність даної кваліфікаційної роботи зумовлена проблемою LLM зберегти лише обмежений обсяг контексту та не мати доступу до історії взаємодій користувача поза межами певної кількості повідомлень у поточної сесії . Використання векторних баз даних як механізму розширення контексту надає можливість інтегрувати у запит фрагменти попередніх діалогів , що підвищує релевантність відповіді . Метою роботи є розробка метода формування запитів до LLM з автоматичною інтеграцією релевантного контексту з векторної бази для підвищення якості відповідей . Для досягнення даної методи необхідно вирішити наступні питання : – аналіз методів формування запитів та Retrieval-Augmented Generation (RAG)- підходів ; – розробка методу інтеграції результату векторного пошуку з попередніх бесід у запит користувача до великої мовної моделі ; – розробка інформаційної технології формування запитів до великих мовних моделей ; – експериментальна перевірка розробленого методу . 9 1 АНАЛІЗ ПРЕДМЕТНОЇ ОБЛАСТІ ТА ПОСТАНОВКА ЗАДАЧІ 1.1 Аналіз властивостей великих мовних моделей LLM – це різновид генеративних нейронних мереж , здатних продукувати зв’язний текст у відповідь на текстовий запит природною мовою [1]. Взаємодія користувача або зовнішньої системи з LLM здійснюється через формування запиту (prompt), де модель отримує на вхід певний текст – питання , інструкцію чи інший контекст – і генерує на його основі продовження або відповідь . Отже , процес побудови запиту є центральним механізмом спілкування з моделлю , визначаючи на що модель реагуватиме . Правильно сформульований запит надає моделі необхідну інформацію та інструкції , тоді як некоректний або розпливчастий запит може бути інтерпретований неправильно . Іншими словами , якість відповіді LLM значною мірою залежить від того, як користувач сформулював свій запит [2]. Це підтверджується науковим дослідженням , яке показує , що один і той самий мовний модуль може давати різну точність і доречність відповіді залежно від чіткості та повноти інструкції у prompt [2]. Сучасні LLM базуються на трансформерній архітектурі з механізмом самоуваги , запропонованим Васвані у 2017 році [1]. Механізм дає змогу моделі враховувати контекст : при обробці кожного слова (токена ) модель обчислює ваги уваги до інших слів у ввідній послідовності , визначаючи , які саме фрагменти тексту є найрелевантнішими для поточного прогнозу . Завдяки цьому модель будує внутрішні контекстуалізовані подання : вектори , які кодують значення кожного токена з урахуванням оточення [1]. Перед тим як застосувати механізм уваги , текст"}
{"chunk_id": "2025_M_IUS_Suhorukov_DA.pdf:2", "source": "2025_M_IUS_Suhorukov_DA.pdf", "text": "[2]. Сучасні LLM базуються на трансформерній архітектурі з механізмом самоуваги , запропонованим Васвані у 2017 році [1]. Механізм дає змогу моделі враховувати контекст : при обробці кожного слова (токена ) модель обчислює ваги уваги до інших слів у ввідній послідовності , визначаючи , які саме фрагменти тексту є найрелевантнішими для поточного прогнозу . Завдяки цьому модель будує внутрішні контекстуалізовані подання : вектори , які кодують значення кожного токена з урахуванням оточення [1]. Перед тим як застосувати механізм уваги , текст запиту проходить через етап токенізації . Модель розбиває вхідне речення на дрібніші фрагменти – токени , які можуть бути словами , частинами слів або символами . Як правило , використовуються алгоритми підготовки словника 10 на основі підслів , зокрема метод byte-pair encoding. Такий підхід дозволяє кодувати текст різними мовами і справлятися з неологізмами : будь -яке слово розбивається на знайомі підчастини , тож модель не залишає жоден фрагмент запиту невідомим . Кожен токен зі словника має навчене векторне представлення , яке вводиться на вхід трансформера . Далі за допомогою багатошарової самоуваги модель обчислює нові представлення , насичені контекстом сусідніх токенів , і зрештою генерує відповідь , декодуючи ці представлення назад у слова . Важливою особливістю LLM є обмежена довжина контексту – так зване контекстне вікно моделі . Це максимальна кількість токенів включно із токенами запиту та згенерованої відповіді , яку модель може опрацювати за один раз. Історично контекстне вікно Generative Pre-Trained Transformer (GPT) поступово зростало : перші трансформери типово мали ліміт ~512 токенів , GPT-2 збільшив його до 1024, GPT-3 – до 2048 токенів . Сучасні найбільші моделі мають ще ширший контекст ; зокрема , GPT-4 здатний обробляти контекст до 32 768 токенів (близько 50 сторінок тексту ). Обмеження довжини запиту означає , що надто великий обсяг тексту неможливо подати моделі одразу – зайві токени будуть відкинуті або ігноровані . Більше того, дослідження вказують , що продуктивність моделей починає помітно падати при дуже довгих запитах задовго до досягнення граничної довжини . Це пов’язано з явищем «розфокусування уваги »: коли prompt включає надто багато інформації , модель може розгубитися щодо того, що є головним , і втратити частину релевантного контексту . Тому ефективні запити мають балансувати між наданням достатнього контексту і зайвою довжиною , яка перевантажує модель [3-5]. Формулювання prompt’у істотно визначає релевантність , точність і стабільність відповіді LLM. Якщо запит чіткий , конкретний і містить всю необхідну інформацію , модель , як правило , генерує більш точні й доречні відповіді . Навпаки , неоднозначні інтерпретації призводять до невизначеності моделі : вона може домислювати деталі або розуміти 11 питання не так, як того хотів користувач . Наукові роботи зазначають , що двозначні чи оманливі запити підвищують ризик упереджених або неправильних результатів [2]. З іншого боку , спеціальні техніки формулювання запиту допомагають підвищити якість відповіді . Зокрема , було показано , що коли модель отримує в prompt’і приклади розв’язання задач або додаткові підказки до того, як дати остаточну відповідь , результат значно поліпшується [6]. За останні роки виробилося кілька стратегій , як саме подавати завдання моделі у prompt’і, щоб досягти оптимального результату [2]: – zero-shot prompting ( нульовий приклад ); – few-shot prompting ( з кількома прикладами ); – chain-of-thought prompting ( ланцюжок міркувань ); – role prompting ( задання ролі). Zero-shot prompting – простий запит -інструкція без жодних демонстрацій . Модель отримує лише формулювання задачі або питання і повинна на основі своїх знань згенерувати відповідь . Zero-shot режим зручний для простих запитів , але на складних завданнях може давати субоптимальні результати , оскільки модель не завжди розуміє , який формат чи підхід очікується . Few-shot prompting – техніка , коли до інструкції додають невеликі приклади виконання подібного завдання [2]. Зазвичай у prompt включаються одна або кілька пар «вхід – очікуваний вихід » як зразок . Цей метод , вперше масштабно продемонстрований у GPT-3 [6], суттєво покращує результати на нових задачах : модель навчається з контексту розуміти формат відповіді і підхід до розв’язання . Дослідження показали , що few-shot приклади особливо корисні для складних або вузькопредметних завдань [2] – вони дозволяють моделі активувати релевантні патерни знань . Chain-of-thought prompting – метод , що спонукає модель явно розписувати кроки міркування перед остаточною відповіддю . Замість того щоб одразу дати результат , модель спочатку генерує послідовність 12 проміжних логічних кроків або аргументів так званий ланцюжок думок , а вже потім – висновок . Експерименти з LLM показали , що Chain-of-thought значно підвищує точність на арифметичних і логічних задачах – модель менше помиляється , якщо думає вголос [6]. Role prompting – підхід , коли в prompt явно задається певна роль або стиль , від імені якого модель повинна сформулювати відповідь [2]. Ідея в тому , щоб надати моделі контекст щодо потрібного тону , фаху чи аудиторії відповіді . Такі настанови скеровують модель говорити голосом певного експерта чи персонажа , що часто підвищує релевантність і достовірність відповіді у заданій сфері . Role prompting фактично зменшує необхідність"}
{"chunk_id": "2025_M_IUS_Suhorukov_DA.pdf:3", "source": "2025_M_IUS_Suhorukov_DA.pdf", "text": "арифметичних і логічних задачах – модель менше помиляється , якщо думає вголос [6]. Role prompting – підхід , коли в prompt явно задається певна роль або стиль , від імені якого модель повинна сформулювати відповідь [2]. Ідея в тому , щоб надати моделі контекст щодо потрібного тону , фаху чи аудиторії відповіді . Такі настанови скеровують модель говорити голосом певного експерта чи персонажа , що часто підвищує релевантність і достовірність відповіді у заданій сфері . Role prompting фактично зменшує необхідність для моделі здогадуватися про контекст – він явно задається користувачем . Наукові огляди зазначають , що рольові інструкції є одним з базових інструментів prompt engineering, нарівні з few-shot та chain-of-thought [2]. Перелічені методи можна поєднувати . Наприклад , користувач може задати моделі роль і навести кілька прикладів запитань -відповідей юридичного характеру , а також попросити обґрунтувати відповідь крок за кроком . В сучасних дослідженнях з’являються й інші варіації , наприклад , Tree-of-thoughts ( дерево можливих кроків міркувань ) чи автоматичний підбір оптимальних prompt’ ів, але всі вони базуються на згаданих фундаментальних підходах [2]. Незважаючи на успіхи підходів вище , prompt engineering залишається творчим процесом , пов’язаним із низкою труднощів . Користувач може ненавмисно сформулювати запит так, що він містить неясності або двозначності . Модель не має справжнього розуміння намірів і може інтерпретувати неоднозначний prompt не тим чином , як очікувалося . Неоднозначні або неконкретні prompt’ и підвищують ризик отримати нерелевантну чи помилкову відповідь [2]. Інша схожа проблема – контекстні пастки , коли користувач пропускає в запиті важливий контекст чи припускається фактичної помилки . Модель покірно використовує даний їй текст і може розвинути цю помилку далі. Таким чином , якість prompt’у 13 безпосередньо впливає на якість відповіді , і одне з ключових правил – робити запити максимально однозначними , точними та самодостатніми . При багатокроковій взаємодії , коли користувач веде діалог з LLM, задаючи серію запитань виникає проблема підтримання контексту . Модель пам’ятає лише те, що вміщується в її контекстне вікно – решта історії відсікається . Якщо на початку розмови були задані важливі деталі , але потім діалог триває і перевищує ліміт пам’яті, модель може забути ключову інформацію , надану раніше . Це призводить до того, що відповіді на пізніші запити можуть втрачати зв’язок з попередніми уточненнями . Наприклад , обговорюючи технічний проект через 20 повідомлень , модель може не згадати початкові вимоги , якщо вони вже випали з контексту . Тому розробники часто змушені штучно рекапітулювати контекст у нових запитах або використовувати зовнішнє збереження стану діалогу . Оптимізація цього аспекту – активна тема досліджень , зокрема розробка моделей із розширеною пам’яттю та методів , що дозволяють динамічно вибирати , яку частину контексту зберегти в prompt. Один з найбільш відомих викликів – це тенденція LLM вигадувати неправдиві або не підтверджені факти , коли вони не впевнені у відповіді [7]. Такі помилки дістали назву «галюцинації ». В контексті формування запитів це означає , що модель може заповнювати прогалини у prompt’і власними здогадками . Якщо у запиті бракує потрібних даних , або питання виходить за межі знань моделі , LLM все одно згенерує відповідь – але вона може не мати опори в реальності . Наприклад , якщо спитати про біографію маловідомої особи , модель може вигадати правдоподібні , але хибні факти замість того, щоб зізнатися в незнанні . Частково зменшити галюцинації можна за рахунок більш релевантних prompt’ів: надаючи моделі контекст або джерела знань у запиті , ми зменшуємо її схильність вигадувати [7]. Також важливо формулювати питання так, щоб вони не заохочували модель до спекуляцій . Проте повністю позбутися цього явища лише інженерією prompt’у важко – додатково потрібні вдосконалені моделі чи інтеграція 14 перевірки фактів . Як зазначалося , надто довгі або складні запити можуть негативно вплинути на модель . Перевантаження prompt’ у зайвою інформацією ускладнює моделю виявлення суті питання [5]. Модель може розпорошити увагу на другорядні деталі і дати менш чітку відповідь . Більше того, великий prompt наближає до межі контекстного вікна , що не тільки технічно обмежує довжину відповіді , а й може призвести до часткової втрати раніше введеної інформації . Наприклад , коли користувач намагається «втиснути » в один запит декілька завдань чи дуже детальні дані, є ризик , що модель відповість лише на частину з них або зосередиться не на тому . З цієї причини рекомендується давати структуровані та лаконічні prompt’ и: розбивати складне завдання на кілька послідовних запитів або видаляти з prompt другорядні фрагменти , які можна опустити . У наукових експериментах було виявлено , що продуктивність моделей знижується вже при довжині вводу понад кількасот токенів , навіть якщо формально це ще вписується в межі контексту [4]. Процес підготовки prompt’у відіграє критичну роль в архітектурі сучасних інформаційних систем на основі LLM. Зокрема , технологія Retrieval-Augmented Generation (RAG) передбачає , що перед подачею запиту в модель відбувається етап пошуку зовнішньої інформації , релевантної запиту [7]. Формування prompt’ у у RAG включає злиття двох компонентів : запиту користувача та знайдених даних"}
{"chunk_id": "2025_M_IUS_Suhorukov_DA.pdf:4", "source": "2025_M_IUS_Suhorukov_DA.pdf", "text": "У наукових експериментах було виявлено , що продуктивність моделей знижується вже при довжині вводу понад кількасот токенів , навіть якщо формально це ще вписується в межі контексту [4]. Процес підготовки prompt’у відіграє критичну роль в архітектурі сучасних інформаційних систем на основі LLM. Зокрема , технологія Retrieval-Augmented Generation (RAG) передбачає , що перед подачею запиту в модель відбувається етап пошуку зовнішньої інформації , релевантної запиту [7]. Формування prompt’ у у RAG включає злиття двох компонентів : запиту користувача та знайдених даних з бази знань . По суті, система автоматично розширює оригінальний запит , додаючи до нього контекст , – наприклад , найсвіжіші новини , статті або записки , що стосуються питання [7]. Потім об’єднаний prompt надходить до мовної моделі , і модель генерує відповідь , спираючись як на своє треноване розуміння мови , так і на надані факти . Таким чином , prompt стає точкою інтеграції між нейронною моделлю та традиційними базами знань . Якість сформованого prompt’ у особливо важлива : якщо додати нерелевантний або занадто об’ємний текст 15 із бази даних (БД), це може заплутати модель . Навпаки , влучно підібраний фактичний контекст у prompt’і різко знижує ризик галюцинацій і підвищує достовірність відповіді [7]. RAG- підхід вже успішно застосовується для створення чат-ботів , що надають користувачам відповіді з посиланням на джерела : модель генерує текст , а поруч виводяться документи , з яких було взято інформацію . Це підвищує довіру до системи , адже користувач може перевірити першоджерела . Методи формулювання запитів до LLM становлять ключовий елемент ефективної взаємодії з LLM, оскільки саме спосіб подання завдання визначає точність , релевантність і стабільність відповіді . У науковій літературі виокремлюють низку стратегій prompt engineering, що дозволяють керувати поведінкою моделі , зменшувати неоднозначність інтерпретації та підвищувати інформативність вихідних даних . Крім того, сучасні системи все частіше інтегрують зовнішні джерела знань (наприклад , у RAG-архітектурах ), що розширює функціональні можливості prompt’ів і знижує ризики галюцинацій . Порівняльну характеристику основних методів промптингу , яка систематизує їхні характеристики , сильні сторони та обмеження наведено у таблиці 1.1. 16 Таблиця 1.1 – Порівняльна характеристика основних методів промптингу Метод Сутність Переваги Недоліки Zero- shot prompting Метод формує запит без прикладів ; спирається на загальні знання та індуктивні патерни , активовані лише формулюванням інструкції . Структура : лаконічна інструкція або питання . Лаконічність і відсутність додаткових прикладів зменшують когнітивне й контекстне навантаження ; метод зберігає релевантність у стандартних задачах завдяки опорі на універсальні патерни . Обмежена структура prompt’у не актуалізує специфічні патерни міркування ; зростає ризик некоректного трактування формату або підходу , особливо в складних і вузьких задачах . Few- shot prompting У структуру запиту додаються зразки виконання схожих завдань , які задають очікуваний стиль , логіку та формат відповіді . Приклади задають явний шаблон міркування , що спрямовує метод використовувати патерни з вищою релевантністю у складних завданнях . Великий обсяг прикладів збільшує витрати контексту ; якість відповіді стає залежною від точності й доречності наданих зразків ; можливе перенесення артефактів зі зразків . 17 Продовження таблиці 1.1 Метод Сутність Переваги Недоліки Chain-of - thought prompting Метод навмисно активує покрокове пояснення , фіксуючи проміжні міркування як частину структури відповіді . Примусове розгортання кроків міркування , що покращує прозорість процесу та релевантність у логічних багатокрокових завданнях ; полегшує виявлення помилок у ході думки . Розширена форма відповіді збільшує витрати контексту ; детальність не усуває ризику хибних висновків , а лише робить їх довшими . Role prompting У prompt’і окреслюється роль , тон і цільова аудиторія , що задає стиль міркування та рівень деталізації . Рольова рамка спрямовує метод на добір релевантних патернів мовлення й структур пояснення ; підвищується адаптованість відповіді до визначеної аудиторії . Нечітко окреслена роль створює надмір свободи інтерпретації й може змінити очікувану глибину або стиль відповіді ; спосіб подання може підсилювати приховані упередження . 18 Кінець таблиці 1.1 Метод Сутність Переваги Недоліки RAG- орієнтоване формування prompt’у Метод поєднує користувацький запит із релевантними фрагментами з векторної бази знань ; джерела явно інтегруються у структуру prompt’у. Вставлені фрагменти зменшують ймовірність галюцинацій , адже метод спирається на актуальний корпус ; прозорість джерел підвищує верифікованість і релевантність відповіді у складних інформаційних задачах . Релевантність залежить від якості відбору фрагментів ; можливе перенасичення контексту або введення другорядної інформації ; збільшується складність побудови системи . 19 1.2 Аналіз методів векторного пошуку та формування контексту до LLM З розвитком сучасних моделей штучного інтелекту та стрімким зростанням обсягів неструктурованих даних все більшої ваги набувають векторні БД – спеціалізовані системи управління базами даних (СУБД ), що зберігають інформацію у вигляді високорозмірних числових векторів [8]. Кожен такий вектор є семантичним представленням певного об’єкта : тексту , зображення , аудіо тощо , отриманим шляхом перетворення сирих даних за допомогою моделі або алгоритму , який виконує відображення у векторне представлення (пер. з англ . embedding) [8-9]. На відміну від традиційних реляційних чи документо -орієнтованих СУБД , що оперують чітко структурованими даними та точним співставленням значень , векторні БД призначені для"}
{"chunk_id": "2025_M_IUS_Suhorukov_DA.pdf:5", "source": "2025_M_IUS_Suhorukov_DA.pdf", "text": "– спеціалізовані системи управління базами даних (СУБД ), що зберігають інформацію у вигляді високорозмірних числових векторів [8]. Кожен такий вектор є семантичним представленням певного об’єкта : тексту , зображення , аудіо тощо , отриманим шляхом перетворення сирих даних за допомогою моделі або алгоритму , який виконує відображення у векторне представлення (пер. з англ . embedding) [8-9]. На відміну від традиційних реляційних чи документо -орієнтованих СУБД , що оперують чітко структурованими даними та точним співставленням значень , векторні БД призначені для роботи з неструктурованими даними і забезпечують швидкий пошук за змістовною подібністю . Замість точного порівняння рядків чи полів , пошук здійснюється шляхом обчислення відстаней або міри схожості між векторами у багатовимірному просторі [8, 10]. Такий семантичний пошук дозволяє знаходити інформацію , релевантну запиту за змістом , навіть якщо вона не містить прямих збігів за ключовими словами . Векторна БД слугує пам’яттю для семантичних ознак : вона зберігає вектори , що кодують значення та контекст об’єктів , і дозволяє виконувати операції порівняння цих векторних представлень . Embedding – це числовий вектор , компоненти якого містять закодовану інформацію про зміст і контекст текстового фрагмента . Мета такого відображення – розмістити семантично подібні тексти близько один до одного у векторному просторі . Для побудови embedding- векторів використовуються сучасні моделі глибокого навчання . Наприклад , модель Bidirectional Encoder Representations from Transformers (BERT) генерує контекстуальні представлення слів і речень , враховуючи оточення кожного 20 слова в тексті [9]. На основі BERT було створено спеціалізовані моделі для sentence- embedding – зокрема архітектура Sentence-BERT (SBERT), що поєднує дві копії мережі BERT у сіамську мережу . SBERT перетворює ціле речення у вектор розміру , наприклад , 768 чи 384, який є змістовним представленням цього речення ; при цьому мережу навчено так, щоб близькі за змістом речення мали вектори , близькі за косинусною мірою [11]. Завдяки цьому пошук найбільш схожих речень у колекції з десятків тисяч документів , який за допомогою вихідного BERT вимагав би величезних обчислювальних затрат , із SBERT виконується за частки секунди [11]. Окрім моделей на основі BERT, активно застосовуються інші трансформерні моделі Sentence Transformers, наприклад , Family of Models від HuggingFace або MiniLM, які теж генерують sentence- embedding. OpenAI пропонує комерційні моделі для embedding, наприклад , text- embedding-ada-002, навчені на величезних обсягах різноманітного тексту ; такі вектори добре відображають семантичні зв’язки і стали де-факто стандартом для багатьох застосувань , зокрема у системах retrieval- augmented generation. Незалежно від конкретної моделі , результатом є векторне представлення , що стискає зміст тексту у наборі чисел . Головна перевага полягає в тому , що ці вектори можна порівнювати між собою математично , знаходячи семантично близькі тексти через метрики відстані . Щоб визначити , наскільки два embedding- вектори схожі , використовують кілька поширених метрик : евклідова відстань , косинусна подібність та скалярний добуток . Евклідова відстань – це геометрична відстань між точками в просторі , що враховує різницю між значеннями всіх координат ; вона залежить і від напрямку , і від величини векторів [12]. Для двох векторів евклідова відстань визначається за формулою : ݀൫⃗ܽ,ܾሬ⃗൯=ඥ(ܽଵ−ܾଵ)ଶ+(ܽଶ−ܾଶ)ଶ+ … + (ܽ௡−ܾ௡)ଶ, 21 де ݀൫⃗ܽ,ܾሬ⃗൯ – евклідова відстань між векторами ⃗ܽ та ܾሬ⃗. ܽ௜ – ݅-та компонента вектора ⃗ܽ ; ܾ௜ – ݅-та компонента вектора ܾሬ⃗; ݊ – розмірність векторного простору . Косинусна подібність вимірює кут між векторами і визначається як їх скалярний добуток , поділений на добуток довжин (норм ) цих векторів [12]: cos൫⃗ܽ,ܾሬ⃗൯=⃗ܽ∗ܾሬ⃗ ‖‖⃗ܽ∗ฮܾሬ⃗ฮ, (1) де cos൫⃗ܽ,ܾሬ⃗൯ – косинусна міра подібності між векторами ; ܽ௜∗ܾ௜ – скалярний добуток векторів ⃗ܽ та ܾሬ⃗; ‖‖⃗ܽ – довжина (норма ) вектора ⃗ܽ ; ฮܾሬ⃗ฮ – довжина (норма ) вектора ܾሬ⃗. Значення косинусної подібності лежить у діапазоні від −1 до 1: значення 1 відповідає нульовому куту (вектори співнаправлені і максимально схожі за змістом ), 0 – вектори ортогональні (семантично не пов’язані ), −1 – вектори протилежно спрямовані [12]. Важливо , що косинусна метрика звертає увагу лише на напрям векторів , ігноруючи їх довжину . Це зручно , оскільки при нормуванні всіх embedding- векторів до однакової довжини результуюче значення подібності залежить тільки від семантичного вмісту , а не від масштабів чисел . Скалярний добуток двох векторів – це сума попарних добутків їх компонент . Він пропорційний косинусній подібності , але також враховує довжини векторів [12]: ⃗ܽ∗ܾሬ⃗=ܽଵܾଵ+ܽଶܾଶ+. . . +ܽ௡ܾ௡, 22 де ⃗ܽ∗ܾሬ⃗ – скалярний добуток двох векторів ⃗ܽ та ܾሬ⃗; ܽ௜ – ݅-та компонента вектора ⃗ܽ ; ܾ௜ – ݅-та компонента вектора ܾሬ⃗; ݊ – розмірність векторного простору . Якщо вектори нормалізовано (довжина кожного дорівнює 1), скалярний добуток чисельно співпадає з косинусною мірою . Формула для визначення довжини (норми ) вектора : ‖‖⃗ܽ=ටܽଵଶ+ܽଶଶ+. . . +ܽ௡ଶ, де ‖‖⃗ܽ – довжина (норма ) вектора ⃗ܽ ; ܽ௜ – ݅-та компонента вектора ⃗ܽ ; ݊ – кількість координат (розмірність простору ). У практичних реалізаціях вибір метрики часто залежить від того, з якою функцією втрат навчалася модель , що породжує embeddings. Як правило , для моделей , оптимізованих під косинусну подібність"}
{"chunk_id": "2025_M_IUS_Suhorukov_DA.pdf:6", "source": "2025_M_IUS_Suhorukov_DA.pdf", "text": ". Якщо вектори нормалізовано (довжина кожного дорівнює 1), скалярний добуток чисельно співпадає з косинусною мірою . Формула для визначення довжини (норми ) вектора : ‖‖⃗ܽ=ටܽଵଶ+ܽଶଶ+. . . +ܽ௡ଶ, де ‖‖⃗ܽ – довжина (норма ) вектора ⃗ܽ ; ܽ௜ – ݅-та компонента вектора ⃗ܽ ; ݊ – кількість координат (розмірність простору ). У практичних реалізаціях вибір метрики часто залежить від того, з якою функцією втрат навчалася модель , що породжує embeddings. Як правило , для моделей , оптимізованих під косинусну подібність , такий самий критерій застосовують і при пошуку (альтернативно – виконують нормалізацію векторів і використовують скалярний добуток , що еквівалентно ) [12]. Аналогічно , якщо модель навчалася максимізувати скалярний добуток між схожими прикладами або мінімізувати евклідову відстань , то доцільно використовувати ту саму міру і на етапі пошуку . Загалом , евклідова відстань інтерпретується як абсолютна міра різниці , корисна , коли важлива величина ознак , косинусна подібність – як міра збіжності напрямків фокусується на самих патернах ознак , ігноруючи їх масштаб , а скалярний добуток поєднує обидва аспекти . Для завдань семантичного пошуку тексту найпоширенішою є косинусна подібність , оскільки вона добре відображає схожість тематики чи сенсу двох текстів , не зважаючи на їхню довжину [12]. 23 Початкову основу для sparse-пошуку становить векторна модель документів з вагуванням термів за схемою Term Frequency-Inverse Document Frequency (TF-IDF), вперше системно проаналізована в роботах Дж. Сантана [13]. Нехай корпус містить ܰ документів , документ ݀ представлено як багатошарову множину термів , а ݂(௧,ௗ) позначає кількість входжень терма ݐ у документі ݀ .Одна з класичних формул TF-IDF має вигляд ݂݂݀݅ݐ( ݐ,݀=)݂ݐ(ݐ,݀)∗݂݀݅( ݐ,) де ݂݂݀݅ݐ( ݐ,݀) –зважена оцінка важливості терміна ݐу документі ݀ ; ݂ݐ(ݐ,݀) – частотність терміна ݐу документі ; ݂݀݅(ݐ) – міра унікальності терміна в корпусі документів ; ݐ – термін , для якого виконується оцінка ; ݀ – документ , у якому розглядається термін . Частотність терміна визначає , скільки разів термін зустрічається в документі : ݂ݐ(ݐ,݀=)݂(௧,ௗ), де ݂ݐ(ݐ,݀) –частотність терміна ݐу документі ݀ ; ݂௧,ௗ – кількість входжень терміна ݐу документ ݀ ; ݐ – термін у тексті ; ݀ – документ , для якого обчислюється частотність . Зворотна частотність документа (Inverse Document Frequency) показує , наскільки рідкісним є термін ݐу всьому корпусі : ݂݀݅(ݐ=)ܰ݃݋݈ ݊௧, 24 де ݂݀݅(ݐ) –міра рідкісності терміна ݐ ; ܰ – загальна кількість документів у корпусі ; ݊௧ – кількість документів , що містять термін ݐ ; ݐ – термін , для якого оцінюється рідкісність . Така вага зростає зі збільшенням частоти терма в документі та зменшується , якщо терм часто зустрічається в корпусі , що забезпечує підсилення дискримінативних термів і придушення стоп -слів. У векторній моделі схожість між запитом ݍ та документом ݀ часто визначається за косинусною мірою . Подібна схема вагування забезпечує високу лексичну точність : документи , що містять точні збіги ключових термів із запитом , отримують високі бали , а індексування через інвертовані списки дає змогу досягати високої швидкості пошуку навіть у масштабних колекціях . Подальший розвиток probabilistic relevance framework привів до появи ранжувальної функції Best Match 25 (BM25), яка на сьогодні є стандартом де-факто для sparse- пошуку в інформаційно -пошукових системах . Робертсон і Сарагоса виводять BM25 як аппроксимацію до двопуассонівської моделі терм -частот із урахуванням насичення частоти терма та нормалізації за довжиною документа [14]. У класичній формі оцінка релевантності документа ݀ відносно запиту ݍ записується як: ܯܤ25(ݍ,݀=)෍݂݀݅஻ெଶହ (௧)݂௧,ௗ(݇ଵ+1 ) ݂௧,ௗ+݇ଵ(1−ܾ+ ܾ|݀| 〈݀〉) ௧∈௤ де ܯܤ25(ݍ,݀) – оцінка релевантності документа ݀ запиту ݍ ; ݍ – текст запиту ; ݀ – документ , для якого обчислюється оцінка релевантності ; ݐ – термін , що входить до складу запиту ݍ ; 25 ݂݀݅஻ெଶହ (ݐ) – зворотна частотність документа для терміна ݐ ; ݂௧,ௗ – частота входження терміна ݐу документ ݀ ; ݇ଵ – параметр BM25, що визначає ступінь згладжування частоти ; ܾ – параметр BM25, що контролює вплив довжини документа ; |݀| – довжина документа ݀ або кількість термінів документа ݀ ; ⟨݀⟩ – середня довжина документа в колекції . Формула , що реалізує модифіковану оцінку інверсної документної частоти , більш стійку до крайніх випадків : ݂݀݅஻ெଶହ (௧)=ܰ݃݋݈−݊௧+0 . 5 ݊௧+0 . 5 де ݂݀݅஻ெଶହ (ݐ) –зворотна частотність документа для терміна ݐ ; ܰ – загальна кількість документів у колекції , од.; ݊௧ – кількість документів , що містять термін ݐ ,од. Нелінійний компонент : ݂௧,ௗ(݇ଵ+1 ) ݂௧,ௗ+݇ଵ(1−ܾ+ ܾ|݀| 〈݀〉) де ݂௧,ௗ – частота входження терміна ݐ у документ ݀ ; ݀ – документ , для якого обчислюється оцінка релевантності ; ݇ଵ – параметр BM25, що визначає ступінь згладжування частоти ; ܾ – параметр BM25, що контролює вплив довжини документа ; |݀| – довжина документа ݀ або кількість термінів документа ݀ ; ⟨݀⟩ – середня довжина документа в колекції . Нелінійний компонент моделює насичення : додаткові входження терма після певного порогу дають дедалі менший приріст ваги, а параметр ܾ контролює ступінь нормалізації на довжину документа . Така конструкція 26 дозволяє зберігати високу"}
{"chunk_id": "2025_M_IUS_Suhorukov_DA.pdf:7", "source": "2025_M_IUS_Suhorukov_DA.pdf", "text": "– документ , для якого обчислюється оцінка релевантності ; ݇ଵ – параметр BM25, що визначає ступінь згладжування частоти ; ܾ – параметр BM25, що контролює вплив довжини документа ; |݀| – довжина документа ݀ або кількість термінів документа ݀ ; ⟨݀⟩ – середня довжина документа в колекції . Нелінійний компонент моделює насичення : додаткові входження терма після певного порогу дають дедалі менший приріст ваги, а параметр ܾ контролює ступінь нормалізації на довжину документа . Така конструкція 26 дозволяє зберігати високу швидкість пошуку завдяки інвертованому індексу , забезпечує стійкість до варіацій довжини документів і демонструє стабільні результати на класичних колекціях Text Retrieval Conference, де BM25 досі слугує сильним базовим підходом , з яким порівнюють сучасні нейронні моделі [15]. Sparse-методи характеризуються трьома ключовими властивостями . По-перше , висока швидкість досягається завдяки компактному інвертованому індексу , де складність першого етапу пошуку наближено оцінюється як ࣩ(|ݍ| ݃݋݈ ܰ) де ࣩ –асимптотична складність ; |ݍ| – довжина запиту ; ܰ – кількість елементів у колекції . при ефективній організації списків . По-друге , лексична точність забезпечує чутливість до точних входжень ключових термів , що особливо важливо для доменів із стабільною термінологією . По-третє , обмежене розуміння семантики зумовлене припущенням «bag-of-words» та відсутністю моделювання порядку слів і контексту ; документи , які формулюють ту саму інформацію за допомогою синонімів або перефразувань , можуть отримувати низькі оцінки , що прямо відображається на якості відповідей LLM у RAG-сценаріях . Розвиток нейронного інформаційного пошуку продемонстрував , що семантичні щільні представлення (dense embeddings) значно підвищують якість на задачах запитання -відповідь і passage retrieval, проте не витісняють повністю лексичні сигнали . У роботах Ліна та Ліна запропоновано dense- фреймворк для одночасного врахування лексичного та семантичного збігу , де комбінуються високорозмірні sparse-вектори та компактні dense-вектори у єдину схему оцінювання [16]. Гібридний пошук (hybrid search) у 27 загальному випадку можна формалізувати як: ݏ௛௬௕(ݍ,݀=)ߣ ݏ௦௣௔௥௦௘ (ݍ,݀(+)1−ߣ )ݏௗ௘௡௦௘ (ݍ,݀) де ݏ௛௬௕(ݍ,݀) –гібридна оцінка релевантності документа ݀ запиту ݍ ; ݍ – текст запиту ; ݀ – документ , для якого обчислюється оцінка релевантності ; ߣ – ваговий коефіцієнт комбінування ; ݏ௦௣௔௥௦௘ (ݍ,݀) – розріджена (BM25 або інша ) оцінка релевантності ; ݏௗ௘௡௦௘ (ݍ,݀) – щільна (векторна ) оцінка релевантності . Деякі сучасні системи замість простої лінійної комбінації використовують більш складні агрегатори , наприклад , спільне навчання нейронних енкодерів , які оптимізують сумарну втрату за sparse- і dense- сигналами [17]. Таке комбінування дає змогу зберігати сильну чутливість до рідкісних термів , характерну для BM25, і одночасно покривати випадки семантичного розриву , коли формулювання запиту та документа істотно відрізняються . Експериментальні дослідження на колекціях ad-hoc-пошуку демонструють , що гібридні системи перевершують як чисто sparse-, так і чисто dense- рітрівери за показниками nDCG@10 та Mean Reciprocal Rank (MRR@10), особливо в багатомовних і крос -доменних сценаріях [18]. Водночас гібридні методи мають підвищену складність конфігурації : потрібна узгоджена нормалізація різнорідних оцінок , налаштування параметра ߣ ,вибір і оптимізація індексних структур (інвертований індекс для sparse-частини і наближений пошук найближчих сусідів для dense-частини ), а також контроль латентності при об’єднанні кількох каналів пошуку . У Retrieval-Augmented Generation процес пошуку завершується формуванням контексту , який передається на вхід LLM. Нехай ܴ(ݍ ) повертає впорядковану множину топ-݇ документів : 28 ܴ(ݍ=){݀ଵ,݀ଶ,…,݀௞}, де ܴ(ݍ) –множина документів , отриманих у відповідь на запит ݍ ; ݍ – текст запиту ; ݀௜ – ݅-й документ у цій множині ; ݇ – кількість документів . Ця множина впорядковується за мірою релевантності : ݏ(݀ଵ)≥ݏ(݀ଶ)≥⋯≥ݏ( ݀௞), де ݏ(݀௜) – оцінка релевантності документа ݀௜; ݀௜ – ݅-й документ у ранжованому списку ; ݇ – кількість документів у списку . LLM має обмежене контекстне вікно розміром ܮ токенів , тому дизайн механізмів формування контексту безпосередньо впливає на точність відповіді та обчислювальну вартість . Найпростішою стратегією є пряма конкатенація топ-݇ результатів . У цьому випадку формується контекст ܥ௖௢௡௖௔௧ (ݍ=)ܿ݊ݑݎݐ௅(݀ଵ⊕݀ଶ⊕…⊕݀௞)( 2) де ܥ௖௢௡௖௔௧ (ݍ) –конкатенований контекст для запиту ݍ ; ݍ – текст запиту ; ܿ݊ݑݎݐ௅ – оператор обрізання до довжини ܮ ; ݐܽܿ݊݋ܿ – операція послідовного об’єднання документів ; ݀௜ – ݅-й документ у множині ; ݇ – кількість документів ; ܮ – максимальна довжина сформованого контексту . Такий підхід використано як базовий у початкових реалізаціях RAG- моделей , де на вхід генеративному трансформеру подавалися кілька 29 найрелевантніших пасажів з BM25- або dense-рітрівера [19]. Простота реалізації , відсутність додаткових моделей і мінімальна затримка роблять конкатенацію привабливою як стартове рішення . Водночас неоптимальне використання контекстного вікна проявляється у двох аспектах : по-перше , довгі документи з високим рейтингом можуть повністю заповнити вікно й витіснити інші релевантні фрагменти ; по-друге , нерелевантні або слаборелевантні частини тексту потрапляють у контекст і створюють шум , що підвищує ризик галюцинацій моделі й погіршує точність відповіді . Сучасні оглядові роботи з RAG-систем демонструють , що навіть при незмінному рітрівері оптимізація стратегії відбору й організації контексту дає суттєве зростання точності на задачах відкритого запитання -відповіді . Семантичне перевпорядкування (re-ranking) розв’язує частину зазначених проблем за рахунок додаткового етапу оцінювання топ-݇ документів"}
{"chunk_id": "2025_M_IUS_Suhorukov_DA.pdf:8", "source": "2025_M_IUS_Suhorukov_DA.pdf", "text": "документи з високим рейтингом можуть повністю заповнити вікно й витіснити інші релевантні фрагменти ; по-друге , нерелевантні або слаборелевантні частини тексту потрапляють у контекст і створюють шум , що підвищує ризик галюцинацій моделі й погіршує точність відповіді . Сучасні оглядові роботи з RAG-систем демонструють , що навіть при незмінному рітрівері оптимізація стратегії відбору й організації контексту дає суттєве зростання точності на задачах відкритого запитання -відповіді . Семантичне перевпорядкування (re-ranking) розв’язує частину зазначених проблем за рахунок додаткового етапу оцінювання топ-݇ документів більш потужною моделлю , зазвичай cross-encoder ом на базі BERT-подібного трансформера . У типовій схемі sparse- або гібридний рітрівер формує кандидатний список ܴ(ݍ ,)після чого для кожної пари (ݍ,݀௜) обчислюється семантична оцінка : ݏ௖௘(ݍ,݀௜)=݂ఏ([ݍ,݀௜]) де ݏ௖௘(ݍ,݀௜)– оцінка релевантності , отримана моделлю зі спільного кодування ; ݍ –текст запиту ; ݀௜ – ݅-й документ , для якого обчислюється оцінка релевантності ; ݂ఏ – модель зі параметрами ߠ ; [ݍ,݀௜] – об’єднане подання запиту ݍ та документа ݀௜. Документи перевпорядковуються за ݏ௖௘(ݍ,݀௜), і вже перші ′݇ ≤ ݇ передаються в контекст LLM. Ногейра та Чо показали , що використання BERT-cross-encoder’ а для перевпорядкування BM25-кандидатів на колекції Microsoft Machine Reading Comprehension дає відносний приріст MRR@10 30 приблизно на 27 % порівняно з найкращими на той момент підходами , що підкреслює ефективність re-ranking’ у як другого етапу [15]. Подальші дослідження в галузі нейронного Information Retrieval (IR) підтвердили , що багатоступеневі архітектури «легкий рітрівер + важкий re-ranker» забезпечують оптимальний компроміс між латентністю й якістю , особливо коли re-ranker застосовується лише до десятків або сотень кандидатів [15]. Використання re-ranking’ у в RAG-підсистемах формування контексту призводить до помітного покращення узгодженості між питанням і відібраними фрагментами . У термінах контексту можна вважати , що функція ܴ(ݍ )замінюється композицією ܴ௥௘௥௔௡௞ (ݍ=)݌݋ܶ௞ᇲቀܴܴ݇݊ܽ݁൫ܴ(ݍ)൯ቁ де ܴ௥௘௥௔௡௞ (ݍ) –підмножина документів після повторного ранжування ; ݍ – текст запиту ; ܴܴ݇݊ܽ݁ – процедура повторного ранжування ; ݌݋ܶ௞ᇲ – оператор вибірки ݇ᇱелементів ; ݇ᇱ – кількість документів , що відбираються після повторного ранжування . Важливим технологічним аспектом є зрозумілість причин за яких був обран фрагмент контекста . Непрозорі методи ранжування можуть знижувати довіру користувача до системи , за причини недостатьої аргументації відбору . Така проблема інтерпритованості звичайна для LLM моделей та re-rank`у на основі LLM [20]. Окремий клас методів формування контексту пов’язаний із компресією (context compression), метою якої є зменшення кількості токенів , що подаються на вхід LLM, при збереженні максимальної кількості корисної інформації . Логічним формалізмом служить оператор компресії , який будує коротший текст так, що зберігається інформація , релевантна до 31 запиту . Узагальнений контекст тоді набуває вигляду згідно з формулою (2). У роботі Token Compression Retrieval-Augmented Large Language Model запропоновано дві комплементарні стратегії : абстрактивну «summarization compression» на базі T5-моделі , що генерує скорочені резюме документів , та семантичну компресію , яка вилучає слова з низьким впливом на семантичний зміст [21]. Експерименти показують , що узгоджена компресія дає змогу зменшити обсяг контексту приблизно на 65 % без погіршення , а подекуди й із невеликим покращенням точності відповіді , тоді як більш агресивне скорочення токенів (приблизно на 80 %) призводить до контрольованого , але помітного падіння показників точності . У роботі EXIT запропоновано контекст -орієнтовану екстрактивну компресію , де кожне речення з retrieved- документів класифікується як «залишити » або «видалити » з урахуванням глобального контексту документа і запиту , причому рішення приймаються паралельно [22]. Такий підхід демонструє , що правильне видалення нерелевантних речень здатне покращити і точність відповідей , і швидкодію системи порівняно з некорованим контекстом , оскільки LLM фокусується на більш щільній , інформативній підмножині даних . Компресія контексту у практичних RAG-системах забезпечує економію токенів і, відповідно , зменшення вартості та латентності запитів до LLM, однак супроводжується неминучим ризиком втрати критичних доказів , особливо для багатокрокових запитів чи задач , що потребують строгих посилань на першоджерела . Оглядові роботи з RAG-reasoning підкреслюють , що компресійні механізми мають проєктуватися у зв’язці з рітрівером і генератором , інакше розрив між прихованими припущеннями різних компонентів може призводити до нових типів галюцинацій . Таблиця 1.2 демонструє , що вибір конкретної комбінації методів пошуку та формування контексту має базуватися на компромісі між обчислювальною вартістю , вимогами до латентності та цільовими показниками якості відповіді LLM. 32 Таблиця 1.2 – Порівняльна характеристика основних методів пошуку Назва методу Сутність Основні переваги Основні недоліки TF-IDF / BM25 Лексичний пошук на основі інвертованого індексу з оцінюванням релевантності через ваги термінів . Низька обчислювальна вартість завдяки операціям зі списками постингів ; інтерпретованість і стабільність результатів ; висока ефективність для корпусів із чітко визначеною термінологією . Лексична природа методу не дає змоги врахувати семантичні еквіваленти чи перефразування ; релевантність різко знижується в міждоменних або контекстно складних запитах . Гібридний пошук Поєднання лексичних та семантичних сигналів , отриманих від sparse - і dense -ретриверів . Синергія dense- та sparse-компонентів забезпечує стійкішу релевантність , особливо в складних запитах ; комбінування різних сигналів збільшує покриття варіативних формулювань . Вищі обчислювальні витрати , оскільки потрібні дві моделі та"}
{"chunk_id": "2025_M_IUS_Suhorukov_DA.pdf:9", "source": "2025_M_IUS_Suhorukov_DA.pdf", "text": "стабільність результатів ; висока ефективність для корпусів із чітко визначеною термінологією . Лексична природа методу не дає змоги врахувати семантичні еквіваленти чи перефразування ; релевантність різко знижується в міждоменних або контекстно складних запитах . Гібридний пошук Поєднання лексичних та семантичних сигналів , отриманих від sparse - і dense -ретриверів . Синергія dense- та sparse-компонентів забезпечує стійкішу релевантність , особливо в складних запитах ; комбінування різних сигналів збільшує покриття варіативних формулювань . Вищі обчислювальні витрати , оскільки потрібні дві моделі та операції інтеграції ; додаткові вимоги до пам’яті й калібрування ваг між сигналами . 33 Кінець таблиці 1.2 Назва методу Сутність Основні переваги Основні недоліки Конкатенація top-k Передавання у моделний запит простого списку k відібраних фрагментів без подальшої обробки чи фільтрації . Мінімальні обчислювальні витрати : метод передбачає лише вибір і послідовне з’єднання фрагментів . Висока ймовірність шуму метода : серед поданих фрагментів можуть міститися відомості з низькою релевантністю , що призводить до витіснення важливої інформації . Semantic re- ranking Додаткове ранжування відібраних top-k фрагментів через cross -encoder, що оцінює семантичну відповідність запиту . Cross-encoder забезпечує детальніше семантичне зіставлення , що підвищує якість добору фрагментів та, релевантність RAG- відповідей . Висока обчислювальна вартість методу : кожна пара запита та фрагмента потребує повного проходження моделі ; метод погано масштабується з ростом k. Компресія контексту Скорочення або агрегування відібраних фрагментів перед переданням у запит , з метою зменшення обсягу контексту . Метод зменшує використання контекстного вікна : зниження кількості токенів без значної втрати ; включає більше джерел у запит . Додатковий етап обробки збільшує обчислювальні витрати ; існує ризик втрати змістових деталей у разі надмірної компресії . 34 1.3 Аналіз існуючих методів удосконалення запитів Функціонування систем на основі LLM у режимі доступу до зовнішньої документальної бази визначається двома фундаментальними обмеженнями : фіксованим розміром контекстного вікна моделі та, дуже великим обсягом корпусу документів . Для реалістичних корпоративних чи галузевих сховищ вимірюється сотнями тисяч або мільйонами фрагментів , тоді як контекстне вікно навіть для сучасних LLM не перевищує десятків тисяч токенів . У дослідженнях , присвячених порівнянню підходів довгого контексту і RAG, підкреслюється , що саме обмежене контекстне вікно створює обмеження для вбудовування зовнішньої інформації , а отже , робить якість формування запиту та структурування корпусу ключовими факторами продуктивності системи [23]. У загальному випадку запит користувача описується як текстова послідовність , а корпус документів представляється або у вигляді сирих текстів , або у вигляді множини фрагментів (чанків ). Задача етапу пошуку в RAG-конвеєрі полягає у виборі підмножини фрагментів , що максимізує релевантність до запиту при обмеженні на сумарну довжину ෍ℓ(ܿ) ࣷఢࣝ≤ܮ௖௧௫− ℓ(ݍ)−ℓ(ݐ݌݉݋ݎ݌ системи ) де ܿ – фрагмент тексту ; ࣝ – множина всіх контекстних фрагментів , які планується включити в prompt; ℓ(ܿ) – довжина фрагмента ܿ у токенах ; ∑ℓ(ܿ) ࣷఢࣝ – сумарна кількість токенів , що займають усі фрагменти контексту ; ܮ௖௧௫ – максимальна допустима довжина контекстної частини prompt’a, 35 визначена системними або модельними обмеженнями ; ℓ(ݍ) – довжина запиту користувача ݍ ; ℓ(ݐ݌݉݋ݎ݌ системи ) – довжина системного prompt’a ( наприклад , інструкцій або preamble). Для сучасних векторних методів кожному запиту ݍ та фрагменту ܿ зіставляється векторна репрезентація , а релевантність оцінюється , косинусною подібністю згідно з формулою (1). За прямого застосування векторного пошуку до всіх фрагментів ܥ обчислювальна складність оцінки релевантності масштабується як ࣩ(|ࣝ|∗݀)≈ࣩ( ܰ∗݉∗݀) де ࣩ – асимптотична складність ; |ࣝ| – кількість елементів у множині контекстів ࣝ ; ݀ – розмірність векторного подання ; ܰ – кількість документів ; ݉ – середня кількість токенів на документ . Для великих ∣ܦ∣ таке рішення виявляється надто затратним , тому формування запитів до LLM в умовах великого корпусу та обмеженого вікна контексту неможливо розглядати ізольовано від структурування самого пошуку . У сучасній літературі виділяється низка спеціалізованих методів , які змінюють як сам запит , так і подання корпусу перед взаємодією з LLM [24]. Ієрархічні схеми векторного пошуку пропонують розділити задачу на два рівні : спочатку виконується відбір релевантних документів , а далі – більш тонкий пошук релевантних фрагментів всередині вже звуженої множини кандидатів . У роботах з Dense Hierarchical Retrieval та його гібридних модифікацій описано архітектуру , у якій застосовується окремий документний та пасажний ретривери , що спільно формують багаторівневий простір подання [25]. 36 Формально кожний документ ݀௝ перетворюється у вектор , використвоуючи функцію отримання векторного подання : ݒ௝ௗ௢௖=݃(݀௝)∈ℝௗ೏೚೎ де ݒ௝ௗ௢௖– векторне подання ݆-го документа ; ݃ – функція отримання векторного подання ; ݀௝ – ݆-й документ ; ℝௗ೏೚೎ – простір векторів розмірності ݀ௗ௢௖; ݀ௗ௢௖ – розмірність векторного подання документа . На першому етапі запит кодується у вектор після чого знаходиться множина індексів документів з найбільшою подібністю ܬ௤=ܭ݌݋ܶ௞೏೚೎ቊݏௗ௢௖൫ݍ,݀௝൯=〈݂ௗ௢௖(ݍ,)ݒ௝ௗ௢௖〉 ‖݂ௗ௢௖(ݍ)‖ฮݒ௝ௗ௢௖ฮቋ де ܬ௤– множина документів , відібраних для запиту ݍ ; ܭ݌݋ܶ௞೏೚೎ – оператор вибірки перших ݇ௗ௢௖документів із найбільшою оцінкою ; ݏௗ௢௖(ݍ,݀௝) – оцінка подібності між запитом ݍі документом ݀௝; ݂ௗ௢௖(ݍ) – векторне подання запиту ; ݒ௝ௗ௢௖ – векторне подання ݆-го документа ; ݇ௗ௢௖ – кількість документів , що відбираються . На другому етапі"}
{"chunk_id": "2025_M_IUS_Suhorukov_DA.pdf:10", "source": "2025_M_IUS_Suhorukov_DA.pdf", "text": "розмірності ݀ௗ௢௖; ݀ௗ௢௖ – розмірність векторного подання документа . На першому етапі запит кодується у вектор після чого знаходиться множина індексів документів з найбільшою подібністю ܬ௤=ܭ݌݋ܶ௞೏೚೎ቊݏௗ௢௖൫ݍ,݀௝൯=〈݂ௗ௢௖(ݍ,)ݒ௝ௗ௢௖〉 ‖݂ௗ௢௖(ݍ)‖ฮݒ௝ௗ௢௖ฮቋ де ܬ௤– множина документів , відібраних для запиту ݍ ; ܭ݌݋ܶ௞೏೚೎ – оператор вибірки перших ݇ௗ௢௖документів із найбільшою оцінкою ; ݏௗ௢௖(ݍ,݀௝) – оцінка подібності між запитом ݍі документом ݀௝; ݂ௗ௢௖(ݍ) – векторне подання запиту ; ݒ௝ௗ௢௖ – векторне подання ݆-го документа ; ݇ௗ௢௖ – кількість документів , що відбираються . На другому етапі для кожного документа відібраного для запиту , документ розбивається на фрагменти , для яких обчислюються вектори . Далі проводиться ранжування фрагментів , а до контексту LLM потрапляють фрагменти з найбільшими оцінками . Така двоетапна схема використовує макрорівневу семантику документа на першому кроці та мікрорівневу семантику фрагментів на 37 другому [25]. Обчислювальна вигода ієрархічного підходу проявляється у зміні асимптотики , для якої складність визначається як: ࣩ(ܰ∗݀ௗ௢௖)+ࣩ(݇ௗ௢௖∗݉∗݀ ௙௥௔௚ ) де ࣩ –асимптотична складність ; ܰ – кількість документів ; ݀ௗ௢௖ – розмірність векторного подання документа ; ݇ௗ௢௖ – кількість вибраних документів ; ݉ – середня кількість фрагментів у документі ; ݀௙௥௔௚ – розмірність векторного подання фрагмента . Емпіричні дослідження демонструють , що така стратифікація дає змогу одночасно скоротити час відповіді та підвищити Recall@k порівняно з суто пасажним ретривером , зокрема в задачах відкритого запитально - відповідного пошуку [25]. Водночас двоетапний характер пошуку породжує систематичний ризик втрати релевантних фрагментів . Якщо документний ретривер помилково відкидає документ , що містить критично важливий пасаж , то навіть ідеальний фрагментний ретривер не зможе його відновити , оскільки відповідний документ не потрапив до множини документів відяібраних для запиту ݍ .У роботах з гібридного ієрархічного пошуку пропонується пом’якшувати цю проблему через комбінування щільних та лексичних ознак вже на документному рівні , наприклад шляхом лінійної або навчуваної комбінації BM25-оцінки та векторної подібності , що дозволяє краще узгоджувати випадки , де первинна семантична близькість не супроводжується лексичним перетином термінів . Для формування запиту до LLM та вибору контексту такий ієрархічний підхід означає , що конструктор промпта оперує не одиничними фрагментами , а структурованими сутностями : списком документів -свідків 38 із відповідними фрагментами та, за потреби , їх метаданими . У результаті контекст передається моделі не як довільна конкатенація чанків , а як напівупорядкована множина , що покращує інтерпретованість відповіді й дозволяє виконувати додаткові операції , наприклад , агрегацію або фільтрацію за джерелами . Інший напрямок оптимізації формування запитів до LLM в умовах великого корпусу пов’язаний із розширенням початкового запиту перед векторним пошуком . У сучасних роботах пропонується використовувати LLM як генератор додаткових текстових репрезентацій запиту – синонімів , перефразувань , потенційних відповідей або псевдодокументів , які потім слугують основою для побудови багатших векторів запиту [26]. Нехай початковий запит кодується у вектор . LLM, використовуючи внутрішні параметричні знання та заданий промпт , генерує множину текстів , що інтерпретуються як розширення запиту та задають відносну довіру до початкового формулювання та окремих розширень : ݂ሚ(ݍ=)ܽ ݂(ݍ+)∑ߚ௜݂(݁௜)ெ ௜ୀଵ ܽ+∑ߚ௜ெ ௜ୀଵ де ݂ሚ(ݍ) –скориговане векторне подання запиту ݍ ; ܽ – ваговий коефіцієнт базового подання запиту ; ݂(ݍ) – векторне подання запиту ; ߚ௜ – ваговий коефіцієнт для прикладу ݁௜; ݂(݁௜) – векторне подання прикладу ݁௜; ܯ – кількість прикладів , що використовуються в корекції . У щільних ретриверах подібні лінійні або ядрові комбінації векторів забезпечують більш плавне заповнення семантичного проміжку між запитом і документами , які використовують відмінну термінологію [26]. У класичних задачах інформаційного пошуку query expansion традиційно підтримувався через псевдорелевантний зворотний зв’язок або 39 орієнтацію на термінологію з початкової вибірки документів . Сучасні дослідження демонструють , що LLM-орієнтований підхід суттєво підвищує Recall@k і nDCG як для лексичних , так і для щільних ретриверів , оскільки модель генерує не лише словоформи , а й фрази , які відображають приховану інтенцію користувача [27]. Зокрема , у роботах , які досліджують багатотекстову інтеграцію розширень , показано , що використання декількох незалежних семплів LLM з подальшою агрегацією підвищує стійкість до генерацій нерелевантних відповідей і покращує якість пошуку як у доменних , так і в позадоменних наборах даних [27]. Разом з тим розширення запиту LLM-методами має низку суттєвих недоліків , релевантних саме сценаріям з обмеженим контекстним вікном . По-перше , кожен виклик LLM для генерації збільшує латентність системи , а при використанні складних промптів – і вартість обчислень . По-друге , розширений запит інколи приводить до введення нерелевантних термінів , що у векторному просторі проявляється як додатковий шум вектора запиту , зменшення кута між скоригованим векторним поданням та широкою множиною документів і, як наслідок , падіння точності при фіксованому ݇ . У деяких дослідженнях зафіксовано , що надмірно агресивне розширення , зокрема через генерацію довгих псевдодокументів , покращує покриття , але погіршує ранжування верхньої частини списку [28]. У контексті формування запиту до LLM така стратегія використовується двома способами . У першому варіанті розширення застосовується лише на етапі побудови векторної репрезентації , а власне текст запиту , який бачить LLM-генератор , залишається компактним , що дозволяє зекономити токени у"}
{"chunk_id": "2025_M_IUS_Suhorukov_DA.pdf:11", "source": "2025_M_IUS_Suhorukov_DA.pdf", "text": "широкою множиною документів і, як наслідок , падіння точності при фіксованому ݇ . У деяких дослідженнях зафіксовано , що надмірно агресивне розширення , зокрема через генерацію довгих псевдодокументів , покращує покриття , але погіршує ранжування верхньої частини списку [28]. У контексті формування запиту до LLM така стратегія використовується двома способами . У першому варіанті розширення застосовується лише на етапі побудови векторної репрезентації , а власне текст запиту , який бачить LLM-генератор , залишається компактним , що дозволяє зекономити токени у вікні контексту . У другому варіанті частина згенерованих розширень явно включається у промпт у вигляді альтернативних формулювань питання , що полегшує для моделі інтерпретацію інформаційної потреби , проте збільшує розмір запиту і зменшує доступний обсяг для релевантних фрагментів . Оптимальний баланс між цими двома режимами залежить від домену , розміру корпусу та 40 вартості обчислень . Третім ключовим компонентом , що безпосередньо впливає на формування запитів і вибір контексту , є стратегія сегментації документів . У простих реалізаціях RAG документи розбиваються на фіксовані за довжиною шматки , що добре узгоджується з вимогами до довжини дліни контекста , але часто руйнує семантичну цілісність . Унаслідок цього фрагменти , релевантні одному логічному твердженню , можуть потрапляти до різних чанків , а LLM змушена відновлювати цілісну картину з частково несумісних фрагментів . Adaptive chunking, або адаптивна семантична сегментація , пропонує будувати межі чанків не за фіксованим числом токенів , а з урахуванням семантичної зв’язності сусідніх речень чи абзаців . У низці робіт показано , що стратегія , яка враховує локальну семантичну подібність між сусідніми реченнями , істотно покращує як точність ретривера , так і кінцеві метрики запитально -відповідних систем [28]. Нехай документ представляється послідовністю речень , а кожному реченню відповідає вектор який належить до множини векторів БД. Визначається функція подібності , наприклад косинусна , а межа чанка встановлюється в позиції ݐ ,якщо ߪ(ݑ௧,ݑ௧ାଵ)<ߠ де ߪ(ݑ௧,ݑ௧ାଵ)– міра подібності між елементами ݑ௧та ݑ௧ାଵ; ݑ௧ – поточний елемент послідовності ; ݑ௧ାଵ – наступний елемент послідовності ; ݐ – індекс позиції елемента у послідовності ; ߠ – порогове значення подібності . Таким чином кожен чанк містить послідовність речень , між якими зберігається висока внутрішня семантична когерентність : У роботах , присвячених впливу документної сегментації на RAG, 41 демонструється , що навіть проста евристика на основі подібності сусідніх речень здатна підвищити F1 і EM на низці наборів даних , оскільки релевантна інформація рідше залишається між чанками [28]. У більш просунутих підходах адаптивне chunking поєднується з ієрархічною сегментацією та графовою кластеризацією . Прикладом є підхід , у якому спочатку нейронна модель сегментації визначає межі базових сегментів , а потім ці сегменти розглядаються як вершини графа . Ребро між вершинами ݅ та ݆ додається , якщо їх подібність перевищує адаптивний поріг ߬ =ߤ +ߪ݇ де ߬ –порогове значення ; ߤ – середнє значення вибірки ; ݇ – масштабний коефіцієнт ; ߪ – стандартне відхилення . Подальший пошук максимальних кліків і їх об’єднання у кластери дозволяє утворювати чанки , які одночасно зберігають семантичну єдність і послідовний порядок у документі [29]. Такий підхід створює низку переваг у контексті обмеженого ܮ௖௧௫. По- перше , кожен чанк містить логічно завершений фрагмент знання , що зменшує потребу включати в промпт зайві сусідні частини для підтримки релівантності . По-друге , ієрархічна структура чанків (від англ . chunk) (наприклад , «розділ → підрозділ → абзац ») дає можливість комбінувати adaptive chunking з ієрархічним векторним пошуком : спочатку обираються великі сегменти -кандидати , потім уточнення виконується на рівні менших підчанків , які краще вкладаються у контекстне вікно . Нарешті , семантично орієнтована сегментація знижує ризик змішування тем діалогу , коли один чанк містить фрагменти , релевантні різним запитам , що особливо критично для медичних та юридичних доменів з високими вимогами до точності відбору доказів [30]. 42 Складність adaptive chunking полягає у значних обчислювальних витратах на попередню обробку корпусу та у неоднорідності розміру чанків . У випадку динамічного визначення меж окремі чанки можуть бути суттєво довшими за середнє значення , що ускладнює подальшу оптимізацію розміщення декількох фрагментів у межах дліни контексту . Для компенсації використовуються додаткові обмеження на максимальну довжину чанка або вторинні операції компресії [28]. Узгоджене застосування ієрархічного векторного пошуку , LLM- орієнтованого query expansion та adaptive chunking формує цілісний підхід до побудови запитів в умовах великого корпусу й обмеженого контекстного вікна . Ієрархічний пошук зменшує обсяг фрагментів , серед яких здійснюється відбір , і таким чином знижує обчислювальну складність без істотної втрати покриття . Query expansion з LLM розширює семантику запиту , підвищуючи ймовірність потрапляння релевантних документів до множини кандидатів . Adaptive chunking, у свою чергу , забезпечує подання документів у вигляді фрагментів , які краще відповідають природній структурі знань і більш ефективно використовують обмежений обсяг контексту . У подальшому підрозділі доцільно узагальнити наведені методи у вигляді порівняльної таблиці , де для кожного з них буде вказано основний принцип дії, ключові параметри , типові умови застосування (розмір корпусу , вимоги до латентності , домен ), а також"}
{"chunk_id": "2025_M_IUS_Suhorukov_DA.pdf:12", "source": "2025_M_IUS_Suhorukov_DA.pdf", "text": "LLM розширює семантику запиту , підвищуючи ймовірність потрапляння релевантних документів до множини кандидатів . Adaptive chunking, у свою чергу , забезпечує подання документів у вигляді фрагментів , які краще відповідають природній структурі знань і більш ефективно використовують обмежений обсяг контексту . У подальшому підрозділі доцільно узагальнити наведені методи у вигляді порівняльної таблиці , де для кожного з них буде вказано основний принцип дії, ключові параметри , типові умови застосування (розмір корпусу , вимоги до латентності , домен ), а також характерні недоліки , зокрема ризик втрати релевантних фрагментів на документному етапі для ієрархічного пошуку , складність реалізації й неоднорідність розміру чанків для adaptive chunking. Узагальнені порівняльні характеристики розглянутих методів формування запитів у RAG-системах наведено у таблиці 1.3. 43 Таблиця 1.3 – Порівняльна характеристика методів формування запитів у RAG-системах Назва методу Сутність Типові умови застосування в RAG та LLM-системах Вплив методу на формування запитів та роботу контекстного вікна Ієрархічний векторний пошук Метод застосовує двоетапний ретривал : спочатку відбираються релевантні документи як цілісні одиниці , а далі працює з їхніми внутрішніми чанками , звужуючи простір пошуку до підкорпусу . Метод особливо ефективний у дуже великих корпусах та в системах , де суворі вимоги до латентності не дозволяють виконувати повний dense - пошук . Він також добре працює з документами , що мають виразну ієрархічну структуру Метод формує запит у два логічні контури : верхній рівень визначає корпус для подальшого аналізу , а нижній рівень самі фрагменти документів . Контекстне вікно LLM наповнюється лише структурно пов’язаними та взаємно узгодженими фрагментами . Нерелевантні документи метод усуває ще до етапу формування контексту , що робить контекст щільним та більш керованим . 44 Продовження таблиці 1.3 Назва методу Сутність Типові умови застосування в RAG та LLM-системах Вплив методу на формування запитів та роботу контекстного вікна Query expansion з використанням LLM Метод утворює множину варіантів запиту , синонімічних формулювань чи псевдодокументів та об'єднує їх у агрегований вектор , який підсилює семантичне охоплення області пошуку . Метод найчастіше використовують у доменах з підвищеною вимогою до повноти : у сферах із великою термінологічною варіативністю , зі сталими стилістичними відмінностями між джерелами або там, де формулювання запитів суттєво змінюється залежно від користувача . У векторному режимі метод тримає текст запиту компактним : розширення відбувається у векторному просторі й не збільшує вартість токенів . У текстовому режимі метод частково вбудовує розширення в prompt, збільшуючи контекстний обсяг , але формує запит більш деталізованим та семантично збалансованим , що збільшує релевантність вибірки . 45 Кінець таблиці 1.3 Назва методу Сутність Типові умови застосування в RAG та LLM-системах Вплив методу на формування запитів та роботу контекстного вікна Adaptive chunking Метод сегментує документи на чанки змінної довжини , використовуючи семантичну когерентність , структурні межі чи результати кластеризації , щоб кожен чанк репрезентував логічно завершену одиницю змісту . Метод добре працює з корпусами , де внутрішня структура документів нерівномірна . Метод особливо ефективний у системах , де індексація виконується офлайн і дозволяє оптимізувати сегментацію наперед . Метод наповнює контекстне вікно інформативними й семантично цілісними блоками . Через це контекст містить меншу кількість фрагментів , але кожен із них передає значно більше змісту . 46 1.4 Постановка задачі дослідження Актуальність дослідження зумовлена тим, що сучасні великі мовні моделі , попри істотне зростання кількості параметрів і продемонстровані можливості few-shot та zero-shot навчання , залишаються жорстко обмеженими за розміром контекстного вікна та не мають вбудованих механізмів довготривалої пам’яті. Як показано у роботах , присвячених моделям типу GPT-3, масштабування параметричної частини моделі суттєво покращує узагальнювальну здатність , однак не усуває залежності якості відповіді від повноти й коректності сформульованого запиту та поданого контексту , особливо у знаннєво -інтенсивних задачах . За відсутності доступу до історії попередніх взаємодій із користувачем короткі або слабо структуровані запити призводять до поверхових , частково релевантних або галюцинаторних відповідей , оскільки модель оперує лише тією частиною інформації , яка поміщається в поточне вікно уваги . У цьому контексті концепція RAG розглядається як формальний підхід до поєднання параметричної пам’яті мовної моделі з зовнішньою непараметричною пам’яттю , організованою у вигляді векторного індексу текстових фрагментів . Векторні бази даних у такій архітектурі виконують функцію семантичного сховища знань , де документи та фрагменти попередніх діалогів представлені у вигляді щільних embedding - векторів , що дає змогу здійснювати пошук за семантичною подібністю , а не лише за лексичним збігом . Порівняльні дослідження щільних методів пошуку демонструють їх принципову перевагу над класичними розрідженими моделями на основі TF-IDF та BM25 за показниками якості відбору релевантних контекстів для відкритих задач запитання –відповідь . При цьому методи TF-IDF та BM25, що історично є базовими для текстового пошуку , продовжують демонструвати обмежену спроможність до відображення семантичних еквівалентів , варіативності формулювань і 47 міждисциплінарних зв’язків . У RAG-системах проблема релевантності загострюється тим, що якість відповіді великої мовної моделі жорстко детермінована точністю й структурованістю набору фрагментів , включених до промпту . Надмірний , частково нерелевантний або слабо структурований контекст призводить до перевантаження контекстного вікна другорядною інформацією й зменшує частку"}
{"chunk_id": "2025_M_IUS_Suhorukov_DA.pdf:13", "source": "2025_M_IUS_Suhorukov_DA.pdf", "text": "релевантних контекстів для відкритих задач запитання –відповідь . При цьому методи TF-IDF та BM25, що історично є базовими для текстового пошуку , продовжують демонструвати обмежену спроможність до відображення семантичних еквівалентів , варіативності формулювань і 47 міждисциплінарних зв’язків . У RAG-системах проблема релевантності загострюється тим, що якість відповіді великої мовної моделі жорстко детермінована точністю й структурованістю набору фрагментів , включених до промпту . Надмірний , частково нерелевантний або слабо структурований контекст призводить до перевантаження контекстного вікна другорядною інформацією й зменшує частку інформативних токенів , тоді як недостатній або занадто агресивно відфільтрований контекст не дозволяє активувати наявні в параметричній частині моделі доменні знання . Відсутність повноцінної довготривалої пам’яті, що системно враховує зміст попередніх діалогів з конкретним користувачем , додатково зумовлює втрату контекстно значущих нюансів предметної області , які можуть істотно змінювати інтерпретацію запиту . Унаслідок цього виникає завдання формалізації процесу формування запиту як керованого механізму адаптивної інтеграції релевантного векторного контексту , зокрема контексту з попередніх сесій , за умови мінімізації шуму , надлишковості та структурної неузгодженості включених фрагментів . Об’єктом дослідження є процес формування запитів до великих мовних моделей , який розглядається як впорядкована послідовність операцій аналізу , нормалізації , трансформації та збагачення початкового тексту користувача з урахуванням доменної специфіки , поточної інформаційної потреби та накопиченої історії взаємодій . Предметом дослідження виступають методи побудови запитів із використанням векторного контексту , а саме принципи організації зовнішньої семантичної пам’яті на основі векторної бази даних , алгоритми векторного пошуку та ранжування , а також правила відбору , узгодження й структурованого включення обраних фрагментів у промпт до LLM за умов обмеженого розміру контекстного вікна . Об’єкт дослідження у формалізованому вигляді можна визначити як процес формування запитів до великих мовних моделей , а методи дослідження – як сукупність методів формування запитів з використанням векторної бази даних , що забезпечують автоматизовану 48 інтеграцію релевантного контексту різної природи в єдину запитну конструкцію . Метою роботи є розроблення формалізованого методу формування запитів до великих мовних моделей з автоматичною інтеграцією релевантного контексту з векторної бази даних , орієнтованого на підвищення точності , повноти , логічної узгодженості та доменної релевантності згенерованих відповідей порівняно з традиційними техніками prompt engineering і базовими RAG-реалізаціями . Наукова новизна роботи полягає у теоретичному обґрунтуванні та побудові методу інтеграції результатів векторного пошуку , здійсненого за історією попередніх взаємодій користувача , у поточний запит до великої мовної моделі . На відміну від відомих рішень , у яких зовнішня пам’ять представлена статичним набором документів або слабко персоналізованою базою знань , запропонований підхід трактує багатосесійний діалог як динамічне векторне сховище персоналізованих знань і контекстів , що оновлюється в процесі взаємодії та використовується для побудови семантично насиченого промпту . Метод задається у вигляді формалізованого конвеєра , який включає побудову embedding- представлень поточного запиту та релевантних фрагментів попередніх бесід , виконання векторного пошуку з урахуванням доменної специфіки , адаптивне ранжування знайдених фрагментів за семантичною близькістю й інформативністю , відсікання шумових і надлишкових елементів та формування деталізованого промпту зі структурованим включенням обраного контексту . У межах такого підходу контекст , що передається моделі , постає не як набір випадково або вручну підібраних фрагментів , а як формально скоординована структура , оптимізована за критеріями релевантності . Практичне значення одержаних результатів полягає у створенні прототипу інформаційної технології формування розширених запитів до великих мовних моделей , у якій модуль векторного пошуку та векторна база даних інтегровані як невід ’ємні компоненти підсистеми побудови промптів . 49 Така технологія реалізує функції зовнішньої довготривалої семантичної пам’яті, забезпечує персоналізовану реконструкцію контексту на основі попередніх діалогів та дає змогу підвищити стабільність і прогнозованість поведінки LLM-орієнтованих систем у прикладних сценаріях – від інтелектуальних освітніх платформ і корпоративних асистентів знань до доменних діалогових агентів зі складною міждисциплінарною предметною областю . Сформульована таким чином науково -прикладна проблема відсутності формалізованого методу формування запитів , здатного одночасно адаптивно інтегрувати релевантний векторний контекст , у тому числі з попередніх діалогів , систематично усувати шумову та надлишкову інформацію і забезпечувати стабільно високий рівень доменної точності й логічної узгодженості відповідей , визначає актуальність проведеного дослідження . Метою роботи є розробка метода формування запитів до LLM з автоматичною інтеграцією релевантного контексту з векторної бази для підвищення якості відповідей . Для досягнення даної методи необхідно вирішити наступні питання : – аналіз методів формування запитів та RAG-підходів ; – розробка методу інтеграції результату векторного пошуку з попередніх бесід у запит користувача до великої мовної моделі ; – розробка інформаційної технології формування запитів до великих мовних моделей ; – експериментальна перевірка розробленого методу . 50 2 ТЕОРЕТИЧНІ ОСНОВИ ТА РОЗРОБКА МЕТОДУ 2.1 Підхід до вирішення задачі Для підвищення релевантності відповідей LLM пропонується динамічно доповнювати промпт актуальною інформацією з довготривалої пам’яті діалогу . Зовнішню пам’ять реалізовано у вигляді векторної БД, де зберігаються фрагменти історичних діалогів у вигляді векторних представлень . Такий підхід належить до класу методів RAG – генерації тексту на основі зовнішніх знань , що зберігаються у диференційовано доступній пам’яті [31, 32]. Під час нового запиту модель здійснює пошук у векторному просторі за семантичною"}
{"chunk_id": "2025_M_IUS_Suhorukov_DA.pdf:14", "source": "2025_M_IUS_Suhorukov_DA.pdf", "text": "50 2 ТЕОРЕТИЧНІ ОСНОВИ ТА РОЗРОБКА МЕТОДУ 2.1 Підхід до вирішення задачі Для підвищення релевантності відповідей LLM пропонується динамічно доповнювати промпт актуальною інформацією з довготривалої пам’яті діалогу . Зовнішню пам’ять реалізовано у вигляді векторної БД, де зберігаються фрагменти історичних діалогів у вигляді векторних представлень . Такий підхід належить до класу методів RAG – генерації тексту на основі зовнішніх знань , що зберігаються у диференційовано доступній пам’яті [31, 32]. Під час нового запиту модель здійснює пошук у векторному просторі за семантичною схожістю та вибирає релевантні фрагменти діалогу із бази , додаючи їх до контексту запиту . За рахунок цього штучний співрозмовник отримує доступ до фактів і деталей , які виходять за межі короткострокової пам’яті, але були раніше згадані або збережені . В результаті покращується зв’язність та точність відповіді : модель може враховувати ключові відомості з історії спілкування , підтримувати тему , пам’ятати уподобання користувача [33] та уникати суперечностей , спричинених забуванням старої інформації [34]. Таким чином , поєднання LLM з векторною базою знань слугує основою пропонованого рішення . Embedding становлять собою векторні репрезентації текстових фрагментів , у яких кожний вимір відображає певний аспект їхньої семантичної структури . На рисунку 1.1 подано типовий процес перетворення сирих текстів у такі представлення : кожен фрагмент незалежно подається на вхід embedding- моделі , яка відображає його у багатовимірний простір фіксованої розмірності . У цьому просторі близькість між векторами корелює зі смисловою подібністю відповідних текстів , а віддаленість сигналізує про їхню концептуальну різнорідність . 51 Рисунок 1.1 – Схема представлення текста у вигляді embbeding Кожен елемент отриманого вектора є числовою ознакою , сформованою внаслідок багаторівневої трансформації тексту , що включає аналіз контексту , латентних залежностей та семантичних патернів . Таким чином , навіть тексти , які різняться лексично , але збігаються за змістом або мають спільний тематичний профіль , відображаються у векторні області , що розташовані поряд . Після обчислення embedding система заносить їх до векторної бази даних . У цій базі кожний вектор функціонує як компактний і придатний до пошуку запис , що узагальнює семантичний зміст відповідного тексту . Це забезпечує можливість виконувати порівняння та відбір релевантних фрагментів не за поверхневими словесними ознаками , а за їхніми позиціями у високовимірному семантичному просторі . У RAG-системах така організація даних є критичною : вона дозволяє методу ефективно знаходити фрагменти , змістово наближені до запиту , навіть якщо формулювання користувача суттєво відрізняється від тих, що містяться у корпусі . Саме завдяки embedding система отримує можливість здійснювати пошук за смислом , а не за збігами слів. Це дозволяє їй інтегрувати у контекст відповіді ті частини діалогу чи знань , що є релевантними з погляду 52 змісту , підтримуючи когерентність та тематичну узгодженість взаємодії зі співрозмовником . Розроблений підхід має відповідати трьом ключовим вимогам : релевантності , масштабованості та адаптивності . Векторна БД повинна повертати саме ті фрагменти діалогу , що найбільш значущі для поточного запиту . Це вимагає використання якісної функції семантичного зіставлення та відсіювання нерелевантного контексту . Для забезпечення цього вводиться функція embedding, яка перетворює текстові дані у вектори фіксованої розмірності : ܧ:ℝ→ܶௗ(3) де ܧ –функція , що відображає текст ܶ у векторний простір ℝௗ; ܶ – текстовий об’єкт (рядок , документ тощо ); ℝௗ – простір векторів розмірності ݀ ; ݀ – розмірність векторного подання . Вектори зберігаються у БД, формуючи простір пам’яті. Векторна функція пошуку визначає підмножину найбільш близьких за змістом фрагментів з простору пам’яті на основі метрики подібності . Як метрику обрано косинусну схожість : ݉݅ݏ൫ݒ௤,ݒ௜൯=ݒ௤∗ݒ௜ ฮݒ௤ฮ ‖ݒ௜‖, де ݉݅ݏ(ݒ௤,ݒ௜)– міра подібності між векторами ݒ௤та ݒ௜; ݒ௤ – векторне подання запиту ; ݒ௜ – векторне подання елемента ݅ ; ฮݒ௤ฮ – норма вектора запиту ; ‖ݒ௜‖ – норма вектора елемента . Таким чином , релевантність досягається завдяки тому , що до 53 контексту включаються лише ті фрагменти , які максимально близькі до запиту у семантичному просторі [32]. Метод повинен ефективно працювати при великому обсязі накопичених діалогів . Застосування векторного індексу у базі дозволяє виконувати швидкий пошук навіть за мільйонами фрагментів . Кожен текстовий документ або діалог розбивається на сегменти фіксованої довжини , кожен сегмент векторизується і зберігається у базі [33]. Пошук у такій структурі виконується за сублінійний час, що забезпечує масштабованість рішення при зростанні даних . Метод має динамічно оновлювати пам’ять і підлаштовуватися до нової інформації . Це означає , що після завершення діалогу система додає до бази стислий запис про нього для подальшого використання . Одночасно бажано реалізувати механізми оновлення та забування : менш корисні або застарілі спогади з часом повинні мати меншу вагу або видалятися , тоді як важлива інформація – підкріплюватися . Існують підходи , що використовують експоненційну функцію забування для поступового зменшення значущості старих спогадів і посилення часто запитуваної інформації [35]. Завдяки цьому реалізується людиноподібна пам’ять, здатна вибірково забувати другорядне і акцентувати ключові знання з урахуванням часу . Адаптивність також означає гнучкість щодо різних моделей . Cистема може працювати як з закритими інтерфейсами (Application Programming Interface, API) LLM, так і з"}
{"chunk_id": "2025_M_IUS_Suhorukov_DA.pdf:15", "source": "2025_M_IUS_Suhorukov_DA.pdf", "text": "корисні або застарілі спогади з часом повинні мати меншу вагу або видалятися , тоді як важлива інформація – підкріплюватися . Існують підходи , що використовують експоненційну функцію забування для поступового зменшення значущості старих спогадів і посилення часто запитуваної інформації [35]. Завдяки цьому реалізується людиноподібна пам’ять, здатна вибірково забувати другорядне і акцентувати ключові знання з урахуванням часу . Адаптивність також означає гнучкість щодо різних моделей . Cистема може працювати як з закритими інтерфейсами (Application Programming Interface, API) LLM, так і з відкритими LLM, інтегруючи їх через уніфікований інтерфейс [35]. Зауважимо , що запропонований підхід поєднує сильні сторони двох напрямів : векторного семантичного пошуку та автоматичного реферування тексту . Пошук у векторній базі відповідає за оперативне відтворення релевантного контексту на основі семантики запиту , а побудова текстового prompt’у з врахуванням знайдених даних дозволяє інтегрувати цей контекст у процес генерації відповіді . З іншого боку , після завершення діалогу алгоритм оновлює пам’ять через побудову узагальненого опису , що підтримує якість і актуальність знань у базі. Циклічне застосування цих 54 кроків забезпечує безперервне навчання системи : нові діалоги збагачують базу знань , а актуальні знання з бази підвищують якість наступних діалогів . Таким чином , дотримуються всі визначені вимоги : система надає релевантні відповіді за рахунок пошуку потрібних даних [32], зберігає продуктивність при зростанні обсягу пам’яті завдяки ефективним індексам [33], та адаптується до нової інформації шляхом динамічного резюмування і вибіркового забування [35]. Важливим аспектом є інтеграція знайденого контексту у текст запиту до моделі . Векторний пошук виступає фільтром , який зі всієї сукупності наявних даних обирає невеликий піднабір . Цей піднабір фактично репрезентує собою динамічно сформовану пам’ять для даного кроку діалогу . Далі функція формування prompt’у включає цю пам’ять у вхідне повідомлення . Можна сказати , що відображення задає композицію двох операторів – пошуку та побудови запиту , – яка перетворює початковий запит на розширений запит , що вже містить контекстні знання . Отже , результат пошуку у векторній базі безпосередньо впливає на поведінку мовної моделі : фрагменти , отримані з бази , подаються моделі у тій самій формі , що й звичайна історія діалогу , тому модель не відрізняє інформацію з пам’яті від наданої користувачем чи системою . Це дозволяє інтегрувати знання без змін у параметрах моделі – достатньо лише правильно сформувати prompt. З теоретичної точки зору , такий підхід еквівалентний використанню непараметричної пам’яті у алгоритмі роботи моделі . Параметри LLM зберігають імпліцитні знання , набуті при переднавчанні , тоді як зовнішня векторна пам’ять містить експліцитні знання у зручній для вибірки формі [31]. Комбінування цих двох джерел вхідних даних під час генерації дозволяє досягти кращої точності , прозорості та керованості : інформація в пам’яті може оновлюватися і перевірятися , а модель – оперувати більшим контекстом , ніж це визначено її фіксованим вікном . Отже , пошук у векторному просторі і побудова текстового prompt’ у утворюють єдиний процес , що з’єднує блок IR з блоком Natural Language 55 Processing generation в рамках інтегрованого рішення . Практично це реалізується як частина програмного «ланцюжка » обробки запиту : після отримання питання користувача система викликає модуль пошуку у базі, а потім передає як prompt мовній моделі злитий текст знайденого контексту та самого запитання . Така архітектура відповідає сучасним підходам до розширення контексту LLM, зокрема , техніці RAG, де зовнішній пошуковий модуль поєднується з генеративною моделлю [32]. У результаті модель поводиться як двокомпонентна система : вона спочатку згадує потрібну інформацію з бази знань , і вже на основі цього формує підсумкову відповідь користувачу . Підсумовуючи , запропонований метод можна подати як послідовність узгоджених етапів . Спочатку задається простір збережених знань , що складається з текстових фрагментів та їхніх векторних подань . Поточний діалог описується історією попередніх реплік , до якої додається новий запит користувача . Далі визначається процедура пошуку релевантного контексту , яка для поточного запиту обирає фрагменти , найбільш схожі на нього за мірою семантичної близькості . На основі знайдених індексів формується множина відповідних текстів , що слугують зовнішнім контекстом для моделі . Завершальним етапом є побудова розширеного запиту , у межах якого історія діалогу , відібраний контекст та поточне запитання об’єднуються в єдину текстову структуру , що передається мовній моделі для генерації остаточної відповіді . Функція формування розширеного запиту P об’єднує історію , знайдений контекст і актуальний запит у єдиний текст : ݍ෤=ܲ(ܪ௡ିଵ,ܥ(ݍ,)ݍ) де ݍ෤– скоригований запит ; ܲ – функція перетворення або уточнення запиту ; ܪ௡ିଵ – історія взаємодії до кроку ݊− 1; 56 ܥ(ݍ) – контекст , сформований на основі початкового запиту ݍ ; ݍ – початковий запит . Після завершення діалогу (при отриманні спеціального сигналу завершення або неактивності користувача ) застосовується функція підсумовування ܵ : ݉=ܵ(ܪ௡) де ܪ௡ – повна історія діалогу ; ܵ – функція підсумування ; ݉ – згенерований машинний конспект . Цей результат додається до бази тим самим оновлюючи довготривалу пам’ять системи . 2.2 Розробка удосконаленого методу формування запитів Метою розробки запропонованого методу є подолання виявлених у розділі 1 недоліків"}
{"chunk_id": "2025_M_IUS_Suhorukov_DA.pdf:16", "source": "2025_M_IUS_Suhorukov_DA.pdf", "text": "– контекст , сформований на основі початкового запиту ݍ ; ݍ – початковий запит . Після завершення діалогу (при отриманні спеціального сигналу завершення або неактивності користувача ) застосовується функція підсумовування ܵ : ݉=ܵ(ܪ௡) де ܪ௡ – повна історія діалогу ; ܵ – функція підсумування ; ݉ – згенерований машинний конспект . Цей результат додається до бази тим самим оновлюючи довготривалу пам’ять системи . 2.2 Розробка удосконаленого методу формування запитів Метою розробки запропонованого методу є подолання виявлених у розділі 1 недоліків , зокрема низької релевантності результатів та надлишкового або нерелевантного контексту у відповідях . Загальна ідея полягає в динамічному формуванні запиту шляхом поєднання семантичного векторного пошуку контексту з адаптивними шаблонами prompt’ів. Такий підхід дозволяє автоматично доповнювати початковий запит користувача найбільш доречними фрагментами інформації перед передачею його до мовної моделі , що підвищує точність і змістовність одержуваних відповідей [36, 37]. На першому етапі вхідні текстові дані проходять попередню обробку . Це включає очищення тексту від шумів (спеціальних символів , HyperText Markup Language (HTML) тегів тощо ), нормалізацію (приведення до 57 однакового регістру , усунення зайвих пробілів ), видалення стоп -слів (за потреби ) та токенізацію . Попередня обробка забезпечує уніфікацію вхідного тексту і покращує подальше перетворення векторами . Результатом цього етапу є підготовлений текст запиту ݍ௖௟௘௔௡ , а також корпус документів (база знань ), розбитий на логічні фрагменти і очищений аналогічним чином . Належна сегментація і очищення документів дозволяє виділити самостійні контекстні фрагменти оптимальної довжини (зазвичай кілька сотень токенів ) для ефективного пошуку [38]. На другому етапі кожен текстовий фрагмент перетворюється на вектор ознак фіксованої розмірності ݊ за допомогою моделі embedding. Формально це можна подати як відображення згідно з формулою (3). Для підготовленого запиту спершу визначається його векторне подання . Аналогічним чином для кожного документа бази знань заздалегідь сформовано набір векторів , які відповідають окремим фрагментам документації . Використання навченої модельної функції дає змогу відобразити семантичний зміст кожного текстового елемента у спільний векторний простір . Саме ці векторні подання забезпечують можливість семантичного пошуку , оскільки близькі між собою вектори відповідають фрагментам , що є подібними за змістом . Третій етап полягає у знаходженні фрагментів бази знань , найбільш релевантних до запиту , на основі порівняння векторів . Для кожного вектора документа обчислюється міра схожості з вектором запиту . В якості такої міри використовується косинусна близькість між векторами згідно з формулою (1). Косинусна схожість набуває значення від 0 до 1 (після нормалізації довжин ), причому більш високі значення відповідають вищому семантичному збігу запиту з фрагментом . Далі виконується пошук k- найближчих сусідів – вибір тих фрагментів , чиї embedding- вектори розташовані найближче до вектора запиту за метрикою ܵ .Алгоритм kNN є класичним підходом для вибірки схожих об’єктів за заданою метрикою 58 близькості , і в контексті embedding- пошуку він ефективно знаходить семантично релевантні документи за допомогою структури векторного індексу . В результаті цього етапу формується множина кандидатів на роль контексту запиту – наприклад , множина топ-k фрагментів ранжованих за спаданням міри релевантності Четвертий етап передбачає фільтрацію отриманих кандидатів контексту за порогом схожості . Встановлюється мінімально допустиме значення ߬ .Усі фрагменти , для яких значення релевантності менше за ߬ , вилучаються зі списку кандидатів як нерелевантні . Така фільтрація запобігає включенню фрагментів , що випадково містять спільні з запитом слова , але не несуть корисної інформації за змістом . Іншими словами , поріг ߬ забезпечує базовий рівень релевантності : до наступного кроку потрапляють тільки фрагменти , схожість яких з запитом достатньо висока . Застосування порогового відсіву особливо важливе , оскільки простий вибір Top-K без порогу може призвести до потрапляння певної кількості нерелевантних даних ; введення ж глобального порогу схожості (після відповідного масштабування оцінок ) дає змогу відфільтрувати малоінформативні результати і підвищити точність вибірки . У формалізованому вигляді множину відібраних релевантних embedding- векторів можна подати як ܴ={݁ௗ ߳ ܧ஽ |ܵ൫݁௤,݁ௗ൯≥τ, де ܴ –множина відібраних елементів ; ݁ௗ – елемент із множини ܧ஽; ܧ஽ – множина всіх доступних елементів ; ܵ(݁௤,݁ௗ) – функція подібності між елементами ݁௤та ݁ௗ; ߬ – порогове значення подібності . Тобто ܴ містить embeddings тих фрагментів , що мають косинусну схожість із запитом не меншу за поріг ߬ .Далі з цієї множини ܴ обираються 59 k найбільш схожих фрагментів (якщо після фільтрації залишилося більше k кандидатів ). Таким чином , отримується підсумковий набір контекстних фрагментів , які будуть використані для побудови запиту до мовної моделі . На п’ятому етапі відбувається безпосереднє конструювання фінального запиту ܳ′ для передавання в LLM. Для цього оригінальний запит користувача ݍ доповнюється відібраними фрагментами контексту з множини ܥ .Об’єднання здійснюється , наприклад , шляхом конкатенації тексту запиту з текстами відповідних фрагментів (з додаванням необхідних розділювачів або підказок щодо формату ). Формально побудований запит можна подати так: ܳᇱ=ݐܽܿ݊݋ܿቀݍ,ܭ݌݋ܶ൫ܧ஽,݁௤,݇൯ቁ, де ܳᇱ – розширений запит ; ݐܽܿ݊݋ܿ – операція конкатенації ; ݍ – початковий запит ; ܭ݌݋ܶ( ܧ஽,݁௤,݇) – вибір ݇елементів із множини ܧ஽, найподібніших до ݁௤; ܧ஽ – множина доступних елементів ; ݁௤ – векторне подання запиту ; ݇ –"}
{"chunk_id": "2025_M_IUS_Suhorukov_DA.pdf:17", "source": "2025_M_IUS_Suhorukov_DA.pdf", "text": "запит користувача ݍ доповнюється відібраними фрагментами контексту з множини ܥ .Об’єднання здійснюється , наприклад , шляхом конкатенації тексту запиту з текстами відповідних фрагментів (з додаванням необхідних розділювачів або підказок щодо формату ). Формально побудований запит можна подати так: ܳᇱ=ݐܽܿ݊݋ܿቀݍ,ܭ݌݋ܶ൫ܧ஽,݁௤,݇൯ቁ, де ܳᇱ – розширений запит ; ݐܽܿ݊݋ܿ – операція конкатенації ; ݍ – початковий запит ; ܭ݌݋ܶ( ܧ஽,݁௤,݇) – вибір ݇елементів із множини ܧ஽, найподібніших до ݁௤; ܧ஽ – множина доступних елементів ; ݁௤ – векторне подання запиту ; ݇ – кількість елементів для включення . Іншими словами , ܳ′ складається з початкового запиту ݍ і контексту , що містить ݇ найбільш релевантних фрагментів інформації з бази . Структура prompt- шаблону при цьому є адаптивною : вона може змінюватися в залежності від кількості та змісту включених фрагментів . Зокрема , шаблон передбачає маркери або інструкції , що відділяють контекст від тексту запиту , а також можуть додаватися спеціальні вказівки моделі . Адаптивність шаблону означає , що якщо фрагментів декілька або вони різного типу , форматування запиту ܳ′ підлаштовується відповідним чином для оптимальної взаємодії з LLM. 60 На завершальному етапі сформований розширений запит ܳ′ передається до LLM. Модель обробляє запит , який вже містить додатковий контекст , і генерує відповідь ܣ .Завдяки вбудованим у ܳ′ зовнішнім відомостям , LLM може надати більш точну та обґрунтовану відповідь , оскільки вона бачить актуальну інформацію , якої могло бракувати у її параметричній пам’яті. Таким чином , вихід моделі є результатом динамічно побудованого запиту , що містить релевантні знання , і цей підхід відповідає парадигмі RAG. 61 3 РОЗРОБКА ІНФОРМАЦІЙНОЇ ТЕХНОЛОГІЇ ФОРМУВАННЯ ЗАПИТІВ ДО ВЕЛИКИХ МОВНИХ МОДЕЛЕЙ 3.1 Опис інформаційної технології Загальна концепція архітектури інформаційної технології полягає в тому , що перед генерацією відповіді мовною моделлю виконується пошук необхідних знань і підживлення моделі знайденими фактами , що підвищує якість та достовірність отриманих результатів . Розглянута архітектура підтримує повний цикл обробки запиту : від формування користувацького запитання до отримання кінцевої відповіді . На рисунку 3.1 схематично зображено ключові компоненти та потоки даних у системі . Процес починається з того, що користувач вводить запит через інтерфейс . Цей запит спочатку обробляється й готується до пошуку . Далі нейронна модель для побудови embedding- представлень перетворює текст запиту на вектор , який компактно кодує його зміст у n-вимірному просторі ознак . Отриманий вектор запиту спрямовується до векторної БД, де виконується пошук найбільш подібних векторів із попередньо проіндексованого корпусу знань . На основі міри близькості знаходяться топ-k релевантних фрагментів інформації . Ці відібрані фрагменти потім додаються до початкового запиту при формуванні підсумкового prompt’у для LLM. Іншими словами , модуль побудови запиту об’єднує текст запитання користувача з знайденими релевантними даними , а також може додати інструкції щодо формату відповіді . Таким чином , мовна модель одержує розширений prompt, що вже містить необхідні факти , і генерує фінальну відповідь , спираючись на наданий контекст . На завершальному етапі модуль аналізу результатів може перевірити відповідь та здійснює логування : зберігає пари запит -відповідь та використаний контекст для збору метрик якості . 62 Рисунок 3.1 – Архітектурна схема системи , що реалізує метод доповнення запитів контекстом 63 3.2 Реалізація інформаційної технології Розроблена система має багаторівневу архітектуру , що включає модулі фронтенду та бекенду , а також інтегровані компоненти : векторну БД знань і локальну LLM. На рисунку 3.2 надана архітектура взаємодії між компонентами системи під час формування відповіді LLM з використанням векторної БД. Фронтенд забезпечує інтерактивний інтерфейс , через який користувач вводить запит та отримує згенеровану відповідь . Бекенд відповідає за обробку запиту та координує взаємодію між усіма компонентами . Зокрема , бекенд -модуль реалізує pipeline обробки : нормалізацію тексту запиту , обчислення векторного подання , семантичний пошук у базі знань та генерування відповіді за допомогою LLM. Векторна БД слугує довготривалою пам’яттю системи – у ній зберігаються текстові фрагменти у вигляді числових векторів ознак . LLM-модель інтегрована для генерації текстових відповідей ; вона отримує розширений запит з доданим контекстом із бази знань і формує змістовну відповідь користувачеві . Для спрощення розгортання прототипу та забезпечення портативності використано контейнеризацію . Застосовано технологію Docker, що дозволяє упакувати кожен компонент системи у відокремлений контейнер із усіма необхідними бібліотеками та налаштуваннями середовища . Таким чином досягається ізоляція процесів та відтворюваність запуску : незалежно від локального оточення користувача всі модулі стартують у стандартизованому вигляді . Наприклад , векторна БД розгорнута як окремий контейнер , що прослуховує локальний порт і обробляє запити семантичного пошуку . Бекенд -сервер розміщено в іншому контейнері на базі образу Python з необхідними залежностями . 64 Рисунок 3.2 – Архітектура взаємодії між компонентами системи під час формування відповіді LLM з використанням векторної БД 65 Можливе також виділення контейнера для LLM-модуля , якщо модель потребує окремого сервісу або апаратного прискорення ; у прототипі натомість використано вбудований запуск моделі в межах бекенду для спрощення . Координація кількох контейнерів здійснюється через Docker Compose: відповідний Yet Another Markup Language- файл описує склад системи і дозволяє запустити усі компоненти однією командою . Окрім"}
{"chunk_id": "2025_M_IUS_Suhorukov_DA.pdf:18", "source": "2025_M_IUS_Suhorukov_DA.pdf", "text": "базі образу Python з необхідними залежностями . 64 Рисунок 3.2 – Архітектура взаємодії між компонентами системи під час формування відповіді LLM з використанням векторної БД 65 Можливе також виділення контейнера для LLM-модуля , якщо модель потребує окремого сервісу або апаратного прискорення ; у прототипі натомість використано вбудований запуск моделі в межах бекенду для спрощення . Координація кількох контейнерів здійснюється через Docker Compose: відповідний Yet Another Markup Language- файл описує склад системи і дозволяє запустити усі компоненти однією командою . Окрім того, додаткові скрипти автоматизують підготовчі кроки : ініціалізацію сховища , запуск завдання зі створення embedding для нових даних , моніторинг стану сервісів . В ході роботи система здійснює логування ключових подій – запити користувачів , результати пошуку , час відгуку моделей – що спрощує налагодження та аналіз ефективності . Логи виводяться в консоль кожного сервісу і при необхідності можуть зберігатися у файл для подальшого аналізу . Користувач вводить текст запиту у інтерфейсі фронтенду . Після натискання кнопки відправлення запит у форматі HyperText Transfer Protocol-запиту передається на бекенд -сервер . Фронтенд не містить бізнес - логіки обробки – його роль полягає у зборі вводу , відправленні його на сервер та відображенні отриманої відповіді . Бекенд отримує запит користувача через Representational State Transfer API і розпочинає обробку . Першим кроком застосовується модуль нормалізації тексту : видаляються зайві пробіли , специфічні символи або розмітка , за потреби приводиться до єдиного регістру або формату . Метою цього етапу є усунути можливі шуми і неоднозначності у вхідному тексті , які могли б негативно вплинути на подальше семантичне порівняння . На рисунку 3.3 надана схема етапів очищення користувацького запиту на етапі нормалізації тексту . Нормалізований та підготовлений до аналізу текст передається на наступний етап. 66 Рисунок 3.3 – Схема етапів очищення користувацького запиту на етапі нормалізації тексту 67 На основі очищеного запиту обчислюється його векторне представлення за допомогою заздалегідь навченого embedding- модуля . Цей модуль побудований на моделі глибокого навчання , що перетворює рядок тексту у точку в багатовимірному векторному просторі . На рисунку 3.4 надана Pipeline векторизації запиту з використанням моделі MiniLM. В прототипі використано готову модель трансформера для embedding, що повертає вектор фіксованої розмірності . Отриманий вектор характеризує семантичний зміст запиту : близькі за змістом запити матимуть розташування векторів , близьке один до одного . Важливо зазначити , що використання однакової моделі та методики нормалізації як для запитів , так і для текстів у базі знань забезпечує коректність порівняння – всі вектори знаходяться в одному просторі ознак . Розрахований вектор запиту надходить до векторної БД, де виконується семантичний пошук – визначення найбільш схожих векторів серед тих, що збережені у сховищі знань . Бекенд надсилає запит до API БД, вказуючи вектор запиту та бажану кількість результатів ݇ .Векторна СУБД здійснює високопродуктивний пошук найближчих сусідів : обчислює міру схожості між вектором запиту та кожним вектором у колекції . У результаті база повертає топ-k найбільш релевантних фрагментів – тобто тих, чиї вектори максимально близькі до вектора запиту . Разом з кожним знайденим вектором зберігається пов’язаний з ним контент : текст фрагмента знань та додаткова мета -інформація . Бекенд отримує список знайдених фрагментів упорядкований за спаданням схожості . Таким чином відбирається невелика кількість контекстних даних , найбільш релевантних до запиту ; нерелевантні дані відсіюються і не потрапляють до контексту , що підвищує точність подальшої відповіді . На основі отриманих з БД фрагментів знань бекенд будує розширений контекст для мовної моделі . До оригінального запиту користувача автоматично додаються знайдені релевантні тексти – цей об’єднаний текст і становить prompt для LLM. 68 Рисунок 3.4 – Pipeline векторизації запиту з використанням моделі MiniLM 69 На рисунку 3.5 надана Pipeline збагачення запиту контекстом попередніх діалогів . Формування prompt’у може включати шаблонізацію : наприклад , спершу вставляються фрагменти контексту зі сховища , а далі – сам запит користувача . Такий підхід дозволяє інтегрувати зовнішні знання у процес генерації : мовна модель отримує не лише питання , а й додаткові підказки або факти , на які вона повинна спиратися . У результаті зменшується ризик так званих «галюцинацій » моделі , коли вона вигадує відповіді без опори на дані: LLM має актуальний контекст і менше схильна генерувати недоречну інформацію . LLM отримує на вхід злитий prompt і генерує продовження у вигляді текстової відповіді . Генерація здійснюється авторегресивно токен за токеном до досягнення умов завершення . Відповідь моделі містить сформульовану природною мовою реакцію на запит користувача з урахуванням наданого контексту . Наприклад , якщо запит стосується раніше обговорюваної теми , LLM збереже наступність , використовуючи факти з витягнутих фрагментів діалогу . Згенерована моделью відповідь надсилається бекендом назад на фронтенд у форматі HyperText Transfer Protocol- відповіді . Фронтенд отримує ці дані через веб-запит і відображає текст відповіді у інтерфейсі для користувача . На цьому цикл обробки завершується , і користувач бачить результат . Якщо діалог продовжується , система може додатково зберегти нову інформацію у базі знань : наприклад , отримане запитання -відповідь або згенерований моделью підсумок поточної"}
{"chunk_id": "2025_M_IUS_Suhorukov_DA.pdf:19", "source": "2025_M_IUS_Suhorukov_DA.pdf", "text": "раніше обговорюваної теми , LLM збереже наступність , використовуючи факти з витягнутих фрагментів діалогу . Згенерована моделью відповідь надсилається бекендом назад на фронтенд у форматі HyperText Transfer Protocol- відповіді . Фронтенд отримує ці дані через веб-запит і відображає текст відповіді у інтерфейсі для користувача . На цьому цикл обробки завершується , і користувач бачить результат . Якщо діалог продовжується , система може додатково зберегти нову інформацію у базі знань : наприклад , отримане запитання -відповідь або згенерований моделью підсумок поточної розмови . Такий механізм дозволяє поповнювати векторну базу актуальними даними по мірі взаємодії . Кожен новий запит повторює описаний pipeline, забезпечуючи динамічне врахування як поточної , так і довготривалої інформації . 70 Рисунок 3.5 – Pipeline збагачення запиту контекстом попередніх діалогів 71 Таким чином , реалізована інформаційна технологія являє собою демонстраційний прототип , який поєднує векторне семантичне сховище знань та генеративну мовну модель . Архітектура модульна і масштабована : за необхідності фронтенд можна замінити або доповнити , модель – оновити на потужнішу , а базу знань – розгорнути на віддаленому сервері для роботи з більшими даними . Використання контейнерів Docker забезпечує легкий запуск на будь -якій машині та ізоляцію компонентів один від одного . Потокова обробка запитів гарантує , що кожна відповідь моделі максимально враховує релевантний контекст із наявних даних , підтримуючи цілісність діалогу і точність інформації . У цілому , обрані технічні рішення і структура системи підтверджують практичну здійсненність запропонованого методу формування запитів та створюють основу для його подальшого вдосконалення і масштабування . 72 4 ПРОГРАМНА РЕАЛІЗАЦІЯ ТА ЕКСПЕРИМЕНТАЛЬНА ПЕРЕВІРКА 4.1 Опис програмного модуля Програмна реалізація запропонованої інформаційної технології формування запитів до великих мовних моделей організована у вигляді низки взаємопов ’язаних модулів на мові Python продемонстровані у додатку А. У складі програмного модуля , що реалізує запропоновану інформаційну технологію , ключову роль відіграє послідовний pipeline перетворення тексту запиту , взаємодії з векторною базою пам’яті та формування розширеного промпту для великої мовної моделі . Логіка цього pipeline матеріалізується у вигляді низки функцій в окремих підмодулях , які узгоджуються з формалізацією методу , поданою в теоретичних розділах роботи . Функція preprocess_text реалізує описаний у роботі модуль нормалізації вхідного запиту й виконує попередній етап очищення тексту перед векторизацією . На вхід надходить довільний рядок , що може містити HTML-розмітку , спеціальні символи та артефакти форматування , характерні для веб-інтерфейсів . Усередині функції послідовно застосовуються регулярні вирази для видалення HTML- тегів , далі здійснюється фільтрація символів за узгодженим алфавітом : зберігаються літери , цифри , базова пунктуація та пробіли , а також символи , що належать до літерних категорій Unicode. Такий підхід дозволяє зберегти корисну текстову інформацію , одночасно усуваючи шумові елементи , які не несуть змістовного навантаження , але здатні впливати на embedding- представлення . Наступний крок полягає у нормалізації пропусків : множинні пробіли та переведення рядків замінюються одним пробілом , крайні пробіли видаляються . Завершальним етапом є приведення тексту до нижнього 73 регістру , що робить подальший семантичний пошук інваріантним до регістру та узгоджує поведінку із підходами , описаними в теоретичній частині під час аналізу нормалізації запиту . У підсумку функція повертає стандартизований рядок , який виступає стабільною основою для подальшої векторизації та порівняння у векторному просторі . Наступна група функцій пов’язана з формуванням векторних представлень сумаризованих спогадів і їхнім розміщенням у векторному сховищі . Функція create_embedding_from_summary приймає на вхід текст резюме діалогу й передає його до embedding- моделі типу sentence- transformers, яка в прототипі відповідає попередньо навченій моделі all- MiniLM-L6-v2. Модель перетворює текст у вектор фіксованої розмірності 384, що узгоджується з описом структури експериментальних даних : підсумки діалогів у роботі також зберігаються разом з embedding- координатами цієї розмірності . Вектор повертається у вигляді масиву типу NumPy, що використовується як вхід для індексації у векторній базі. Функція add_memory_to_vector_store реалізує інтеграцію об’єкта довготривалої пам’яті до Facebook Artificial Intelligence Similarity Search (FAISS)-індексу , який у роботі розглядається як основний інструмент семантичного пошуку найближчих сусідів . На першому етапі одновимірний embedding перетворюється у двовимірне представлення розміру 1×d та приводиться до типу float32 відповідно до вимог FAISS. Далі застосовується L2-нормалізація , що забезпечує одиничну довжину вектора . Така нормалізація узгоджується з теоретичним положенням про те, що для нормалізованих векторів скалярний добуток та евклідова відстань дають еквівалентні ранжування відносно косинусної подібності , яка використовується як базова міра семантичної близькості у роботі . Після нормалізації вектор додається до FAISS-індексу плоского типу , а поточна позиція в індексі зберігається у двосторонніх асоціативних структурах : від ідентифікатора MemoryUnit до індексу й навпаки . Така мапа забезпечує можливість швидко відновити текстовий спогад за індексом , 74 отриманим під час пошуку , та підтримує узгодженість між текстовою та векторною репрезентаціями пам’яті. Функція search_relevant_memories реалізує семантичний пошук релевантних одиниць пам’яті для поточного запиту користувача . На вхід надходить текст запиту , який нормалізується функцією preprocess_text, що забезпечує одноманітність обробки запитів і сумаризованих спогадів . Нормалізований текст кодується embedding- моделлю у вектор запиту , який , приводиться до типу float32 та L2- нормалізується"}
{"chunk_id": "2025_M_IUS_Suhorukov_DA.pdf:20", "source": "2025_M_IUS_Suhorukov_DA.pdf", "text": "індексу й навпаки . Така мапа забезпечує можливість швидко відновити текстовий спогад за індексом , 74 отриманим під час пошуку , та підтримує узгодженість між текстовою та векторною репрезентаціями пам’яті. Функція search_relevant_memories реалізує семантичний пошук релевантних одиниць пам’яті для поточного запиту користувача . На вхід надходить текст запиту , який нормалізується функцією preprocess_text, що забезпечує одноманітність обробки запитів і сумаризованих спогадів . Нормалізований текст кодується embedding- моделлю у вектор запиту , який , приводиться до типу float32 та L2- нормалізується . Отриманий вектор подається до FAISS- індексу для виконання операції пошуку топ-k найближчих сусідів . У відповідь індекс повертає матриці подібностей і індексів елементів , де кожне значення подібності вже відповідає косинусній мірі схожості в діапазоні від −1 до 1. Індекси переводяться у ідентифікатори MemoryUnit через попередньо сформовану мапу , після чого формується список пар об’єкт пам’яті та числове значення подібності . Така реалізація відтворює формальний механізм вибору множини фрагментів ܥ(ݍ ,)описаний у теоретичній частині , і виступає основою для побудови розширеного запиту . Подальшим кроком у pipeline є формування промптів для великої мовної моделі з урахуванням знайденого контексту . Функція generate_query_with_memory приймає нормалізований текст запиту та список релевантних спогадів із векторної бази . Всередині функції формується текстовий блок контексту : кожний спогад супроводжується його порядковим номером , значенням подібності , заокругленим до двох десяткових знаків , та текстом підсумку , що відповідає властивості summary об’єкта MemoryUnit. Така форма представлення контексту робить pipeline прозорим з погляду відлагодження та аналізу , оскільки дозволяє явно бачити , які саме спогади було активовано для даного запиту . Далі контекстні фрагменти конкатенуються у єдиний текстовий блок , який разом із нормалізованим запитом включається до повідомлення користувача у форматі , сумісному з API LLM. Паралельно формується системне 75 повідомлення з інструкцією для моделі : явно зазначається , що подається додатковий контекст з попередніх діалогів , який слід використовувати за наявності релевантності . Така двокомпонентна структура промпту фактично реалізує функцію ܲ(ܪₙ₋₁, ܥ(ݍ ,)ݍ ,)описану у формальній постановці методу , де історія діалогу відображена через узагальнені спогади з векторної бази . Функція generate_query_without_memory реалізує базовий сценарій без залучення довготривалої пам’яті. На відміну від попереднього варіанту , у системному промпті LLM задається інструкція ігнорувати будь -який зовнішній контекст , а в повідомленні користувача передається лише текст вихідного запиту . Порівняння результатів роботи двох функцій дозволяє кількісно оцінити внесок векторної пам’яті в релевантність відповідей . Окремий функціональний блок присвячений підсумовуванню завершених діалогів і створенню одиниць пам’яті. Функція summarize_dialog отримує екземпляр Dialog, у якому збережено повну історію взаємодії користувача й асистента . Через метод get_text_content діалог перетворюється на лінійний текстовий опис , що узгоджується з форматом , використаним при побудові експериментального корпусу . На основі цього тексту формується спеціалізований промпт до LLM, у якому задається інструкція створити компактний резюме -фрагмент знань обсягом 5–10 речень із фокусом на ключових питаннях , відповідях та деталях , потенційно корисних у майбутніх діалогах . Виклик до llm_client.chat виконується з низьким значенням параметра температури , що мінімізує стохастичність та підвищує повторюваність підсумків для тієї самої розмови . Отримане резюме передається у фабричний метод MemoryUnit.create_from_dialog, який створює об’єкт довготривалої пам’яті з посиланням на вихідний діалог , часовими мітками та текстовим підсумком . Далі над підсумком обчислюється embedding за допомогою тієї ж моделі all-MiniLM-L6-v2, що використовується і для інших частин системи , після чого створений вектор разом з об’єктом пам’яті додається у 76 VectorStore. Такий механізм забезпечує узгодженість між описаною в роботі концепцією людоподібної пам’яті й конкретною програмною реалізацією , де зберігаються не повні транскрипти , а структуровані узагальнення . Функція handle_user_query виступає центральним оркестратором pipeline обробки запиту користувача , логіка якого у розділі програмної реалізації пов’язується з файлом pipeline.py. На вхід подається сирий текст запиту , екземпляр менеджера пам’яті та клієнт LLM. Спочатку викликається preprocess_text, що приводить запит до нормалізованої форми . Потім через метод retrieve_relevant_memories виконується семантичний пошук у векторному сховищі з відбором до трьох найближчих спогадів з урахуванням порогу подібності , як це описано при характеристиці роботи MemoryManager та VectorStore. Після отримання контексту формуються два промпти : базовий – за допомогою generate_query_without_memory, та розширений – через generate_query_with_memory. Обидва варіанти подаються до LLM через виклики llm.chat, унаслідок чого отримуються дві відповіді : без пам’яті та з пам’яттю . Завершальним етапом функції є оцінювання релевантності обох відповідей за допомогою функції evaluate_relevance, однак із різними умовами : для відповіді без пам’яті оцінка проводиться лише відносно вихідного запиту , тоді як для відповіді з пам’яттю до промпту арбітра додається текстовий контекст активованих спогадів . Результатом роботи функції є структурований словник , що містить обидві відповіді , їхні числові оцінки та перелік використаних спогадів , що повністю узгоджується з форматом експериментального набору даних , представленим у роботі . Функція evaluate_relevance реалізує підхід автоматизованого оцінювання , у якому роль експерта -оцінювача виконує окрема велика мовна модель , налаштована на виконання функції арбітра релевантності . Усередині функції формується системний промпт , де задається роль моделі як експерта з оцінки відповідей ,"}
{"chunk_id": "2025_M_IUS_Suhorukov_DA.pdf:21", "source": "2025_M_IUS_Suhorukov_DA.pdf", "text": "додається текстовий контекст активованих спогадів . Результатом роботи функції є структурований словник , що містить обидві відповіді , їхні числові оцінки та перелік використаних спогадів , що повністю узгоджується з форматом експериментального набору даних , представленим у роботі . Функція evaluate_relevance реалізує підхід автоматизованого оцінювання , у якому роль експерта -оцінювача виконує окрема велика мовна модель , налаштована на виконання функції арбітра релевантності . Усередині функції формується системний промпт , де задається роль моделі як експерта з оцінки відповідей , а також описується шкала від 0 до 3 балів , що використовується в експериментальній частині роботи . Далі 77 створюється користувацький промпт , який містить текст запиту , відповідь , що підлягає оцінці , та, за наявності , додатковий контекст . Застосовується низька температура генерації , що зменшує варіативність рішень арбітра і забезпечує більш стабільні оцінки . Після виклику arbitrator_llm.chat функція отримує вербальне пояснення та числову оцінку , інкапсульовану в тексті відповіді . Допоміжна процедура parse_score_from_evaluation виділяє числове значення , яке інтерпретується як оцінка релевантності , тоді як текстова частина використовується як обґрунтування . У результаті повертається структура з полями score і justification, що надалі використовується у агрегуванні статистичних показників , наведених у розділі експериментальної оцінки . Сукупність описаного функціоналу підтверджує практичну здійсненність запропонованого підходу та створює основу для його подальшого розширення в напрямку складніших стратегій векторного пошуку й оптимізованих схем формування контексту до LLM. 4.2 Опис експериментальних даних Експериментальний набір даних сформовано таким чином , щоб відобразити повний цикл роботи системи : від накопичення історичних діалогів та їхньої сумаризації до отримання відповідей LLM з підключенням або без підключення довготривалої пам’яті та подальшої оцінки релевантності цих відповідей . Фокус робиться саме на структурі даних , а не на алгоритмічних деталях , що дає змогу розглядати набір як самодостатній об’єкт для відтворення та аналізу експериментів . Початкову основу становить корпус історичних діалогів , які моделюють різні сценарії взаємодії користувача з асистентом у технічних , навчальних та прикладних запитах . Корпус включає діалогів українською та 78 частково англійською мовами . Середня довжина одного діалогу – приблизно 13 реплік . Для кожного діалогу фіксується текст усіх повідомлень у хронологічному порядку , часові мітки , ролі, а також службові поля , зокрема ідентифікатор діалогу та ознака активності чи завершеності . Такий формат дає можливість як відтворювати повну історію спілкування , так і використовувати її для побудови стиснутих спогадів . На основі завершених діалогів сформовано набір сумаризованих спогадів , що зберігаються у векторній БД. Середня довжина одного підсумку знаходиться в діапазоні 70–90 слів, що забезпечує баланс між інформативністю та компактністю ; максимальна довжина не перевищує приблизно 150 слів. Для кожного резюме у наборі даних збережено текст підсумку , посилання на вихідний діалог , час створення та векторні координати embedding- представлення розмірності 384, сформовані вбудованою моделлю embedding. Окрему частину експериментальних даних становить тестова множина запитів . Вона містить запити , які сконструйовано таким чином , щоб частина з них критично залежала від інформації , що міститься в попередніх діалогах , тоді як інша частина могла бути коректно оброблена без звернення до історії . Наприклад , значна частина «контекст -залежних » запитів має форму уточнень щодо раніше узгоджених планів , параметрів задачі або раніше введених позначень , тоді як контекст -незалежні запити , як правило , належать до загальних інформаційних питань . Для кожного тестового запиту зберігаються його текстова форма , ідентифікатор , зв’язок із конкретними діалогами та службові поля , які використовуються на етапі побудови вибірок . Зведена структура основних компонентів експериментальних даних представлена на лістингу 4.1. 79 Лістинг 4.1 – Зведена структура основних компонентів експериментальних даних { \"id\": \"seed-10\", \"messages\": [ { \"role\": \"user\", \"content\": \" Перед релізом потрібно узгодити документацію .\", \"timestamp\": \"2025-10-17T13:30:00Z\" }, { \"role\": \"assistant\", \"content\": \" Документація має містити опис API, схеми інтеграції та приклади тестових сценаріїв .\", \"timestamp\": \"2025-10-17T13:30:04Z\" } ], \"created_at\": \"2025-10-17T13:30:00Z\", \"last_activity_at\": \"2025-10-17T13:30:04Z\", \"is_active\": false } Для кожного запиту проводяться два окремі прогони мовної моделі . Перший прогін використовує лише текст запиту , без додавання будь -яких резюме з векторної БД, унаслідок чого формується відповідь , що відображає «чисту » поведінку моделі . Другий прогін здійснюється після виконання пошуку у векторному сховищі : на основі embedding- вектора запиту відбираються до трьох найближчих спогадів , які перевищили заданий поріг подібності , після чого їхні тексти додаються як контекст перед основним запитанням . Таким чином для кожного експериментального запиту у наборі даних наявна пара вихідних текстів : відповідь без пам’яті та відповідь з пам’яттю . На рисунку 4.2 зображено приклад роботи програми на тестовому запиті . 80 Рисунок 4.1 – Приклад роботи програми на тестовому запиті Формат таких записів ілюструє таблиця 4.1, у якій наведено три типові приклади для різних сценаріїв залежності від контексту . Ключова відмінність полягає у способі оцінювання релевантності відповідей . Роль експерта -оцінювача виконує окрема LLM від OpenAI, налаштована через спеціальний системний промпт на виконання функції «арбітра релевантності ». Для кожної пари «запит – відповідь »"}
{"chunk_id": "2025_M_IUS_Suhorukov_DA.pdf:22", "source": "2025_M_IUS_Suhorukov_DA.pdf", "text": "та відповідь з пам’яттю . На рисунку 4.2 зображено приклад роботи програми на тестовому запиті . 80 Рисунок 4.1 – Приклад роботи програми на тестовому запиті Формат таких записів ілюструє таблиця 4.1, у якій наведено три типові приклади для різних сценаріїв залежності від контексту . Ключова відмінність полягає у способі оцінювання релевантності відповідей . Роль експерта -оцінювача виконує окрема LLM від OpenAI, налаштована через спеціальний системний промпт на виконання функції «арбітра релевантності ». Для кожної пари «запит – відповідь » формується службовий промпт , у якому описано шкалу оцінювання та наведено інструкцію : визначити , наскільки відповідь відповідає інформаційному запиту , уникнути упередженості та надати результат у вигляді цілого числа від 0 до 3. Модель -експерт обробляє окремо відповідь без контексту та відповідь з контекстом , повертаючи числову оцінку й коротке текстове обґрунтування . 81 Таблиця 4.1 – Приклади записів у експериментальному наборі Короткий опис запиту Кількість релевантних спогадів Сумарна довжина контексту , токенів Оцінка без контексту (0–3) Оцінка з контекстом (0–3) Повернення до раніше узгодженого плану впровадження 2 310 1 3 Уточнення раніше введеного позначення у формальному описі 1 120 2 3 Загальне питання , що не спирається на історію 0 0 3 3 82 У наборі експериментальних даних зберігаються щонайменше числові бали ; текстові пояснення можуть додатково використовуватися для аналізу типових помилок . На рисунку 4.3 відображен результат оцінки відповідей . Рисунок 4.2 – Результат оцінки відповідей Шкала інтерпретується наступним чином : значення 0 відповідає повній нерелевантності або серйозним логічним помилкам ; одиниця означає часткову релевантність , коли відповідь торкається теми , але пропускає важливі аспекти або містить істотні неточності ; значення 2 характеризує загалом коректну , але неповну або з незначними похибками відповідь ; значення 3 відображає повну , логічно послідовну й релевантну відповідь , що повністю задовольняє запит . Завдяки використанню однієї й тієї ж моделі OpenAI як автоматизованого експерта для всіх прикладів досягається консистентність критеріїв оцінювання , а також знімається необхідність у залученні декількох людських анотараторів для кожного прикладу . Сформований набір експериментальних даних , таким чином , фіксує усі необхідні вхідні й вихідні величини для об’єктивного порівняння поведінки LLM до та після додавання сумаризованого контексту з векторної 83 БД. Нижче подано узагальнювальну таблицю 4.2, яка демонструє вплив вставки сумаризованого історичного контексту з векторної БД на якість відповіді LLM. Дані відповідають логіці експериментального набору , сформованого у роботі , та ґрунтуються на оцінюванні відповідей моделлю - експертом OpenAI, що виконує функцію незалежного арбітра релевантності . Таблиця 4.2 – Узагальнений вплив технології на якість відповідей LLM Показник Режим без пам’яті Режим з пам’яттю Абсолютна зміна Середня оцінка релевантності (0–3) 1,82 2,64 +0,82 Частка відповідей , що отримали ≥2 58% 86% +28% Частка відповідей з максимальною оцінкою 3 22% 49% +27% Узагальнені результати демонструють стійку позитивну тенденцію : підключення сумаризованої пам’яті з векторної БД суттєво підвищує релевантність відповідей моделі . Найбільш помітним є збільшення частки відповідей з максимальною оцінкою – майже удвічі , що вказує на покращення якості саме там, де потрібні точні та контекстуально складні відповіді . Приріст середньої оцінки у +0.82 бала на чотирибальній шкалі розглядається як значний ефект для задач , залежних від довготривалої історії . 84 ВИСНОВКИ У результаті виконання кваліфікаційної роботи було досліджено методи вирішення функціональної задачі підвищення релевантності відповідей пошукових систем на основі інтеграції семантичного пошуку та великих мовних моделей . Проведено аналіз сучасних архітектур пошукових систем , підходів до формування запитів і побудови контексту , а також огляд наявних рішень на базі RAG. Сформульовано та обґрунтовано вимоги до інформаційної технології , яка забезпечує використання векторних представлень документів і історії попередніх звернень користувача для уточнення запитів і відбору релевантних фрагментів . Розроблено математичні моделі та алгоритмічні процедури семантичного пошуку , повторного ранжування та інтеграції результатів у запит до великої мовної моделі , що дозволяє підвищити точність відповідей та зменшити кількість нерелевантних результатів . Реалізовано програмний прототип функціонального модуля , який взаємодіє з векторною базою даних і LLM, забезпечуючи експериментальне порівняння з базовими підходами формування відповідей . Експериментальні дослідження показали покращення показників релевантності відповідей пошукової системи та стабільності результатів при роботі з великими корпусами текстів . Основні результати роботи , пов’язані з дослідженням моделей та методів підвищення релевантності відповідей пошукових систем , були представлені та обговорені в доповіді «Дослідження моделей та методів вирішення функціональної задачі підвищення релевантності відповідей пошукових систем » на II Міжнародній науково -теоретичній конференції «Modern science and innovation: tren ds, challenges, and breakthroughs». 85 ПЕРЕЛІК ДЖЕРЕЛ ПОСИЛАННЯ 1. Bridgelall R. Unraveling the myster ies of AI chatbots. Artificial Intelligence Review. 2024. Т.57. №89. DOI: 10.1007/s10462-024-10720-7. 2. Unleashing the potential of prompt engineering for large language models / Chen B., Zhang Z., Langrené N., Zhu S. // Patterns. 2025. Т. 6, № 6. DOI: 10.1016/j.patter.2025.101260. 3. Nechita M., Raschip M. A Comparative Study of ML Approaches for Detecting AI-Generated Essays // Proceedings of the 14th International Conference on Data Science, Technology and Applications (DATA 2025) – Volume 1: DATA. Setúbal:"}
{"chunk_id": "2025_M_IUS_Suhorukov_DA.pdf:23", "source": "2025_M_IUS_Suhorukov_DA.pdf", "text": "R. Unraveling the myster ies of AI chatbots. Artificial Intelligence Review. 2024. Т.57. №89. DOI: 10.1007/s10462-024-10720-7. 2. Unleashing the potential of prompt engineering for large language models / Chen B., Zhang Z., Langrené N., Zhu S. // Patterns. 2025. Т. 6, № 6. DOI: 10.1016/j.patter.2025.101260. 3. Nechita M., Raschip M. A Comparative Study of ML Approaches for Detecting AI-Generated Essays // Proceedings of the 14th International Conference on Data Science, Technology and Applications (DATA 2025) – Volume 1: DATA. Setúbal: SciTeP ress, 2025. P. 144–155. DOI: 10.5220/0013570200003967. 4. Levy M., Jacoby A., Goldberg Y. Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models // Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Bangkok, Thailand, August 2024. Association for Computational Linguistics, 2024. P. 15339–15353. DOI: 10.18653/v1/2024.acl-long.818. 5. Chatterjee S. The Impact of Prompt Bloat on LLM Output Quality URL: https://mlops.community/the-impact-of-prompt-bloat-on-llm-output-quality/ (дата звернення : 24.11.2025). 6. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models / Wei J., Wang X., Schuurmans D., та ін. arXiv. 2022. DOI: 10.48550/arXiv.2201.11903. 7. A practical guide to Retrieval-Augmented Generation (RAG) URL: https://www.k2view.com/what-is-retrieval-augmented-generation ( дата звернення : 25.11.2025). 8. A Comprehensive Survey on Vector Database: Storage and Retrieval Technique, Challenge / Ma L., Zhang R., Han Y., та ін. arXiv. 2023. 86 DOI: 10.48550/arXiv.2310.11703. 9. Roller J. Vector Databases: The Foundation of Modern AI Applications URL: https://www.computer.org/publications/tech-news/community-voices/ vector-databases-and-ai-applications ( дата звернення : 26.11.2025). 10. Ricadela A. What Is Weaviate? A Semantic Search Database URL: https://www.oracle.com/database /vector-database/weaviate/ ( дата звернення : 07.12.2025). 11. Reimers N., Gurevych I. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks / arXiv. 2019. arXiv:1908.10084. DOI: 10.48550/arXiv.1908.10084. 12. Vector Similarity Explained URL: https://www.pinecone.io/ learn/vector-similarity/ ( дата звернення : 07.12.2025). 13. Salton G., Buckley C. Term-weighting approaches in automatic text retrieval // Information Processing & Ma nagement. 1988. Vol. 24, No. 5. P. 513– 523. DOI: 10.1016/0306-4573(88)90021-0. 14. Robertson S. E., Zaragoza H. The Probabilistic Relevance Framework: BM25 and Beyond // Foundations and Trends in Information Retrieval. 2009. Vol. 3, Nos. 4–5. P. 333–389. DOI: 10.1561/1500000019. 15. Simple Yet Effective Neural Ra nking and Reranking Baselines for Cross-Lingual Information Retrieval / Lin J., Alfonso-Hermelo D., Jeronymo V. та ін. arXiv. 2023. DOI: 10.48550/arXiv.2304.01019. 16. Lin S.-C., Lin J. A Dense Repres entation Framework for Lexical and Semantic Matching // ACM Transactions on Information Systems. 2023. Vol. 41, No. 4. P. 1–110. DOI: 10.1145/3582426. 17. Karpukhin V., Oguz B., Min S. та ін. Dense Passage Retrieval for Open-Domain Question Answering // arXiv. 2020. arXiv:2010.11386. 18. Hybrid Retrieval Approach for Advancing Retrieval-Augmented Generation Systems / Doan N. N., Härmä A., Celebi R., та ін. Proceedings of the 7th International Conference on Natural Language and Speech Processing. Trento: Association for Computationa l Linguistics, 2 024. P. 397–409. 87 19. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks / Lewis P., Perez E., Piktus A. та ін. Advances in Neural Information Processing Systems 33 (NeurIPS 2020). 2020. P. 9459–9474. 20. Чалий С.Ф, Лещинська І.О Доповнення вхідних даних у ментальній моделі користувача інтелектуальної системи . Проблеми iнформатизац iї . Т. 2. 2024. С. 46. 21. ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory // arXiv. 2023. DOI:10.48550/arXiv.2306.03901 22. Qu X. та ін. Unified Debates: Efficient Performance Evaluation with Behavior-based Metrics in Preference Alignment // arXiv. 2024. arXiv:2412.12559. 23. Long Context vs. RAG for LLMs: An Evaluation and Revisits // arXiv. 2025. DOI: 10.48550/arXiv.2501.01880 24. Xu Z., Lin J., Srikumar V. A Survey of Model Architectures in Information Retrieval // arXiv. 2025. DOI: 10.4855 0/arXiv.2502.14822 25. Dense Hierarchical Retrieval for Open-domain Question Answering / Liu Y. та ін. Findings of the Association for Computational Linguistics: EMNLP 2021. 2021. DOI: 10.48550/arXiv.2110.15439. 26. Zhang L., Wu Y., Yang Q. Exploring the Best Practices of Query Expansion with Large Language Models // Findings of the Association for Computational Linguistics: EMNLP 2024. 2024. P. 1872–1883. DOI: 10.48550/arXiv.2401.06311 27. Pan M., Xiong W., Zhou S., Ga o M., Chen J. LLM-Based Query Expansion with Gaussian Kernel Enhanced Semantic Space for Dense Retrieval // Electronics. 2025. Vol. 14, No. 9. Article 1744. DOI: 10.3390/electronics14091744. 28. Document Segmentation Matters for Retrieval-Augmented Generation / Wang Z., Gao C., Xiao C., та ін. Findings of the Association for Computational Linguistics: ACL 2025. Vienna, 2025. P. 8063–8075. DOI: 10.18653/v1/2025.findings-acl.422. 88 29. Nguyen H. T., Nguyen T. D., Nguyen V. H. Evaluating Chunking Strategies to Improve Retrieval-Augmented Generation with Long Clinical Documents // arXiv. 2025. arXiv:2507.09935. 30. Comparative Evaluation of Advanced Chunking for Retrieval- Augmented Generation in Large Language Models for Clinical Decision Support / Gómez-Cabello C. A., Santamaría A., Arizti-Sanz J., та ін. Bioengineering. 2025. Vol. 12, No. 11. Article 1194. 31. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks / Lewis P., Perez E., Piktus A., та ін. arXiv. 2020. DOI: 10.48550/arXiv.2005.11401. 32. Multiple Memory Systems for En hancing the H. Evaluating Chunking Strategies to Improve Retrieval-Augmented Generation with Long Clinical Documents // arXiv. 2025. arXiv:2507.09935. 30. Comparative Evaluation of Advanced Chunking for Retrieval- Augmented Generation in Large Language Models for Clinical Decision Support / Gómez-Cabello C. A., Santamaría A., Arizti-Sanz J., та ін. Bioengineering. 2025. Vol. 12, No. 11. Article 1194. 31. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks / Lewis P., Perez E., Piktus A., та ін. arXiv. 2020. DOI: 10.48550/arXiv.2005.11401. 32. Multiple Memory Systems for En hancing the Long-term Memory of Agent / Zhang G., Wang B., Ma Y., та ін. arXiv. 2025. DOI: 10.48550/arXiv.2508.15294 33. Li K., Jing X., Jing C. Vector Storage Based Long-term Memory Research on LLM // International Journal of Advanced Network Monitoring and Controls. 2024. Vol. 9, No. 3. P. 69–79. DOI: 10.2478/ijanmc-2024-0029. 34. Huang J., Chang K. C.-C. Towards Reasoning in Large Language Models: A Survey // arXiv. 2023 . DOI: 10.48550/arXiv.2212.10403. 35. MemoryBank: Enhancing Large Language Models with Long-Term Memory / Zhong W., Guo L., Gao Q., та ін. Proceedings of the AAAI Conference on Artificial Intelligence. 2024. Vol. 38, No. 17. P. 19724–19731. DOI: 10.1609/aaai.v38i17.29946. 36. Amugongo L. M. та ін. Retrieval augmented generation for large language models in healthcare: A systematic review // PLOS Digital Health. 2025. DOI: 10.1371/journal.pdig.0000877. 37. Lewis P., Perez E., Piktus A. et al. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks // arXiv. 2020. DOI: 10.48550/arXiv.2005.11401. 38. Ford D. Introducing Contextual Retrieval URL: https://www.anthropic. com/engineering/cont extual-retrieval ( дата звернення : 03.12.2025)."}
{"chunk_id": "2025_M_IUS_Trebunskyh_OV.pdf:0", "source": "2025_M_IUS_Trebunskyh_OV.pdf", "text": "Міністерство освіти і науки України Харківський національний університет радіоелектроніки Факультет Комп’ютерних наук (повна назва) Кафедра Інформаційних управляючих систем (повна назва) КВАЛІФІК АЦІЙНА РОБОТА Пояснювальна записка рівень вищої освіти другий (магістерський) Дослідження методів і моделей побудови рекомендацій клієнтам в інформаційній системі агентства нерухомості (тема) Виконав: здобувач 2 року навчання, групи ІУСТм -24-1 Олександр ТРЕБУНСЬКИХ (власне ім’я, прізвище ) Спеціальн ість 122 К омп’ютерні науки (код і повна назва спеціальності) Тип програми освітньо -професійна (освітньо -професійна або освітньо -наукова) Освітня програма Інформаційні управляючі системи та технології (повна назва освітньої програми) Керівни к: доц. каф. ІУС Тетяна БОРИСЕНКО (посада, власне ім’я, прізвище ) Допускається до захисту Зав. кафедри ІУС Костянтин ПЕТРОВ (підпис) (власне ім’я, прізвище ) 2025 р. 2 Харківський національний університет радіоелектроніки ЗАТВЕРДЖУЮ: Зав. кафедри (підпис) “ 24 ” листопада 20 25 р. ЗАВДАННЯ НА КВАЛІФІК АЦІЙНУ РОБОТУ здобуваче ві Требунських Олександру В’ячеславович у (прізвище, ім’я, по батькові) 1. Тема роботи Дослідження методів і моделей побудови рекомендацій клієнтам в інформаційній системі агентства нерухомості затверджена наказом по університету від “ 24 ” листопада 2025 р. № 1055 Ст 2. Термін подання здобувачем роботи до екзаменаційної комісії “ сі ч н я 2 0 2 5 р. 17 ” грудня 20 25 р. 3. Вихідні дані до роботи матеріали передатестаційної практики, науково -технічні публікації та інтернет джерела з тематики кваліфікаційної роботи 4. Перелік питань, що потрібно опрацювати в роботі опрацювати у роботі аналіз методів та підходів для побудови рекомендацій житла в інформаційній системі агентства нерухомості; дослідження та вибір методів для побудови рекомендацій житла в інформаційн ій системі агентства нерухомості; розробка інформаційної технології побудови рекомендацій клієнтам в інформаційній системі агентства нерухомості; реалізація модуля побудови рекомендацій ; експериментальна перевірка наукових результатів . Факультет Комп’ютерних наук Кафедра Інформаційних управляючих систем Рівень вищої освіти другий (магістерський) Спеціальність 122 К омп’ютерні науки (код і повна назва) Тип програми освітньо -професійна (освітньо -професійна або осві тньо-наукова) Освітня програма Інформаційні управляючі системи та технології (повна назва) 3 КАЛЕНДАРНИЙ ПЛАН № Назва етапів роботи Термін виконання етапів роботи Примітка 1 Отримання завдання 24.11.2025 Виконано 2 Аналіз методів формування рекомендацій щодо вибору нерухомості 25.11.2025 – 27.11.2025 Виконано 3 Постановка задач дослідження 28.11.2025 Виконано 4 Дослідження та експериментальний вибір методу розрахунку ступеня подібності об'єктів 29.12.2025 – 01.12.2025 Виконано 5 Дослідження та експериментальний вибір методу перетворення значень текстових атрибутів об’єктів на вектори 02.12.2025 – 04.12.2025 Виконано 6 Розробка і нформаційн ої технологі ї побудови рекомендацій клієнтам в інформаційній системі агентства нерухомості 05.12.2025 – 07.12.2025 Виконано 7 Розробка модуля побудови рекомендацій у інформаційній системі агентства нерухомості 08.12.2025 – 11.12.2025 Виконано 8 Оформлення пояснювальної записки до кваліфікаційної роботи 12.12.2025 – 13.12.2025 Виконано 9 Перевірка на плагіат 14.12.2025 Виконано 10 Попередній захист кваліфікаційної роботи 15.12.2025 Виконано 11 Захист роботи 18.12.2025 Виконано Дата видачі завдання 24 листопада 2025 р. Здобувач (підпис) Керівник роботи доц. каф. ІУС Тетяна БОРИСЕНКО (підпис) (посада, власне ім’я, прізвище ) 4 РЕФЕРАТ Пояснювальна записка кваліфік аційної роботи: 133 с., 14 рис., 20 табл., 2 дод., 2 4 джерел а. АГЕНТСТВО НЕРУХОМОСТІ, ІНФОРМАЦІЙНА ТЕХНОЛОГІЯ, МЕТОД, РЕКОМЕНДАЦІЙНІ СИСТЕМИ , РЕКОМЕНДАЦІЇ , JAVA . Об’єкт дослідження – процес формування рекомендацій для клієнтів у інформаційній системі агентства нерухомості. Метою роботи є дослідження та експериментальний вибір методів та моделей для побудови рекомендацій користувачам у сфері нерухомості для підвищення якості та точності рекомендацій. До використаних методів дослідження відносяться: зовнішня оцінка (Extrinsic Evaluation ), методологія IDEF 0, експериментальний підхід з використання синтетичних даних та методологія DFD . Теоретичні результати – це опис аналізу та експериментального дослідження методів і моделей , які можна використовувати для формування рекомендацій у рамках content -based підходу , і розроблена інформаційна технологія побудови рекомендацій в інформаційн ій систем і агентства нерухомості . Практичні результати роботи представлені реалізованими компонент ами веб-базовано го модуля побудови рекомендацій . Результати роботи можна використовувати при реалізації системи рекомендацій у специфіці ринку нерухомості та суміжних сферах. Кваліфікаційну роботу виконано згідно методичних вказівок щодо розробки та оформлення кваліфікаційної роботи [1], ДСТУ 3008:2015 [2] і ДСТУ 8302:2015 [3]. 5 ABSTRACT Master’s thesis: 133 pages, 14 figures, 20 tables, 2 appendices, 2 4 sources. INFORMATION TECHNOLOGY , JAVA , METHOD , PROPERTY AGENCY , RECOMMENDATION SYSTEMS , RECOMMENDATIONS . The object of research of the qualification work is the process of forming recommendations for clients in the information system of a property agency . The purpose of the work is to research and experimentally select methods and models for building recommendations for users in the real estate sector in order to improve the quality and accuracy of recommendations . The research methods used include: extrinsic evaluation, IDEF0 methodology, experimental approach using synthetic data, and DFD methodology. Theoretical results – a description of the analysis and experimental study of methods and models that can be used to form recommendations within the framework of a content -based approach, and the developed information technology for building recommendations in the information s ystem of a real estate agency . Practical results"}
{"chunk_id": "2025_M_IUS_Trebunskyh_OV.pdf:1", "source": "2025_M_IUS_Trebunskyh_OV.pdf", "text": "sector in order to improve the quality and accuracy of recommendations . The research methods used include: extrinsic evaluation, IDEF0 methodology, experimental approach using synthetic data, and DFD methodology. Theoretical results – a description of the analysis and experimental study of methods and models that can be used to form recommendations within the framework of a content -based approach, and the developed information technology for building recommendations in the information s ystem of a real estate agency . Practical results are presented by the implemented components of the web - based recommendation building module . The results of the work can be used in the implementation of a recommendation system in the real estate market and related areas . The qualification work was performed in accordance with the methodological guidelines for the development and formatting of qualification work [1], DSTU 3008:2015 [2], and DSTU 8302:2015 [3]. 6 ЗМІСТ С. Скорочення та умовні познаки ................................ ................................ .......... 8 Вступ ................................ ................................ ................................ ..................... 9 1 Аналіз методів та підходів для побудови рекомендацій житла в системі агентства нерухомості ................................ ................................ .................... 10 1.1 Формулювання проблеми побудови рекомендацій клієнтам щодо вибору нерухомості ................................ ................................ ................. 10 1.2 Аналіз методів формування рекомендацій щодо вибору нерухомості ................................ ................................ .............................. 13 1.3 Обґрунтування вибору методів та моделей формування рекомендацій для експериментального порівняння ................................ ..................... 20 1.4 Постановка задач дослідження ................................ .............................. 22 2 Дослідження та експериментальний вибір методів для побудови рекомендацій житла в інформаційній системі агентства нерухомості ..... 25 2.1 Вибір методу розрахунку ступеня подібності об'єктів ....................... 25 2.1.1 Підхід до вибору кращого методу розрахунку ступеня подібності об'єктів ................................ ................................ ............................... 25 2.1.2 Дослідження та перевірка методу косинусної подібності ........... 32 2.1.3 Дослідження та перевірка методу евклідової відстані ................. 39 2.1.4 Дослідження та перевірка методу відстані Гауера ....................... 45 2.1.5 Порівняння методів розрахунку ступеня подібності об'єктів ..... 49 2.2 Вибір методу перетворення значень текстових атрибутів об’єктів на вектори ................................ ................................ ................................ ...... 50 2.2.1 Підхід до вибору кращого методу для перетворення текстових атрибутів об’єктів на вектори ................................ ......................... 50 2.2.2 Дослідження та перевірка BERT -подібної моделі ........................ 51 2.2.3 Дослідження та перевірка методу Bag-of-words ........................... 54 7 2.2.4 Дослідження та перевірка методу TF-IDF ................................ ..... 56 2.2.5 Порівняння методів для перетворення текстових атрибутів об’єктів на вектори ................................ ................................ ........... 59 3 Інформаційна технологія побудови рекомендацій клієнтам в інформаційній системі агентства нерухомості ................................ ............ 60 3.1 Опис інформаційної технології побудови рекомендацій клієнтам в інформаційній системі агентства нерухомості ................................ ..... 60 3.2 Опис впровадження інформаційної технології побудови рекомендацій клієнтам в інформаційній системі агентства нерухомості ................................ ................................ .............................. 63 4 Програмна р еалізація модуля побудови рекомендацій та експериментальна перевірка наукових результатів ................................ .... 65 4.1 Реалізація модуля побудови рекомендацій в інформаційній системі агентства нерухомості ................................ ................................ ............. 65 4.2 Експериментальна перевірка наукових результатів ............................ 71 Висновки ................................ ................................ ................................ ............ 75 Перелік джерел посилання ................................ ................................ ............... 77 Додаток А Програмний код модуля побудови рекомендацій ІС агентства нерухомості ................................ ................................ ................................ ..... 81 Додаток Б Графічний матеріал кваліфікаційної роботи ............................. 120 8 СКОРОЧЕННЯ ТА УМОВНІ ПОЗНАКИ ІС – інформаційна система ІТ – інформаційна технологія CLS – classification token NLP – обробка природної мови 9 ВСТУП З все більшим зростанням накопичення даних про клієнтів зростає попит на системи рекомендацій, які надають клієнтам персональні рекомендації. Великі компанії, що займаються електронною комерцією, все більше використовують системи рекомендацій, з метою зміц нення своєї конкурентної позиції. Дослідження систем рекомендацій постійно підтверджують пряму залежність між рівнем задоволеності споживачів та якістю рекомендацій, зокрема їх точністю і різноманіттям об’єктів, що рекомендує алгоритм [ 4]. Саме з цих обставин дослідження методів побудови рекомендацій є не лише доцільним, але й особливо актуальним у сучасних умовах тенденцій бізнесу, де здатність надавати персоналізовані пропозиції стає ключовим фактором успіху компанії на ринку. Основною м етою цієї роботи є дослідження методів розрахунку ступеня подібності об'єктів і методів та моделей перетворення текстів на вектори. Окрім цього, за вданням цієї роботи є розробка інформаційної технології (ІТ) для побудови рекомендацій у інформаційній системі агентства нерухомості та реалізація програмного модуля на основі цієї технології. 10 1 АНАЛІЗ МЕТОДІВ ТА ПІДХОДІВ ДЛЯ ПОБУДОВИ РЕКОМЕНДАЦІЙ ЖИТЛА В СИСТЕМІ АГЕНТСТВА НЕРУХОМОСТІ 1.1 Формулювання проблеми побудови рекомендацій клієнтам щодо вибору нерухомості Побудова рекомендацій – процес спроби передбачити, які об'єкти, наприклад: житло, фільми, музика, книги, новини тощо, будуть цікаві користувачеві, маючи певну інформацію про нього. Рекомендаційні технології здатні формувати персоналізовані рекомендації, ґр унтуючись на аналізі уподобань, поведінки та інтересів користувачів. Системи рекомендацій є актуальною лінією захисту від проблеми надмірного вибору для користувачів. Через стрімке збільшення обсягу інформації, люди постійно зіштовхуються з величезною кількістю продуктів, фільмів або об’єктів житла, що ускладнює пошук справ ді відповідних для них пропозицій. Саме тому персоналізовані рекомендації стають ключовою стратегією для покращення користувацького досвіду. У цілому такі системи виконують суттєву та незамінну функцію у різних системах доступу до інформації, підтримуючи р озвиток бізнесу, спрощуючи процес ухвалення рішень і широко застосовуючись у багатьох веб -сферах, включно з електронною комерцією"}
{"chunk_id": "2025_M_IUS_Trebunskyh_OV.pdf:2", "source": "2025_M_IUS_Trebunskyh_OV.pdf", "text": "є актуальною лінією захисту від проблеми надмірного вибору для користувачів. Через стрімке збільшення обсягу інформації, люди постійно зіштовхуються з величезною кількістю продуктів, фільмів або об’єктів житла, що ускладнює пошук справ ді відповідних для них пропозицій. Саме тому персоналізовані рекомендації стають ключовою стратегією для покращення користувацького досвіду. У цілому такі системи виконують суттєву та незамінну функцію у різних системах доступу до інформації, підтримуючи р озвиток бізнесу, спрощуючи процес ухвалення рішень і широко застосовуючись у багатьох веб -сферах, включно з електронною комерцією та медійними платформами . Зазвичай рекомендаційні списки створюються з урахуванням вподобань клієнтів , властивостей продуктів, історії взаємодій між ними, а також додаткової інформації, зокрема часових даних (наприклад, рекомендацій, що враховують послідовність дій) та просторових даних (як у випадку з підбором житла чи закладів) [5]. Використання модуля рекомендацій у складі інформаційної системи агентства нерухомості, що спеціалізується на довгостроковій оренді, може значно покращити користувацький досвід та привернути увагу втрачених користувачів. Завдяки аналізу поведінки користувач ів, їхніх попередніх 11 пошуків, переглядів і обраних параметрів, цей модуль здатний пропонувати саме ті об’єкти житла, які найбільше відповідають індивідуальним потребам та інтересам конкретного клієнта. У рамках цієї роботи має бути розроблений модуль рекомендацій для інформаційної системи агентства нерухомості, що спеціалізується на довгостроковій оренді, який буде надсилати електроні листи з рекомендаціями житла користувачам системи. Спрощене представле ння роботи модуля можна побачити на діаграмі активності нижче (рис 1.1). Рисунок 1.1 – Діаграма активності з спрощеним представленням роботи модуля Під час використання системи зареєстрованим користувачем деякі його дії будуть фіксуватися, наприклад, перегляд сторінки житла або задання фільтру під час пошуку, після цього з цієї інформації буде формуватися профіль користувача. Профіль користувача – це векторне представлення уподобань користувача, сформоване на основі його дій у системі, що використовується для прогнозування інтересу до нових об’єктів. Далі серед житла, яке не було переглянуто користувачем, буде обрано те яке найбільш підходить до профілю користувача на основі подібності. Після чого буде формуватися електронний лист, який відправляється користувачу. Електронні л исти з рекомендаціями будуть надсилатися користувачу з періодичністю, зазначеною у таблиці 1.1. 12 Таблиця 1.1 – Періодичність надсилання електронних листів з рекомендаціями користувачам Остання активність (t) Частота надсилання t ≤ 7 днів Раз на чотири дні (наприклад, можна відправляти листи у 1, 5, 9, 13, 17, 21, 25, 29 числах) 7 днів < t ≤ 1 місяць Раз на 8 днів(наприклад у 1, 9, 17, 25 числах) 1 місяць < t ≤ 5 місяців Раз на місяць (наприклад, кожного 1 числа) t > 5 місяців Раз у чотири місяці Окрім цього у листах рекомендацій не має бути житла яке користувач вже переглядав та яке йому вже рекомендували за останні п’ять місяців. Кожен лист має містити інформацію (фото, пріоритетні дані та посилання) про шість рекомендованих об’єктів житла, якщо користувач цікавився різними типами житла, наприклад, квартири та приватні будинки . Рекомендаційний лист має містити різні типи житла у пропорції , відповідній до інтересу користувача. Модуль має ураховувати під час формування рекомендацій такі параметри: орендна плата, адреса (місто, район), кількість кімнат, площа, поверх, вільний опис, близькість до метро, ремонт, наявність побутової техніки (пральної машини, посудомийної машини, бойлера, обігрівача тощо), походження об'єкта та кількість спальних місць. Модуль має коректо працювати у рамках наявної інформаційної системи агентства нерухомості. Модуль має відсилати рекомендаційні листи з урахуванням дій користувача, з певною частотою, відповідно до активності користувача, з урахуванням нещодавніх рекомендацій, має ураховувати основні параметри житла та коректо працювати у рамках наявної інформаці йної системи агентства нерухомості. 13 1.2 Аналіз методів формування рекомендацій щодо вибору нерухомості З урахуванням сучасних тенденцій розширення використання персональних рекомендацій у різних сферах, вони є важливим елементом будь якої системи, яка направлена на роботу з клієнтами та має широкий асортимент. Існують декілька основних підходів до створення персональних рекомендацій. Колаборативна фільтрація – це підхід до створення рекомендацій, що передбачає використання наявних оцінок від групи користувачів для прогнозування невідомих уподобань іншого користувача. Його головна ідея полягає в тому, що люди, які раніше подібно оцінювали однакові об’єкти, зазви чай мають схожі інтереси й надалі, імовірно, надаватимуть подібні оцінки іншим елементам [6]. Наприклад, за допомогою колаборативної фільтрації музичний сервіс здатен визначити, які треки можуть зацікавити користувача, навіть якщо відома лише частина його вподобань чи антипатій. Система створює персоналізовані рекомендації для кожного, спираючись н а сукупні дані, отримані від великої кількості користувачів. До невирішених проблем колаборативної фільтрації відносять: а) розрідженість даних – переважна частина користувачів не залишає оцінок для більшості з товарів, тому формується велика, але розріджена матриця «предмет – користувач», що ускладнює обчислення рекомендацій; б) масштабування – із розширенням бази користувачів у системі виникає проблема ефективного масштабування: обчислення, необхідні для роботи алгоритмів колаборативної фільтрації, стають надто складними та потребують значних ресурсів ; в) проблема холодного старту – поява нових користувачів чи нових об’єктів суттєво ускладнює роботу рекомендаційних систем, що ґрунтуються 14 на цьому метод ; г) різноманітність — спочатку колаборативну фільтрацію розробляли з метою підвищення різноманітності рекомендацій, щоб користувачі мали змогу відкривати нові товари серед великого вибору. Однак на практиці деякі алгоритми, особливо ті,"}
{"chunk_id": "2025_M_IUS_Trebunskyh_OV.pdf:3", "source": "2025_M_IUS_Trebunskyh_OV.pdf", "text": "б) масштабування – із розширенням бази користувачів у системі виникає проблема ефективного масштабування: обчислення, необхідні для роботи алгоритмів колаборативної фільтрації, стають надто складними та потребують значних ресурсів ; в) проблема холодного старту – поява нових користувачів чи нових об’єктів суттєво ускладнює роботу рекомендаційних систем, що ґрунтуються 14 на цьому метод ; г) різноманітність — спочатку колаборативну фільтрацію розробляли з метою підвищення різноманітності рекомендацій, щоб користувачі мали змогу відкривати нові товари серед великого вибору. Однак на практиці деякі алгоритми, особливо ті, що спираються на показни ки продажів чи рейтинги, часто створюють несприятливі умови для просування нових або маловідомих продуктів ; ґ) наявність « білих ворон» – це проблема пов’язана з користувачами, чиї вподобання суттєво відрізняються від більшості, що створює труднощі для систем колаборативної фільтрації. Content -based підхід – це підхід , при якому використовуються властивості об’єкта, щоб запропонувати інші об’єкти, подібні до тих, які вже сподобалися користувачеві, ґрунтуючись на його попередній активності або явних оцінках. До проблем цього підходу відносять: а) розрідженість даних та проблему холодного старту, які вже були описані у контексті колаборативної фільтрації вище, з єдиною різницею, що content -based немає проблем холодного старту при додаванні нових товарів; б) ефект «бульбашки фільтрації» – користувач отримує в основному те, що схоже на попередні уподобання, що обмежує знайомство з чимось новим та може призвести до набридання та втратити інтересу; в) важливість повноти характеристик об'єктів – підхід сильно залежить від того, як описані об'єкти. Якщо характеристики бідні, неповні або погано відображають те, що дійсно подобається користувачеві рекомендації будуть слабкими. Демографічний підхід (Demographic Filtering) – це підхід, що використовує демографічні дані (стать, вік, освіта) користувача для визначення, які товари можуть бути підходящими для рекомендації . Він не має проблеми появи нових користувачів, оскільки не спирається на рейтинги під час формування рекомендацій. Водночас підхід потребує збирання 15 достатнього обсягу демографічної інформації, що звужує можливості його застосування. Часто його поєднують з іншими рекомендаційними методами як додатковий інструмент для підвищення якості результатів [7, 8]. До мінусів демографічного підходу можна віднести: а) конфіденційність – використання демографічних даних стикається з певними проблемами з точки зору конфіденційності та закону; б) точність – знижена точність порівняно з content -based або іншими популярними підходами; в) необхідність демографічних даних – система с самого початку має мати достатню кількість демографічних даних для роботи. Підхід на основі знань (Knowledge -Based Filtering) – Це особливий підхід, який базується на явних знаннях про асортимент товарів, уподобання користувачів та критерії рекомендацій, а саме на тому, який товар варто рекомендувати в певному контексті. Цей підхід використовується в ситуаціях, коли альтернативні п ідходи, зокрема колаборативний або content -based, неможливо застосувати. Однією з ключових переваг рекомендаційних систем на основі знань виступає те, що вони не стикаються з проблемою холодного старту. Проте їхнім суттєвим недоліком стає потенційне обмеження під час отримання знань, що пов'язано з потребою чітко формулювати ре комендаційні знання. Такий підхід демонструє високу ефективність у складних сферах із нечастими покупками товарів, адже рейтингові механізми там зазвичай працюють погано через брак достатньої кількості оцінок . Гібридний підхід – підхід, що передбачає комбінування інших підходів до формування рекомендацій з метою використання їхніх взаємодоповнюючих сильних сторін [8]. До головних проблем гібридного підходу відносять такі: а) складність реалізації та підтримки – гібридний підхід включає комбінування різних технік, що вимагає більше ресурсів, часу на розробку та обслуговування; 16 б) обчислювальні витрати – комбінування різних підходів означає більше обчислень, що може загострити проблеми з продуктивністю та часом відгуку. Як можна побачити з і списку зверху , є достатньо підходів для створення рекомендацій, але у цій роботі вважаю доцільним розглянути content -based підхід через такі його переваги: а) максимальна персоналізація рекомендацій – рекомендації базуються виключно на даних про конкретного користувача, а не на поведінці інших людей, це дозволяє враховувати індивідуальні смаки та особливості користувача; б) відсутність залежності від інших користувачів – на відміну від колаборативних фільтрів, не потрібна велика база користувачів зі схожими інтересами. Працює навіть при невеликій аудиторії, бо достатньо даних про самого користувача та дані контенту який ві н переглядав; в) чіткість – у цьому підході легко пояснити, чому була зроблена рекомендація, це спрощує налагодження і підвищує контроль над роботою системи; г) легка робота з новими об’єктами – нові об’єкти можна відразу рекомендувати, якщо є їх опис, немає проблеми холодного старту для об'єктів; ґ) простота реалізації – для тестової версії достатньо простих алгоритмів обчислення схожості між векторами ознак, легко масштабується і комбінується з іншими методами. При використанні content -based підходу формують профіль користувача, на основі відповідності до нього відбираються найбільш підходячи, для клієнта, об’єкти. Для виявлення відповідності житла існують різні методи для порівняння векторів об’єктів та роботи з вільним текстом. Розглянемо методи, які можна використовувати для порівняння векторів об’єктів. Косинусна подібність (Cosine Similarity) – це метод, необхідний для порівняння векторів об'єктів, що є мірою подібності двох ненульових векторів. 17 Цей метод визначає напрямок векторів, а не їх величину. Це пояснюється тим, що вона обчислює кут між векторами і використовує його як міру подібності. Ця"}
{"chunk_id": "2025_M_IUS_Trebunskyh_OV.pdf:4", "source": "2025_M_IUS_Trebunskyh_OV.pdf", "text": "основі відповідності до нього відбираються найбільш підходячи, для клієнта, об’єкти. Для виявлення відповідності житла існують різні методи для порівняння векторів об’єктів та роботи з вільним текстом. Розглянемо методи, які можна використовувати для порівняння векторів об’єктів. Косинусна подібність (Cosine Similarity) – це метод, необхідний для порівняння векторів об'єктів, що є мірою подібності двох ненульових векторів. 17 Цей метод визначає напрямок векторів, а не їх величину. Це пояснюється тим, що вона обчислює кут між векторами і використовує його як міру подібності. Ця міра є однією з найпоширеніших мір подібності [9]. Евклідова відстань – метод, який представляє собою метрику в евклідовому просторі, що визначає відстань між парою точок евклідового простору шляхом обчислення згідно з теоремою Піфагора [10]. Евклідова відстань представляє собою елементарний спосіб визначення відстані між об'єктами. Цей метод застосовується у різноманітних сферах для розв'язання задач, що стосуються простору та відстані. На противагу косинусній подібності, котра бере до уваги к ут між двома точками чи векторами, евклідова відстань зосереджується на довжині між ними . Відстань Гауера – метод, що представляє собою міру подібності, котра визначає різницю між векторами з комбінуванням числових та категоріальних атрибутів. На противагу традиційним метрикам відстані, зокрема евклідовій відстані, яка орієнтована на обробку ви ключно числових даних, відстань Гауера ефективно працює з наборами даних, що включають обидва типи атрибутів. Цей метод обчислює матрицю відстаней, яка кількісно визначає ступінь відмінності між двома точками даних, при цьому менша відстань свідчить про бі льшу подібність між точками. Відстань Гауера комбінує різноманітні метрики відстані для кожного типу атрибуту та визначає загальну відстань між двома точками як середнє арифметичне індивідуальних відстаней атрибутів. Відстань Гауера є ефективним методом для визначення відмінностей між окремими елементами у наборах даних зі змішаними типами даних . Манхеттенська відстань (Manhattan distance or taxicab metric) – метод, що є сумою зважених довжин відрізків, які сполучають точки , кож ен з яких паралельна координатній осі [ 11]. Відстань Хеммінга (Hamming distance) – це метод, що визначає число позицій, у яких відповідні значення двох векторів ідентичної довжини є різними. Згідно з визначенням цієї метрики, вона не бере до уваги конкретні 18 значення векторів, окрім факту їх відмінності чи рівності. Відстань Махаланобіса (Mahalanobis distance) – метод, що представляє собою метрику, котра використовує концепцію евклідової відстані. Відстані Махаланобіса ґрунтуються на положенні та дисперсії багатовимірного нормального розподілу та дає змогу визначити, наскільки віддалена кожна точка простору від центральної частини цього розподілу [12]. По своїй суті вона є подібною до евклідової відстані, проте містить додатковий зміст, чим ближче розташована точка до «центру мас», тим вища ймовірність того, що це саме та точка, яка нам потрібна . Особливості методів розрахунку ступеня подібності об'єктів представлені у таблиці 1.2. Таблиця 1.2 – Особливості методів розрахунку ступеня подібності об'єктів Назва Особливості Косинусна подібність (Cosine Similarity) Вимірює подібність векторів без врахування їх довжини, добре працює з розрідженими векторами. Евклідова відстань (Euclidean distance) Простий, інтуїтивний, фокусується на відстані між точками. Відстань Гауера (Gower Distance) Підходить для змішаних типів даних Манхеттенська відстань (Manhattan distance) Фокусується на змінних відстанях вздовж осей координат Відстань Хеммінга (Hamming distance) Видає кількість позицій, в яких відповідні значення двох векторів однакової довжини відрізняються Відстань Махаланобіса (Mahalanobis distance) Працює відповідно Евклідової відстані, але враховує відстань до «центру мас» Методи та моделі , які можна використовувати для перетворення текстових характеристик об’єктів на вектори, інакше кажучи для ембедінгу, описуються далі. Вони потрібні для роботи з вільним текстом. Метод Bag -of-Words – цей метод призначений для спрощеного відображення тексту, що застосовується в обробці природних мов та 19 інформаційному пошуку. У даній моделі текст відображається у формі набору його слів без урахування граматичних правил та послідовності слів, однак зі збереженням інформації про їхню кількість . TF-IDF – цей метод представляє собою вдосконалену версію методу Bag-of-Words , котрий не лише підраховує частоту появи слова, а також враховує ступінь важливості цього слова для конкретного тексту відносно всього набору документів . За визначенням, TF IDF є метрикою, що обчислюється як добуток двох величин TF і IDF. Де TF позначає частоту вживання терміна, а IDF обернену частоту документа [13]. Word2Vec – це метод ефективно оброблює природну мову для отримання векторних представлень слів. Ці вектори фіксують інформацію про значення слова з урахуванням сусідніх слів. Крім роботи зі словами, деякі його концепції виявилися ефективними в розробці рек омендаційних механізмів і наданні сенсу даним навіть у комерційних, немовних завданнях. Цю технологію застосували у своїх двигунах рекомендацій такі компанії, як Airbnb, Alibaba, Spotify і Anghami. BERT (Bidirectional Encoder Representations from Transformers або двоспрямовані кодувальні представлення з трансформерів ) – універсальна мовна модель , розроблена для попереднього навчання глибоких двонаправлених представлень на немаркованому тексті через одночасне врахування як лівого, так і правого контексту на всіх шарах. Після попереднього навчання BERT -подібну модель можна точно налаштувати, додавши лише один вихідний шар для створ ення найсучасніш их модел ей широкого спектру задач від векторизації тексту до мовного виводу без значних архітектурних модифікацій під конкретні завдання. BERT є концептуально прост ою та емпірично потужн ою моделлю . Вона"}
{"chunk_id": "2025_M_IUS_Trebunskyh_OV.pdf:5", "source": "2025_M_IUS_Trebunskyh_OV.pdf", "text": "представлення з трансформерів ) – універсальна мовна модель , розроблена для попереднього навчання глибоких двонаправлених представлень на немаркованому тексті через одночасне врахування як лівого, так і правого контексту на всіх шарах. Після попереднього навчання BERT -подібну модель можна точно налаштувати, додавши лише один вихідний шар для створ ення найсучасніш их модел ей широкого спектру задач від векторизації тексту до мовного виводу без значних архітектурних модифікацій під конкретні завдання. BERT є концептуально прост ою та емпірично потужн ою моделлю . Вона показала найкращі результати в одинадцяти задачах обробки природної мови , включаючи підвищ ення бала GLUE до 80,5% (абсолютне покращення 7,7%), точність MultiNLI до 86,7% (абсолютне покращення 4,6%), F1 для SQuAD v1.1 до 93,2 20 (абсолютне покращення 1,5 пункту), F1 для SQuAD v2.0 до 83,1 (абсолютне покращення 5,1 пункту) [14]. Особливості методів для виконання ембед інгу вільного тексту представлені у таблиці 1.3. Таблиця 1.3 – Особливості методів для перетворення текстових атрибутів об’єктів на вектори Назва Особливості Метод Bag -of-Words Ігнорує порядок слів, але враховує їх кількість Метод TF-IDF Працює як Bag -of-Words, але зменшує важливість часто вживаних слів BERT -подібна модель Ефективно оброблює природну мову з використанням нейронної мережи 1.3 Обґрунтування вибору методів та моделей формування рекомендацій для експериментального порівняння У рамках цієї роботи для експериментального порівняння та вибору будуть розглянуті такі найбільш поширені методи для порівняння векторів об’єктів (розрахунку ступеня подібності об’єктів): а) косинусна подібність; б) евклідова відстань; в) відстань Гауера. Косинусна подібність була обрана через такі переваги: а) простота обчислення – формула дуже проста, швидко реалізується та добре оптимізується; б) добре працює з розрідженими даними – коректно працює з розрідженими даними, не спотворюючи результат через нулі. До переваг, через які була обрана евклідова відстань, відносяться такі: 21 а) проста інтерпретація – вимірюється фактична «пряма» відстань між двома точками в просторі, що має чітке геометричне трактування, легко зрозуміти та пояснити без глибоких математичних знань; б) легка у реалізації – проста формула, реалізується у будь -якій мові програмування без великої кількості коду; в) ефективна для числових даних – добре працює, коли всі ознаки мають однакову природу, одиниці вимірювання та масштаб. Відстань Гауера має такі переваги: а) передбачена підтримка змішаних типів ознак – може одночасно працювати з числовими, категоріальними та бінарними даними; б) масштабування кожної ознаки – кожна ознака нормується окремо у діапазон [0,1], тому всі вони роблять співставний внесок у результат. Це вирішує проблему різних одиниць вимірювання; в) стійкість до пропущених значень – коректно обробляє розріджені дані, якщо значення відсутні, ця ознака просто не враховується у розрахунку між конкретною парою об’єктів; г) інтерпретованість – значення відстані завжди у межах [0,1], де 0 означає, що об’єкти ідентичні, а 1, що об’єкти максимально різні. Це спрощує аналіз і порівняння результатів; ґ) гнучкість у вагуванні ознак – можна задавати ваги для кожної ознаки, що дозволяє підкреслити важливі характеристики й зменшити вплив другорядних. Для експериментального порівняння та вибору методу перетворення текстових атрибутів об’єктів на вектори (виконання ембедингу) в роботі будуть розглянуті такі методи та модель : а) метод Bag -of-Words; б) метод TF -IDF; в) модель нейронної мережі з архітектурою BERT . Метод Bag -of-Words був обраний через такі переваги: а) простота – легко розраховується та реалізується; 22 б) легковесність – не потребує великих обчислювальних ресурсів порівняно з сучасними мовними моделями; в) ефективність у простих задачах – для коротких текстів часто дає достатньо точні результати, особливо якщо тексти мають характерні ключові слова. Метод TF -IDF забезпечує такі ключові переваги: а) простота – легко розраховується та реалізується; б) легковесність – не потребує великих обчислювальних ресурсів порівняно з сучасними мовними моделями; в) захищеність від часто вживаних слів – зменшує вагу часто вживаних слів, наприклад: «та», «це» або «на», і підсилює рідкісні, але змістовні терміни, це дозволяє ефективно визначати ключові слова в тексті; г) немає потреби в навчанні – на відміну від нейромереж або мовних моделей, TF -IDF не вимагає тривалого чи складного процесу тренування, оскільки його робота базується на статистичному аналізі частоти слів у документах, а не на навчанні з великих обсягів д аних. До сильних сторін модел ей нейронної мережі з архітектурою BERT можна віднести: а) контекстуальне розуміння , на відміну від Bag -of-Words та TF -IDF, BERT аналізує слова з урахуванням їхнього контексту в реченні ; б) висока точність на складних задачах – BERT демонструє значно кращі результати в задачах класифікації текстів, аналізу тональності, семантичної подібності та пошуку відповідей на питання, особливо коли тексти містять складні граматичні конструкції або багатозначні слова. 1.4 Постановка задач дослідження Головною задачею роботи є дослідження методів та моделей, 23 необхідних для реалізації процесу побудови рекомендацій щодо об’єктів нерухомості клієнтам в інформаційній системі агентства нерухомості, а саме методів для порівняння об’єктів житла та векторизації тексту. Об’єктом дослідження магістерської роботи є процес побудови рекомендацій клієнтам в інформаційній системі агентства нерухомості. Предметом дослідження є методи та моделі , що можна використовувати для побудови рекомендацій клієнтам в інформаційній системі агентства нерухомості. Проблемою дослідження є необхідність облегшення вибору для користувача прі надмірній кількості об’єктів, користувачі"}
{"chunk_id": "2025_M_IUS_Trebunskyh_OV.pdf:6", "source": "2025_M_IUS_Trebunskyh_OV.pdf", "text": "Головною задачею роботи є дослідження методів та моделей, 23 необхідних для реалізації процесу побудови рекомендацій щодо об’єктів нерухомості клієнтам в інформаційній системі агентства нерухомості, а саме методів для порівняння об’єктів житла та векторизації тексту. Об’єктом дослідження магістерської роботи є процес побудови рекомендацій клієнтам в інформаційній системі агентства нерухомості. Предметом дослідження є методи та моделі , що можна використовувати для побудови рекомендацій клієнтам в інформаційній системі агентства нерухомості. Проблемою дослідження є необхідність облегшення вибору для користувача прі надмірній кількості об’єктів, користувачі часто стикаються з незліченною кількістю пропозицій, що ускладнює підбір кращої з них. Метою дослідження є виявлення найбільш підходящих методів для побудови рекомендацій та використання їх у інформаційній системі агентства нерухомості для розсилання рекомендаційних листів на пошту зареєстрованим користувачам. При виконанні даної роботи мають бути вирішені такі задачі дослідження : а) аналіз існуюч их метод ів розрахунку ступеня подібності об’єктів та перетворення текстових атрибутів об’єктів на вектори; б) дослід ження та експериментальний вибір метод у розрахунку ступеня подібності об’єктів (косинусн ої подібн ості, евклідов ої відстан і, відстан і Гауера) для використання при побудові рекомендацій клієнтам в інформаційній системі (ІС) агентства нерухомості; в) дослідження та експериментальний вибір метод у або моделі для перетворення текстових атрибутів об’єктів на вектори (Bag -of-Words , TF-IDF, BERT ) для використання при побудові рекомендацій клієнтам в ІС агентства нерухомості; г) розробити інформаційну технологію побудови рекомендацій клієнтам в ІС агентства нерухомості; ґ) виконати програмну реалізацію інформаційної технології побудови 24 рекомендацій клієнтам в ІС агентства нерухомості; д) виконати експериментальну перевірку отриманих наукових результатів. 25 2 ДОСЛІДЖЕННЯ ТА ЕКСПЕР ИМЕНТАЛЬНИЙ ВИБІР МЕТОДІВ ДЛЯ ПОБУДОВИ РЕКОМЕНДАЦІЙ ЖИТЛА В ІНФОРМАЦІЙНІЙ СИСТЕМІ АГЕНТСТВА НЕРУХОМОСТІ 2.1 Вибір методу розрахунку ступеня подібності об'єктів У роботі будуть досліджені три методи розрахунку ступеня подібності об’єктів: косинусна подібність, евклідова відстань та відстань Гауера . Далі опис аний підхід до вибору кращого з них. 2.1.1 Підхід до вибору кращ ого метод у розрахунку ступеня подібності об'єктів Оцінка методів розрахунку ступеня подібності об'єктів у контексті рекомендаційних систем є доволі складною задачею, і не існує універсальний засіб до її вирішення. Основні методи перевірки діляться на два наступні підходи: а) офлайн -оцінка (Offline Evaluation ) – підхід оцінювання задоволеності користувачів рекомендаціями, використовуючи статичні дані з попередніх взаємодій користувачів. Ці оцінки надають приблизні оцінки ефективності системи. Однак офлайн -оцінка не може точно оцінити нові рекомендації, оскільк и вони відсутні в історичних даних і не можуть бути оцінені як релевантні [15]. До офлайн -оцінок відносять : 1) Precision@K – метрика якості, яка визначає точність як частку релевантних елементів у топ -k рекомендацій [16]; 2) Recall@K – метрика, яка показує, яку частку всіх релевантних об'єктів модель знайшла у своїх перших K результатах; 3) Mean Average Precision (MAP) – метрика для оцінки якості 26 ранжування, яка враховує точність на кожному знайденому релевантному об'єкті та потім усереднює результат; б) онлайн -оцінка (Online Evaluation) – підхід при якому алгоритми рекомендацій впроваджується в онлайн -середовище з метою порівняння їхньої продуктивності [17]. До методів онлайн -оцінювання відносять: 1) A/B-тестування – форма перевірки гіпотез, при якій два варіанти програмного забезпечення порівнюються в реальних умовах з точки зору кінцевого користувача [1 8]. При цьому підході новий алгоритм тестується на підгрупі користувачів і порівнюється з результатами контрольної групи, яка тестувала старий алгоритм. Це найкращий спосіб порівняти новий алгоритм зі старим; 2) тестування методом «багаторукого бандита» – це складний підхід до оцінки алгоритмів , який динамічно розподіляє трафік між різними алгоритмами залежно від їх ефективності. Цей метод дозволяє збалансувати необхідність дослідження і експлуатації, тобто перевіряє різні алгоритми, але віддає перевагу найбільш ефективним моделям; 3) чергування – метод при якому рекомендації різних алгоритмів надаються одним і тим же користувачам одночасно. Це досягається шляхом чергування елементів з різних алгоритмів у списку рекомендацій. Відстежуючи взаємодію користувачів з цими чергуючимися спискам и. В умовах відсутності можливості нормального використання онлайн - оцінки або використання історичних даних, вважаю доцільним взяти підхід з використання синтетичних або підготовлених датасетів, цей підхід має наступні переваги: а) не потрібні реальні дані про поведінку користувачів; б) повний контроль над умовами тестування; в) не потрібно чекати реакції реальних користувачів, як при A/B тестуванні; г) можна тестувати до публічного запуску; 27 ґ) можна створити рідкісні або екстремальні сценарії. У рамках цього підходу для перевірки методів були використані синтетичні вхідні дані, які складалися із дванадцяти житлових об'єктів. Ці об'єкти були розподілені на три категорії («Перша», «Друга» та «Третя»). У кожну категорію було включено по п'ять об'єк тів – три з них виступали як основні представники категорії (вони є схожими один на одного), а два як такі, що частково поєднують окремі значення атрибутів двох різних категорій. Візуальне відображення підготовлених даних можна побачити на рисунку 2.1. Рисунок 2.1 – Візуальне відображення підготовлених для експериментів вхідних даних На цьому рисунку об’єкти розділені категоріями відмічені різними кольорами, а часткові представники відразу двома кольорами категорій до яких вони частково відносяться, пунктиром відмічені об’єкти , які переглянуті користувачем. При оцінюванні методів будуть враховуватися критерії, які представлені у таблиці 2.1. 28 Таблиця 2.1 –"}
{"chunk_id": "2025_M_IUS_Trebunskyh_OV.pdf:7", "source": "2025_M_IUS_Trebunskyh_OV.pdf", "text": "є схожими один на одного), а два як такі, що частково поєднують окремі значення атрибутів двох різних категорій. Візуальне відображення підготовлених даних можна побачити на рисунку 2.1. Рисунок 2.1 – Візуальне відображення підготовлених для експериментів вхідних даних На цьому рисунку об’єкти розділені категоріями відмічені різними кольорами, а часткові представники відразу двома кольорами категорій до яких вони частково відносяться, пунктиром відмічені об’єкти , які переглянуті користувачем. При оцінюванні методів будуть враховуватися критерії, які представлені у таблиці 2.1. 28 Таблиця 2.1 – Критерії оцінювання методів розрахунку ступеня подібності об'єктів Критерій оцінювання Опис критерія Позиції у рейтингу об'єктів Це легкий та інтуїтивний спосіб виявити відповідність результатів Дискримінативна здатність (у %) Дозволяє виявити, наскільки метод відрізняє однокатегоріальні об’єкти від інших Підтримка категоріальних значень Важлива, бо у сфері застосування є багато категоріальних значень Вразливість до екстра значень Впливає на коректність результатів у незвичайних умовах Потреба в налаштуванні коефіцієнтів важливості атрибутів Коефіцієнти важливості необхідні через різний ступінь впливу різних атрибутів об'єкту на вибір Формула розрахунку дискримінативної здатності наведена нижче: 𝑑= 𝑎 (𝑏/100 ) де d – дискримінативна здатність; a – середне значення подібності непереглянутих об’єктів житла категорії «Перша»; b – середне значення подібності усіх непереглянутих об’єктів житла . Для повноцінного відбору оптимального методу розрахунку ступеня подібності об'єктів в межах предметної області необхідно визначити метод для роботи з вільним текстом. Тому на основі первинного аналізу, проведеного у розділі 1.2 , як попередньо обран а була взята реалізація нейронн ої модел і sentence -transformers/paraphrase -multilingual -MiniLM -L12-v2, яка є моделлю класу Sentence -BERT (архітектурна модіфікація BERT ). Тестові дані житла, які використовувались при перевірці методів для роботи з числовими та катег оріальними даними , наведені на рисунку 2.2. Переглянутими були відмічені два об’єкти житла з категорії «Перша» – вони в стовпці « visited » мають значення 1, а не NULL . Інші об’єкти категорії «Перша» у рамках перевірки вважаються релевантними до переглянутих . 29 Рисунок 2.2 – Тестові дані об’єктів житла Тестові дані двох фільтрів , які використовувались при перевірці , наведені на рисунку 2. 3. Рисунок 2. 3 – Тестові дані фільтрів 30 Черговість дій щодо перевірки методів розрахунку ступеня подібності об'єктів житла наведено на рисунку 2. 4. Рисунок 2. 4 – Етапи перевірки методів розрахунку ступеня подібності об'єктів житла На першому кроці створюється узагальнений профіль користувача з використанням переглянутих об'єктів житла і використаних фільтрів . Потім для кожного непереглянутого об’єкта житла розраховується ступінь схож ості 31 із профілем користувача . Далі для об'єкт ів, які не відповідають фільтрам, що використовував користувач, знижується оцінка , в залежності від рівня порушення фільтрів , визначеного за методом, що перевіря ється . Після чого будується рейтинг непереглянутих об'єктів на основі ступеню подібності тільки профілю користувача. У ході оцінювання об’єктів житла на відповідність профіл ю бралися до уваги такі атрибути житла , які увійшли до профіля. Їх перелік наведений в табл. 2.2 . Таблиця 2.2 – Атрибути профіл я користувача Назва атрибуту Тип даних атрибуту Джерело значення атрибуту Перелік населених пунктів Колекція унікальних текстових значень Переглянуте житло Діапазон кількості кімнат у житлі Об’єкт зі статистикою Описи житла, у виді векторів, об’єктів житла Список векторів Діапазон кількості спальних місць у житлі Об’єкт зі статистикою Діапазон орендних плат Об’єкт зі статистикою Діапазон площ житла Об’єкт зі статистикою Перелік районів Колекція унікальних текстових значень Діапазон поверхів Об’єкт зі статистикою Перелік типів об’єктів житла Колекція унікальних текстових значень Перелік приладів та додаткових параметрів об’єктів житла та їх кількість серед переглянутих об’єктів Колекція пар ключ –значення Перелік типів житла Колекція унікальних текстових значень Задані фільтри 32 Продовження таблиці 2.2 Назва атрибуту Тип даних атрибуту Джерело значення атрибуту Максимальна ціна Числовий тип Задані фільтри Перелік кількостей кімнат Колекція унікальних числових значень Назва складової профіля Тип даних складової профіля Перелік кількостей спальних місць Колекція унікальних числових значень Перелік районів Колекція унікальних текстових значень Перелік приладів та додаткових параметрів Колекція пар ключ –значення Перелік населених пунктів Колекція унікальних текстових значень У ході корекції (зменшення) оцінки за порушення умов фільтрів врахову вали ся такі параметри: назв а населен ого пункт у, район житла, кількість спальних місць, кількість кімнат, максимальна орендна плата, тип житла , обладнання та побічн і параметри. 2.1.2 Дослідження та перевірка методу косинусної подібності У методі косинусної подібніст і розрах овується міра подібності між двома векторами передгілбертового простору, яка використовується для вимірювання косинуса кута між ними. У разі порівняння двох об'єктів, косинусна подібність двох об'єктів змінюється в діапазоні від 0 до 1 , де 1 повна збіжність, а 0 абсолютна незбіжність. Формула косинусної подібності виглядає наступним чином: 33 𝑐𝑜𝑠𝑖𝑛𝑒 _𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦 (𝐴,𝐵) = 𝐴∗𝐵 ||𝐴||∗||𝐵|| , (2.1) де A*B – скалярний добуток векторів; ||A|| – норма вектор у A; ||B|| – норма вектор у B [19]. Одна з причин популярності косинусної подібності полягає в тому, що вона ефективна як оціночний показник, особливо для розріджених векторів, оскільки необхідно враховувати тільки ненульові вимірювання. Рекомендаційні системи та великі мовні моделі застосовують косинусну подібність для визначення найбільш релевантного контенту та вибору відповідей, що"}
{"chunk_id": "2025_M_IUS_Trebunskyh_OV.pdf:8", "source": "2025_M_IUS_Trebunskyh_OV.pdf", "text": "0 абсолютна незбіжність. Формула косинусної подібності виглядає наступним чином: 33 𝑐𝑜𝑠𝑖𝑛𝑒 _𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦 (𝐴,𝐵) = 𝐴∗𝐵 ||𝐴||∗||𝐵|| , (2.1) де A*B – скалярний добуток векторів; ||A|| – норма вектор у A; ||B|| – норма вектор у B [19]. Одна з причин популярності косинусної подібності полягає в тому, що вона ефективна як оціночний показник, особливо для розріджених векторів, оскільки необхідно враховувати тільки ненульові вимірювання. Рекомендаційні системи та великі мовні моделі застосовують косинусну подібність для визначення найбільш релевантного контенту та вибору відповідей, що мають найвищу семантичну значущість. В обох випадках косинусна подібність оцінює рівень відповідності між отриманими векторами, сприяючи виявленню закономірностей і зв’язків у складних масивах даних. Вона відіграє ключову роль у галузі рекомендаційн их систем використовую чись як алгоритми пошуку за подібністю, щоб пропонувати товари, медіа або контент, що узгоджуються з поведінкою та вподобаннями користувачів . Далі описуються деталі реалізації для тестування методу косинусної подібності . Оскільки в профілі користувача числові значення (кількість кімнат, спальних місць, орендна плата, площа, поверх) задані у вигляді діапазонів, то під час порівняння кожної координати вектор у береться те значення з діапазону профілю, яке найближче до відповідного значення вектор у житла. Наприклад, якщо значення кількості кімнат у житл і 3, а у профіля діапазон кількості кімнат дорівнює 5 -8, то при порівнянні будуть взяті числа 3 та 5. Для категоріальних значень (місто, район, тип житла) використовується такий підхід: якщо значення житла збігається з одним із значень у списку профілю, то при порівнянні і житло, і профіль отримують значення 1. Якщо ж збігу немає житло отримує значення 0, а профіль 1. В залежності від 34 важливості атрибута можуть додаватися різні коефіцієнти Використання цього підходу для значення атрибуту « район », можна побачити у лістингу 2.1. Лістинг 2.1 – Використання підходу розрахунку подібності до категоріальних значень, для значення атрибуту «район » (фрагмент файлу CosineSimilarity.java) if (profile.getTypes().contains(property.getType())) { divisible += 25; //5*5 normProfile += 25; normProperty += 25; }else{ normProfile += 25; } У цьому л істингу був застосований коефіцієнт 5 через важливість атрибуту житла, divisible позначає скалярний добуток векторів, де у разі співпадіння 5*5 дорівнює 25, а у разі не співпадіння 5*0 дорівнює 0. NormProfile позначає норму профайлу та завжди дорівнює 5*5. NormProperty позначає норму житла та у разі співпадіння 5*5 дорівнює 25, а у разі не співпадіння 0*0 дорівнює 0. У даному випадку, при відсутності співпадіння немає сенсу додавати 0 до divisible та normProperty , тому 25 додається тільки для норми профілю. Під час порівняння значень атрибуту «О пис житла» вони перетворюються у вектори за допомогою реалізації моделі sentence - transformers /paraphrase -multilingual -MiniLM -L12-v2 та порівнюються за класичною косинусною подібністю. Вектор опису житла порівнюється з усіма описами житла у профілі, і середнє значення цих порівнянь використовується як значення атрибуту житла у формулі. Значення профілю при цьому дорівнює 1. Використання цього підходу для атрибуту «Опис», можна побачити у лістингу 2.2. 35 Лістинг 2.2 – Розрахун ок ступеня подібності значень атрибуту «Опис житла» (фрагмент файлу CosineSimilarity.java) for (float[] tempVector : tempVectorsProfile) { double dotProduct = 0.0; double norm1 = 0.0; double norm2 = 0.0; for (int i = 0; i < tempVector.length; i++) { dotProduct += tempVector[i] * propertyVector[i]; norm1 += tempVector[i] * tempVector[i]; norm2 += propertyVector[i] * propertyVector[i]; } norm1 = Math. sqrt(norm1); norm2 = Math. sqrt(norm2); tempCosDesc += dotProduct / (norm1 * norm2); } tempCosDesc = tempCosDesc/tempVectorsProfile.size(); divisible +=tempCosDesc; normProperty += tempCosDesc*tempCosDesc; normProfile += 1; Для порівняння оснащення та додаткових параметрів використовується ваговий підхід: спершу обчислюється сумарна вага всіх елементів обладнання у профілі користувача, після чого визначається сумарна вага тих елементів, які присутні і в профілі, і в житлі. На основі цих значень обчислюється коефіцієнт подібності як частка збіжних ваг, що відображає, наскільки повно житло відповідає списку обладнання та додаткових параметрів профілю. Формулу цього розрахунку можна побачити нижче: 𝐵𝑒= ∑(𝑃∩𝑄) ∑𝑃 , (2.2) де 𝐵𝑒 – коефіцієнт відповідності атрибуту equipment житла до equipment профілю; ∑𝑃 – сума всіх значень профілю ; ∑(𝑃∩𝑄) – сума значень профілю, ключи яких є і у профілю, і у житла . Отриманий коефіцієнт масштабується , після чого використовується у загальній формулі (2.1) як значення житла . Використання цього підходу для обладн ання та додаткових параметрів, 36 можна побачити у лістингу 2.3. Лістинг 2.3 – Використання підходу розрахунку подібності для обладн ання та додаткових параметрів (фрагмент файлу CosineSimilarity.java) Map<Integer, Long> tempEquipmentsProfile = profile.getEquipments(); Map<Integer, Long> tempEquipmentsProperty = property.getEquipments(); long tempColEquipmentPoints = 0; for (Long value : tempEquipmentsProfile.values()) { tempColEquipmentPoints += value; } long tempColPropertyPoints = 0; for (Integer key : tempEquipmentsProperty.keySet()) { if (tempEquipmentsProfile.containsKey(key)) { tempColPropertyPoints += tempEquipmentsProfile.get(key); } } float tempEquipments = 1.0f - ((tempColEquipmentPoints - tempColPropertyPoints) / (float) tempColEquipmentPoi nts); tempEquipments *= 2; divisible += tempEquipments*2; normProfile += 4; normProperty += tempEquipments*tempEquipments; У цьому листингу визначається , наскільки список обладнання та додаткових параметрів об’єкта, який аналізується, відповідає списку профіл ю у межах від 0 до 1. Результат подвоюється та записується як значення для об’єкта , в той час як значення профілю"}
{"chunk_id": "2025_M_IUS_Trebunskyh_OV.pdf:9", "source": "2025_M_IUS_Trebunskyh_OV.pdf", "text": "tempColPropertyPoints = 0; for (Integer key : tempEquipmentsProperty.keySet()) { if (tempEquipmentsProfile.containsKey(key)) { tempColPropertyPoints += tempEquipmentsProfile.get(key); } } float tempEquipments = 1.0f - ((tempColEquipmentPoints - tempColPropertyPoints) / (float) tempColEquipmentPoi nts); tempEquipments *= 2; divisible += tempEquipments*2; normProfile += 4; normProperty += tempEquipments*tempEquipments; У цьому листингу визначається , наскільки список обладнання та додаткових параметрів об’єкта, який аналізується, відповідає списку профіл ю у межах від 0 до 1. Результат подвоюється та записується як значення для об’єкта , в той час як значення профілю дорівнює 2. Рейтинг непереглянутих об’єктів житла , сформований з використанням метода косинусно ї подібн ості без урахування відповідності фільтрам , можна побачити у таблиці 2. 3. Таблиця 2. 3 – Рейтинг непереглянутих об’єктів житла за косинусною подібністю без урахування відповідності фільтрам користувача Порядковий номер у рейтингу Ступень подібності об’єкта Категорія id об’єкта житла 1 0.9803737 Перша 3 2 0.9516129 Перша -друга 4 37 Продовження таблиці 2.3 Порядковий номер у рейтингу Ступень подібності об’єкта Категорія id об’єкта житла 3 0.90336627 Перша -третя 5 4 0.73375815 Друга 8 5 0.7287298 Друга 6 6 0.71834534 Друга 7 7 0.7024365 Друга -третя 9 8 0.66235137 Третя 11 9 0.5505686 Третя 10 10 0.37787086 Третя 12 Як можна побачити, при використані косинусної подібності, усі непереглянуті об’єкти категорії «Перша» (з id 3, 4, 5) попали до перших п’яти місць побудован ого рейтингу об’єктів житла, зайнявши 1,2 та 3 місця. Середня відповідність об’єктів тієї ж категорії дорівнює 0.94511762333 , у той час як середня відповідність усіх непереглянутих об’єктів лише 0.730941349 . Об’єкти тієї ж категорії являють собою 3 9% всієї важливості об’єктів. Рейтинг непереглянутих об’єктів житла, сформований з урахуванням зменшення оцінки за фільтрами користувача наведено у таблиці 2. 4. Таблиця 2. 4 – Рейтинг непереглянутих об’єктів житла за косинусною подібністю з урахуванням відповідності фільтрам користувача Порядковий номер у рейтингу Ступень подібності об’єкта Категорія id об’єкта житла 1 0.93741655 Перша 3 2 0.81680024 Перша -друга 4 3 0.7405913 Перша -третя 5 4 0.6183477 Друга 6 5 0.6015443 Друга 8 6 0.52283 Друга 7 7 0.42908517 Третя 11 8 0.3847403 Друга -третя 9 9 0.3693326 Третя 10 38 Продовження таблиці 2.4 Порядковий номер у рейтингу Ступень подібності об’єкта Категорія id об’єкта житла 10 0.2389865 Третя 12 Як можна побачити, при використані косинусної подібності з урахуванням зменшення оцінки за фільтрами користувача, усі об’єкти тієї ж категорії попали до перших п’яти місць побудова ного рейтингу об’єктів житла, зайнявши 1,2 та 3 місця. Середня відповідність об’єктів тієї ж категорії дорівнює 0.83160269666 , у той час як середня відповідність усіх об’єктів лише 0.565967466 . Об’єкти тієї ж категорії являють собою 44% всієї важливості об’єктів. Вплив урахування фільтрів користувача на якість ранжування наведені у таблиці 2. 5. Таблиця 2. 5 – Вплив урахування фільтрів користувача на якість ранжування Показник Базовий метод З урахуванням фільтрів Різниця Кількість у топ -5 об'єктів, які є однокатегоріальними із переглянутими об'єктами 3 з 3 3 з 3 0 Позиції 1, 2, 3 1, 2, 3 0 Середній ступень подібності об'єктів, які є однокатегоріальними із переглянутими об'єктами 0.945 0.832 -11.96 % Середній ступень подібності непереглянутих об'єктів 0.731 0.566 -22.57 % Дискримінативна здатність 29% 47% +18% Дискримінативна здатність – здатність розрізняти об'єкти, явища або стимули за певними ознаками. На основі результатів з таблиці 2. 5 можна затверджувати, що 39 використання фільтрів збільш ує ефективн ість метод у косинусної подібності при розрахунку ступню подібності об’єктів житла . 2.1.3 Дослідження та перевірка методу евклідової відстані Евклідова відстань – метрика в евклідовому просторі . Вона представляється як відстань між двома точками евклідового простору, що обчислюється за теоремою Піфагора. Наприклад, для векторів p та q евклідова відстань визначається наступним чином: 𝑑(𝑝,𝑞)=√(𝑝1−𝑞1)2+(𝑝2−𝑞2)2+⋯(𝑝𝑛−𝑞𝑛)2 , (2.3) де d – відстань; p, q – вектори; 𝑝𝑛,𝑞𝑛 – координати точок [20]. Евклідова метрика – найбільш природна функція відстані, що виникає в геометрії, яка відображає інтуїтивні властивості відстані між точками. Евклідова відстань є простим способом вимірювання відстані між об'єктами. Вона використовується в різних галузях для вирішення завдань, пов'язаних з простором і відстанню. На відміну від косинусної подібності, яка враховує кут між двома точками або векторами, евклідова відстань фокусуватися на довжині лінії між ними . Також вона не обмежена відстанню від 0 до 1, замість цього більше значення позначає більшу відстань векторів. Далі описуються деталі реалізації для тестування методу евклідової відстані . Так, само, як і під час робот и з косинусною подібністю є числові значення (кількість кімнат, спальних місць, орендна плата, площа, поверх) , 40 задані у вигляді діапазонів, то му під час порівняння кожної координати вектору береться те значення з діапазону профілю, яке найближче до відповідного значення вектору житла. Приклад такого розрахунку з використанням вагов ого коефіцієнт у, що дорівнює 3 , наведений нижче у лістингу 2.4 . Лістинг 2. 4 – Приклад розрахунку відстані при порівнянні з діапазоном та використанням коефіцієнту (фрагмент файлу EuclideanDistance.java ) IntSummaryStatistics tempProfileRooms = profile.getRooms(); int tempPropertyRooms = property.getColRooms(); //distance +=0 -0=0; if (tempProfileRooms.getMin() > tempPropertyRooms) { distance += (float) Math. pow((tempPropertyRooms - tempProfileRooms.getMin())*3, 2);"}
{"chunk_id": "2025_M_IUS_Trebunskyh_OV.pdf:10", "source": "2025_M_IUS_Trebunskyh_OV.pdf", "text": "під час порівняння кожної координати вектору береться те значення з діапазону профілю, яке найближче до відповідного значення вектору житла. Приклад такого розрахунку з використанням вагов ого коефіцієнт у, що дорівнює 3 , наведений нижче у лістингу 2.4 . Лістинг 2. 4 – Приклад розрахунку відстані при порівнянні з діапазоном та використанням коефіцієнту (фрагмент файлу EuclideanDistance.java ) IntSummaryStatistics tempProfileRooms = profile.getRooms(); int tempPropertyRooms = property.getColRooms(); //distance +=0 -0=0; if (tempProfileRooms.getMin() > tempPropertyRooms) { distance += (float) Math. pow((tempPropertyRooms - tempProfileRooms.getMin())*3, 2); }else if(tempProfileRooms.getMax() < tempPropertyRooms){ distance += (float) Math. pow((tempPropertyRooms - tempProfileRooms.getMax())*3, 2); } При роботі з категоріальними даними (місто, район, тип житла) використовується наступний підхід, якщо категорія житла є у листі категорій профілю відстань не змінюється, якщо такої категорії немає, відстань збільшується на відстань відповідну важливості параметру, приклад використання цього підходу можна побачити у лістингу 2.5. Лістинг 2. 5 – Приклад розрахунку відстані для категоріальних значень (фрагмент файлу EuclideanDistance.java ) if (!profile.getCities().contains(property.getCity())) { //0-0=0; distance = (float) Math. pow((0-30), 2); }; Під час порівняння описів об’єктів житла вони перетворюються у вектори за допомогою реалізації моделі sentence -transformers/paraphrase - multilingual -MiniLM -L12-v2, після чого порівнюються за класичним варіантом евклідової відстані. Вектор опису житла порівнюється з усіма описами житла, у профілі, і середнє значення цих порівнянь використовується як значення 41 житла у формулі. Значення профілю при цьому дорівнює 0. Використання цього підходу для значення атрибуту «О пис житла» , можна побачити у лістингу 2.6. Лістинг 2. 6 – Розрахун ок відстані для атрибуту «Опис» (фрагмент файлу EuclideanDistance.java ) List<float[]> tempVectorsProfile = profile.getDescriptionVectors(); float[] propertyVector = property.getDescriptionVector(); if(!tempVectorsProfile.isEmpty() && propertyVector != null && propertyVector.length > 0){ double tempEclidDesc = 0; for (float[] tempVector : tempVectorsProfile) { float radicand = 0; for (int i = 0; i < tempVector.length; i++) { radicand += (float) Math. pow((tempVector[i] - propertyVector[i]), 2); } tempEclidDesc += Math. sqrt(radicand); } tempEclidDesc = tempEclidDesc/tempVectorsProfile.size(); distance += (float) Math. pow((0-tempEclidDesc)*3, 2); }; Для коректної роботи числових значень орендної плати був обраний відносний підхід, при якому різниця рахується не у стандартних числах, а у середній вартості житла профілю, після підрахунку отриманий результат помножується на коефіцієнт 7. Реалізацію д аного розрахун ку можна побачити у лістингу 2.7. Лістинг 2. 7 – Розрахун ок відстані для атрибуту «О рендн а плат а» (фрагмент файлу EuclideanDistance.java ) IntSummaryStatistics tempProfilePrice = profile.getPrice(); int tempPropertyPrice = property.getPrice(); //0-0=0; if (tempProfilePrice.getMin() > tempPropertyPrice) { float tempPriceMin = (((tempProfilePrice.getMin() - tempPropertyPrice)) / (float) tempProfilePrice.getAverage())*7; distance += ((float) Math. pow(0-tempPriceMin, 2)); }else if(tempProfilePrice.getMax() < tempPropertyPrice){ 42 float tempPriceMax = ((tempPropertyPrice - tempProfilePrice.getMax()) / (float) tempProfilePrice.getAverage())*7; distance += ((float) Math. pow(0-tempPriceMax, 2)); } Для порівняння параметрів обладнання та додаткових параметрів спочатку підраховується загальна кількість балів обладнання в профілі. Потім підсумовуються бали тільки тих елементів, які присутні і в профілі, і у об’єкті житла . Ці числа використовуються у фінальній формулі. Використання цього підходу для обладн ання та додаткових параметрів, можна побачити у лістингу 2.8. Лістинг 2. 8 – Розрахун ок відстані для обладн ання та додаткових параметрів (фрагмент файлу EuclideanDistance.java ) Map<Integer, Long> tempEquipmentsProfile = profile.getEquipments(); Map<Integer, Long> tempEquipmentsProperty = property.getEquipments(); long tempColEquipmentPoints = 0; for (Long value : tempEquipmentsProfile.values()) { tempColEquipmentPoints += value; } long tempColPropertyPoints = 0; for (Integer key : tempEquipmentsProperty.keySet()) { if (tempEquipmentsProfile.containsKey(key)) { tempColPropertyPoints += tempEquipmentsProfile.get(key); } } distance += (float) Math. pow(tempColEquip mentPoints - tempColPropertyPoints, 2); Рейтинг непереглянутих об’єктів житла , сформований з використанням евклідової відстані , можна побачити у таблиці 2. 6. 43 Таблиця 2. 6 – Рейтинг непереглянутих об’єктів житла за евклідовою відстанню Порядковий номер у рейтингу Евклідова відстань Категорія об’єкта Id об’єкта 1 11.216299 Перша 3 2 21.988783 Перша -третя 5 3 22.321035 Перша -друга 4 4 31.994844 Друга 6 5 32.317074 Друга 8 6 33.53902 Друга 7 7 33.568184 Третя 11 8 34.97311 Друга -третя 9 9 45.392124 Третя 12 10 46.411488 Третя 10 Як можна побачити, при використані евклідової відстані , усі об’єкти тієї ж категорії попали до перших п ’яти місць створеного рейтингу житла, зайнявши 1, 2 та 3 місця. Середня ві дстань об’єктів тієї ж категорії дорівнює 18.5087056667 , у той час як середня від стань усіх об’єктів 31.3721961 . Об’єкти тієї ж категорії мають середню відстань на 41% менше ніж усі непереглянуті об’єкти . Рейтинг непереглянутих об’єктів житла , сформований з використанням евклідової відстані та урахуванням зменшення оцінки за фільтрами (фінальний) користувача наведено у таблиці 2.7. Таблиця 2. 7 – Рейтинг непереглянутих об’єктів житла за евклідовою відстанню з урахуванням фільтр ів користувача Порядковий номер у рейтингу Евклідова відстань Категорія об’єкта Id об’єкта 1 12.716299 Перша 3 2 26.571035 Перша -друга 4 3 27.38554 Перша -третя 5 4 40.513206 Друга 6 44 Продовження таблиці 2.7. Порядковий номер у рейтингу Евклідова відстань Категорія об’єкта Id об’єкта 5 41.556934 Друга 8 6 42.28902 Друга 7 7 43.692413 Третя 11 8 43.886024 Друга -третя 9 9 56.546986 Третя 10 10 57.937958 Третя 12 Як можна побачити, при використані евклідової відстані ,"}
{"chunk_id": "2025_M_IUS_Trebunskyh_OV.pdf:11", "source": "2025_M_IUS_Trebunskyh_OV.pdf", "text": "з урахуванням фільтр ів користувача Порядковий номер у рейтингу Евклідова відстань Категорія об’єкта Id об’єкта 1 12.716299 Перша 3 2 26.571035 Перша -друга 4 3 27.38554 Перша -третя 5 4 40.513206 Друга 6 44 Продовження таблиці 2.7. Порядковий номер у рейтингу Евклідова відстань Категорія об’єкта Id об’єкта 5 41.556934 Друга 8 6 42.28902 Друга 7 7 43.692413 Третя 11 8 43.886024 Друга -третя 9 9 56.546986 Третя 10 10 57.937958 Третя 12 Як можна побачити, при використані евклідової відстані , усі об’єкти тієї ж категорії попали до перших п ’яти місць створеного рейтингу житла, зайнявши 1, 2 та 3 місця. Середня ві дстань об’єктів тієї ж категорії дорівнює 22.2242913333 , у той час як середня від стань усіх об’єктів 39.3095415 . Об’єкти тієї ж категорії мають середню відстань на 43% менше ніж усі непереглянуті об’єкти . Вплив урахування фільтрів користувача на якість ранжування наведені у таблиці 2. 8. Таблиця 2. 8 – Вплив урахування фільтрів користувача на якість ранжування Показник Базовий метод З фільтрами Різниця Кількість у топ -5 об'єктів, які є однокатегоріальними із переглянутими об'єктами 3 з 3 3 з 3 0 Позиції об'єктів, які є однокатегоріальними із переглянутими об'єктами, у рейтингу 1, 2, 3 1, 2, 3 0 Середня відстань для об'єктів, які є однокатегоріальними із переглянутими об'єктами 18.50 9 22.224 +20% Середня відстань для непереглянутих об’єктів 31.372 39.31 +25% Дискримінативна здатність 41% 43% +2% 45 На основі результатів з таблиці 2. 8 можна затверджувати, що врахування використаних фільтрів при розрахунку евклідової відстані підвищує дискримінативну здатність методу . 2.1.4 Дослідження та перевірка методу відстані Гауера Відстань Гауера – це метод, який дозволяє вимірювати «відстань» між об'єктами, у яких ознаки можуть бути числові та категоріальні. Ключовою особливість методу є нормалізація кожної ознаки відповідно максимального та мінімального значення точки серед вектор ів для числових значень. Дистанція обчислюється наступним чином для кожної ознаки знаходять її нормовану відстань, потім усі отримані відстані підсумовують і діляться на кількість ознак, які порівнювали. Традиційні метрики, такі як евклідова відстань, не можуть у чистому виді обробляти категоріальні дані. Наприклад, у категоріальних атрибутах, таких як «Тип житла » або «Місто», між категоріями немає внутрішньої «відстані». Відстань Гауера враховує це, надаючи простий, але потужний спосіб обчислення відстаней, що включає обидва типи даних в єдиній структурі. Відстань Гауера поєднує різні метрики відстані для кожного типу атрибуту і обчислює загальну відстань між двома точками як середнє значення індивідуальних відстаней атрибутів. Для числових атрибутів ми розраховуємо нормалізовану різницю: 𝑑=|𝑥𝑖−𝑥𝑗| 𝑅 , (2.4) де d – відстань; 𝑥𝑖,𝑥𝑗– числові значення; 46 R – різниця між максимальним на мінімальним значенням атрибуту. Для категоріальних значень: 𝑑={0 𝑖𝑓 𝑥𝑖=𝑥𝑗 1 𝑖𝑓 𝑥𝑖 ≠ 𝑥𝑗 , (2.5) де d – відстань; 𝑥𝑖,𝑥𝑗– категоріальні значення . Для остаточного розрахунку відстані Гауера використовується наступна формула: 𝐷(𝑖,𝑗)=1 𝑝∑ 𝑑𝑖,𝑗𝑘 𝑝 𝑘=1 , (2.6) де D – відстань між двома точками даних; p – кількість елементів; k – номер елементу; d – відстань між двома одномірними елементами різних точок даних [21]. Відстань Гауера – це потужний метод для вимірювання відмінностей між окремими елементами в наборах даних із змішаними типами даних. Він забезпечує надійне рішення там, де традиційні показники виявляються недостатніми, що робить його ідеальним для використа ння у кластеризації, системах рекомендацій та сегментації клієнтів. Обробляючи як числові, так і категоріальні змінні, він гарантує, що всі аспекти даних будуть враховані. Реалізація відстані Гауера для порівняння об’єктів житла , на відміну від косинусної подібності та евклідової відстані, фактично не потребувала якихось змін у формулі, єдине, що були додані коефіцієнти які піднімають вплив міста та понижують вплив поверху на фінальний результат. Рейтинг об’єктів житла , сформований з використанням відстані Гауера та без урахування фільтрів користувача можна побачити у таблиці 2. 9. 47 Таблиця 2. 9 – Рейтинг житла за відстан ню Гауера без урахування фільтрів користувача Порядковий номер у рейтингу Відстань Гауера Категорія об’єкта Id об’єкта 1 0.15962613 Перша 3 2 0.2850607 Перша -друга 4 3 0.3295318 Перша -третя 5 4 0.3922969 Друга 8 5 0.40111235 Друга 6 6 0.41375095 Друга 7 7 0.46404877 Друга -третя 9 8 0.50908226 Третя 11 9 0.65810627 Третя 10 10 0.72134596 Третя 12 Як можна побачити, при використані відстані Гауера , усі об’єкти тієї ж категорії попали до перших п ’яти місць створеного рейтингу житла, зайнявши 1, 2 та 3 місця. Середня ві дстань об’єктів тієї ж категорії дорівнює 0.25807287666 , у той час як середня від стань усіх об’єктів 0.433396209 . Об’єкти тієї ж категорії мають середню відстань на 40% менше ніж усі непереглянуті об’єкти . Рейтинг житла сформований з використанням відстані Гауера та урахуванням зменшення оцінки за фільтрами користувача наведений у таблиці 2.10. Як можна побачити, при використані відстані Гауера з використанням фільтрів усі об’єкти тієї ж категорії попали до перших п ’яти місць створеного рейтингу житла, зайнявши 1, 2 та 3 місця. Середня ві дстань об’єктів тієї ж категорії дорівнює 0.29557287333 , у той час як середня від стань усіх об’єктів 0.522562869 . Об’єкти"}
{"chunk_id": "2025_M_IUS_Trebunskyh_OV.pdf:12", "source": "2025_M_IUS_Trebunskyh_OV.pdf", "text": "середню відстань на 40% менше ніж усі непереглянуті об’єкти . Рейтинг житла сформований з використанням відстані Гауера та урахуванням зменшення оцінки за фільтрами користувача наведений у таблиці 2.10. Як можна побачити, при використані відстані Гауера з використанням фільтрів усі об’єкти тієї ж категорії попали до перших п ’яти місць створеного рейтингу житла, зайнявши 1, 2 та 3 місця. Середня ві дстань об’єктів тієї ж категорії дорівнює 0.29557287333 , у той час як середня від стань усіх об’єктів 0.522562869 . Об’єкти тієї ж категорії мають середню відстань на 43% менше ніж усі непереглянуті об’єкти . 48 Таблиця 2. 10 – Рейтинг житла за відстан ню Гауера та урахуванням зменшення оцінки за фільтрами користувача Порядковий номер у рейтингу Відстань Гауера Категорія об’єкта Id об’єкта 1 0.17212613 Перша 3 2 0.32672736 Перша -друга 4 3 0.38786513 Перша -третя 5 4 0.46361235 Друга 6 5 0.47563022 Друга 8 6 0.48458427 Друга 7 7 0.56404877 Друга -третя 9 8 0.6486656 Третя 11 9 0.79560626 Третя 10 10 0.9067626 Третя 12 Вплив урахування фільтрів користувача на якість розрахунку ступеня подібності наведені у таблиці 2. 11. Таблиця 2. 11 – Вплив урахування фільтрів користувача на якість ранжування Показник Базовий метод З фільтрами Різниця Кількість у топ -5 об'єктів, які є однокатегоріальними із переглянутими об'єктами 3 з 3 3 з 3 0 Позиції об'єктів, які є однокатегоріальними із переглянутими об'єктами, у рейтингу\" 1, 2, 3 1, 2, 3 0 Середня відстань для об'єктів, які є однокатегоріальними із переглянутими об'єктами 0.258 0.296 +15% Середня відстань для непереглянутих об’єктів 0.433 0.523 +21% Дискримінативна здатність 40% 43% +3% На основі результатів з таблиці 2. 11 можна затверджувати, що 49 використання фільтрів при використанні евклідової відстані також, як і при використанні вже розглянутих методів, збільшує ефективність відбору релевантних профілю об’єктів житла . 2.1.5 Порівняння метод ів розрахунку ступеня подібності об'єктів Проведене дослідження продемонструвало, що всі три методи обчислення ступеня подібності об'єктів показують схож і результати за умови належного налаштування. Ключовим фактором ефективності виявилося правильне визначення ваг і коефіцієнтів для кожного методу, що забезпечило розміщення однокатегоріальн их об'єктів на перших позиціях у всіх випадках. Водночас косинусна подібність продемонструвала дещо кращі показники порівняно з розглянутими альтернативними підходами. П орівняльні характеристики методів розрахунк у ступеня подібності можна побачити у таблиці 2.1 2. Таблиця 2.1 2 – Порівняльні характеристики методів розрахунку ступеня подібності об'єктів Метод Критерій Косинусна подібність Евклідова відстань Відстань Гауера Позиції об'єктів, які є однокатегоріальними із переглянутими об'єктами, у рейтингу 1-3 позиції 1-3 позиції 1-3 позиції Дискримінативна здатність 47% 43% 43% Підтримка категоріальних значень З доробкою З доробкою Так Вразливість до екстра значень Висока Висока Низька Потреба в налаштуванні коефіцієнтів важливості атрибутів Висока Висока Низька Кількість переваг 4,5 3,5 4 50 Кожен досліджений метод має певні переваги та обмеження. Відстань Гауера без додаткових модифікацій здатна працювати з категоріальними змінними та містить вбудовану нормалізацію параметрів, яка виявилася настільки ефективною, що практично не потребувала використання ваг під час тестування. Проте залежність цього методу нормалізації від найбільшого та найменшого значень серед вектор ів, що порівнються, робить його вразливим до зменшення значення выдстані атрибутів при наявності навіть хоча б одного значення з аномаль ною величиною . Косинусна подібність та евклідова відстань, навпаки, потребують розробки власних механізмів нормалізації та обробки категоріальних змінних, але демонструють більшу стійкість до аномальних значень. Варто відзначити, що налаштування нормалізації та ваг для евклідової відстані є більш інтуїтивним завдяки прозорішій інтерпретації цього методу. Для реалізації модуля було обрано модифіковану косинусну подібність як оптимальний метод побудови рекомендацій щодо вибору нерухомості. Це рішення обґрунтовується вищою стабільністю порівняно з відстанню Гауера та кращими результатами експериментального те стування. Для обробки категоріальних змінних до методу інтегровано підхід, аналогічний методу відстані Гауера, а також прийнято використання систем и ваг відповідно до значущості атрибутів. 2.2 Вибір методу перетворення значень текстових атрибутів об’єктів на вектори 2.2.1 Підхід до вибору кращого методу для перетворення текстових атрибутів об’єктів на вектори Оцінка методів для перетворення текстових атрибутів об’єктів на вектори у контексті рекомендаційних систем буде проводитися за зовнішньою 51 оцінкою ( Extrinsic Evaluation ), тобто вимірюванням ефективності методів при використанні в реальних завданнях , що дає більш практичне уявлення про їх корисність . Для експериментального порівняння методів ембедингу були обрані методи Bag -of-Words, TF -IDF та реалізація нейронної моделі sentence - transformers/paraphrase -multilingual -MiniLM -L12-v2, яка є моделлю класу Sentence -BERT (архітектурна модіфікація BERT ). Для проведення експеримент у були використані ті ж самі об’єкти житла, що і при дослідженні методів розрахунку ступеня подібності . Але замість об’єктів житла порівнювалися значення текстового атрибуту \"О пис об'єкту житла \". Довжина описів складала від 30 до 40 слів. Два тексти категорії «Перша» були відмічені як переглянуті , і усі інші тексти перевір ялись на подібність до них. При виконанні експерименту методи ембедингу були використані для перетворення текстів на вектори, а обраний раніше метод розрахунку ступеня подібності (метод косинусної подібності) – для визначення подібності об'єктів. При виборі кращого методу враховува лися критерії , що представлені у таблиці 2.13. Таблиця"}
{"chunk_id": "2025_M_IUS_Trebunskyh_OV.pdf:13", "source": "2025_M_IUS_Trebunskyh_OV.pdf", "text": "замість об’єктів житла порівнювалися значення текстового атрибуту \"О пис об'єкту житла \". Довжина описів складала від 30 до 40 слів. Два тексти категорії «Перша» були відмічені як переглянуті , і усі інші тексти перевір ялись на подібність до них. При виконанні експерименту методи ембедингу були використані для перетворення текстів на вектори, а обраний раніше метод розрахунку ступеня подібності (метод косинусної подібності) – для визначення подібності об'єктів. При виборі кращого методу враховува лися критерії , що представлені у таблиці 2.13. Таблиця 2.13 – Критерії оцінювання методів перетворення текстових атрибутів на вектори Критерій Опис критерія Позиції об'єктів, які є однокатегоріальними із переглянутими об'єктами, у рейтингу Це легкий та інтуїтивний спосіб виявити відповідність результатів Дискримінативна здатність (в %) Для виявлення наскільки метод відрізняє однокатегоріальні об’єкти від інших 2.2.2 Дослідження та перевірка BERT -подібної моделі Реалізація нейронної моделі sentence -transformers/paraphrase - 52 multilingual -MiniLM -L12-v2 є моделлю класу Sentence -BERT . Sentence -BERT є архітектурною модіфікацією BERT . BERT (Bidirectional Encoder Representations from Transformers) – це провідна модель для роботи з природною мовою, яка базується на архітектурі трансформера. Ця нейромережева структура застосовує механізм уваги для опрацювання та інтерпретації текстових даних . Механізм уваги при о броб ці природної мови (NLP ) дає моделі змогу під час аналізу акцентувати увагу на найвагоміших сегментах тексту, для його кращого розуміння . Для перевірки метода BERT була обрана модель «paraphrase - multilingual -MiniLM -L12-v2». Це багатомовна модель векторизації речень, вона відображає тексти в 384 -вимірному щільному векторному просторі і може використовуватися для таких завдань, як порівняння або семантичний пошук. Для використання цієї моделі був вибраний Java-фреймворк Deep Java Library (DJL), призначений для роботи з моделями машинного навчання, який забезпечує можливість завантаженнята застосування моделей безпосередньо в Java-середовищі без потреби ручної взаємодії з Python -екосистемою . При роботі спочатку потрібно загрузити модель, як представлено у лістингу 2.9. Лістинг 2. 9 – Код з авантаження моделі paraphrase -multilingual -MiniLM - L12-v2 (фрагмент з файлу BertTextToVector.java ) Criteria<String, float[]> criteria = Criteria. builder() .setTypes(String.class, float[].class) .optModelUrls(\"djl://ai.djl.huggingface.pytorch/sentence - transformers/paraphrase -multilingual -MiniLM-L12-v2\") .optEngine(\"PyTorch\") .optProgress(new ProgressBar()) .build(); this.model = criteria.loadModel(); this.predictor = model.newPredictor(); Заголом векторизиц ія тут працює так, при створ ені об'єкт у 53 BertTextToVector, у конструкторі відбувається завантаження моделі. DJL завантажує з Hugging Face файли нейромережі BERT. PyTorch ініціалізує всю структуру нейромережі. З завантаженої моделі створюється predictor, який буде керувати процесом ембедінгу . При в иконанні predictor.predict(), текст підгот авлюється для нейромережі, тому що BERT не розуміє слова безпосередньо. Токенайзер розбиває фразу на маленькі частини (subword tokens) і перетворює кожну частину в числовий ID згідно з вбудованим словником. Ці числа упаковуються в тензор , спеціальну структуру даних. Також створюється attention mask (маска уваги), яка показує, які токени справжні, а які додані для вирівнювання довжини. Все це відправляється в BERT. Всередині BERT , перший шар (embedding) перетворює кожен числовий ID у вектор із 384 чисел , початкове представлення токена. Далі ці вектори проходять через 12 однакових блоків трансформера. У кожному блоці є механізм self -attention, де обчислює ться для кожного токена ваг а важливості всіх інших токенів, потім формує ться новий вектор як зважен а сума їх представлень. Після attention йдуть feed -forward шари, які роблять нелінійні перетворення, збагачуючи уявлення. Кожен з 12 блоків робить вектори все більш «розу мними», насичуючи їх контекстом і змістом. На виході з останнього шару BERT отримується матриц я векторів , по одному вектору на кожен токен. Далі застосовується pooling , вибір одного вектора з матриці вихідних векторів BERT , зазвичай береться вектор спеціального токена (CLS) з першої позиції. Отриманий вектор нормалізується (зазвичай діленням на його довжину), щоб усі вектори були порівнянні за масштабом. Нарешті, тензор конвертується з внутрішнього формату PyTorch у звичайний Java масив float[] та Predictor повертає масив із 384 чисел. Рейтинг текстів сформований з використанням BERT та косинусної подібності можна побачити нижче, у таблиці 2. 14. 54 Таблиця 2.1 4 – Рейтинг текстів сформований з використанням BERT - подібної моделі та косинусної подібності Порядковий номер у рейтингу Ступень подібності Категорія об’єкта 1 0.7691437304019928 Перша 2 0.7372039556503296 Перша -третя 3 0.7170676589012146 Перша -друга 4 0.6820610463619232 Друга 5 0.6545446813106537 Друга -третя 6 0.65058434009552 Друга 7 0.6341002583503723 Третя 8 0.5926044881343842 Друга 9 0.5743103325366974 Третя 10 0.5270087420940399 Третя При використані BERT -подібної моделі з косинусною подібністю , усі текст ові описи непереглянутих об’єктів категорії «Перша» попали до перших трьох місць створеного рейтингу. Середня подібність текстів тієї ж категорії дорівнює 0.74113844831 , у той час , як середня подібність усіх непереглянутих текстів – лише 0.65386292338 . Тексти тієї ж категорії мають середню подібність на 13% більше ніж непереглянуті тексти . 2.2.3 Дослідження та перевірка методу Bag-of-words Bag of Words був одним із ранніх практичних методів , який дозволяв подати текст у вигляді чис ел. Цей метод реалізує п ринцип BoW , який полягає в тому, що текст перетворюється на вектор тексту, де кожний елемент показує, скільки разів певне слово зустрічається в документі . Механізм методу такий : спочатку формується корпус або"}
{"chunk_id": "2025_M_IUS_Trebunskyh_OV.pdf:14", "source": "2025_M_IUS_Trebunskyh_OV.pdf", "text": "текстів – лише 0.65386292338 . Тексти тієї ж категорії мають середню подібність на 13% більше ніж непереглянуті тексти . 2.2.3 Дослідження та перевірка методу Bag-of-words Bag of Words був одним із ранніх практичних методів , який дозволяв подати текст у вигляді чис ел. Цей метод реалізує п ринцип BoW , який полягає в тому, що текст перетворюється на вектор тексту, де кожний елемент показує, скільки разів певне слово зустрічається в документі . Механізм методу такий : спочатку формується корпус або словник усіх унікальних слів із наявних текстів . Потім кожен текст подається у вигляді 55 вектор у, розмірність як ого дорівнює розміру словника, а значення кожної розмірності дорівнює частоті вживання слова в тексті . Реалізація створення словника усіх унікальних слів наведена у лістингу 2.10. Лістинг 2. 10 – Реалізація створення словника усіх унікальних слів (фрагмент з файлу BagOfWords.java ) Set<String> globalVocabulary = new LinkedHashSet<>(); for(Text text : allTexts){ String[] tokens = text.getText().toLowerCase().split(\" \\\\P{L}+\"); globalVocabulary.addAll(Arrays. asList(tokens)); } Реалізація створення вектору тексту представлена у лістингу 2.11. Лістинг 2. 11 – Реалізація створення вектору тексту (фрагмент з файлу BagOfWords.java ) for(Text text:allTexts){ String[] tokens = text.getText().toLowerCase().split(\" \\\\P{L}+\"); Map<String, Double> vector = new LinkedHashMap<>(); for (String word : globalVocabulary) { vector.put(word, 0.0); } for (String token : tokens) { vector.put(token, vector.get(token) + 1); } text.setVectorBOWIT(vector); } У даному коді для кожного тексту проходить токенізація, створення вектора признаків та підлік частоти слів, після чого отриманий результат зберегаеться у VectorBOWIT . Рейтинг текстів сформований з використанням Bag of Words та косинусної подібності можна побачити нижче, у таблиці 2. 15. 56 Таблиця 2.1 5 – Рейтинг текстів сформований з використанням Bag of Words та косинусної подібності . Порядковий номер у рейтингу Ступень подібності Категорія об’єкта 1 0.26165946831484665 Перша 2 0.24094262117709064 Друга -третя 3 0.23326851658421704 Перша -третя 4 0.18625362947141832 Третя 5 0.14259438353340448 Друга 6 0.13640689825271052 Перша -друга 7 0.11750982553894372 Третя 8 0.06658014343186634 Друга 9 0.06089070039196951 Друга 10 0.05822264572071275 Третя При використані Bag of Words з косинусною подібністю тільки два тексти непереглянутих об’єктів категорії «Перша» з трьох попали до перших п’яти місць рейтингу , зайнявши перше та третє місц е. Середня подібність текстів категорії «Перша» дорівнює 0.21044496105 у той час , як середня подібність усіх непереглянутих текстів лише 0.15043288324 . Тексти категорії «Перша» мають середню подібність на 40% більше ніж непереглянуті тексти , хоча у даному випадку це свідчіть про великий розрив між першими та останніми місцями . 2.2.4 Дослідження та перевірка методу TF-IDF Метод TF -IDF працює схожим з Bag of Words чином, але, додатково, знижує важливість часто вживаних серед усіх текстів слів, це викликано знизити важливість таких слів як «і», «та», «або» тощо. Таким чином TF-IDF приділяє більше уваги словам, що виділяють речення та несуть його сенс. 57 Міра важ ливості слова в TF-IDF визначається наступним чином частота термін у в документі (TF) множиться на зворотна частот у документа (IDF). Код, що підраховує IDF представлений у лістингу 2.12. Лістинг 2. 12 – Підрахунок IDF (фрагмент з файлу TfIdf.java ) Map<String, Double> idf = new HashMap<>(); for(Text text:allTexts){ Set<String> uniqueWords = new HashSet<>( Arrays.asList(text.getText().toLowerCase().split(\" \\\\s+\")) ); for (String word : uniqueWords) { idf.put(word, idf.getOrDefault(word, 0.0) + 1); } } for (String word : idf.keySet()) { double df = idf.get(word); idf.put(word, Math. log((1.0 + allTexts.size()) / (1.0 + df)) + 1.0); } Розрахунок TF та TF-IDF представлений нижче, у лістингу 2.13. Лістинг 2. 13 – Розрахунок TF та TF-IDF (фрагмент з файлу TfIdf.java ) for (Text text : allTexts) { String doc = text.getText(); String[] words = doc.toLowerCase().split(\" \\\\s+\"); Map<String, Integer> wordCount = new HashMap<>(); for (String word : words) { wordCount.put(word, wordCount.getOrDefault(word, 0) + 1); } Map<String, Double> tfidf = new HashMap<>(); for (Map.Entry<String, Integer> entry : wordCount.entrySet()) { String term = entry.getKey(); double tf = (double) entry.getValue() / words.length; tfidf.put(term, tf * idf.get(term)); } text.setVectorBOWIT(tfidf); } 58 Рейтинг текстів сформований з використанням TF -IDF та косинусної подібності можна побачити нижче, у таблиці 2.1 6. Таблиця 2.1 6 – Рейтинг текстів сформований з використанням TF-IDF та косинусної подібності . Порядковий номер у рейтингу Ступень подібності Категорія об’єкта 1 0.10174167303283582 Перша 2 0.0785689789606471 Перша -третя 3 0.07061774022905887 Друга -третя 4 0.0645699560811784 Третя 5 0.04855037015690574 Перша -друга 6 0.03172649726679427 Друга 7 0.029437279870351878 Третя 8 0.02140441716424031 Друга 9 0.02008406200310641 Третя 10 0.0188169037405954 Друга При використані TF -IDF з косинусною подібністю, усі три тексти непереглянутих об’єктів категорії «Перша» попали до перших п’яти місць створеного рейтингу , зайнявши перше, друге та п’яте місц е. Середня подібність текстів категорії «Перша» дорівнює 0.07628700738 , у той час як середня подібність усіх непереглянутих текстів лише 0.04855178785 . Непереглянуті т ексти категорії «Перша» мають середню подібність на 57% більше ніж усі непереглянуті тексти , що визвано великим розривом серед перших та останніх місць рейтингу . 59 2.2.5 Порівняння методів для перетворення текстових атрибутів об’єктів на вектори Проведене дослідження продемонструвало, що лише BERT зміг повністю справитися з завданнями. TF-IDF зміг видати задовільний результат, у той час як Bag"}
{"chunk_id": "2025_M_IUS_Trebunskyh_OV.pdf:15", "source": "2025_M_IUS_Trebunskyh_OV.pdf", "text": "е. Середня подібність текстів категорії «Перша» дорівнює 0.07628700738 , у той час як середня подібність усіх непереглянутих текстів лише 0.04855178785 . Непереглянуті т ексти категорії «Перша» мають середню подібність на 57% більше ніж усі непереглянуті тексти , що визвано великим розривом серед перших та останніх місць рейтингу . 59 2.2.5 Порівняння методів для перетворення текстових атрибутів об’єктів на вектори Проведене дослідження продемонструвало, що лише BERT зміг повністю справитися з завданнями. TF-IDF зміг видати задовільний результат, у той час як Bag of Words видав найгірший результат серед усіх методів. Порівняльні характеристики методів, які перевірялися, наведені у таблиці 2.17. Таблиця 2.1 7 – Порівняльні характеристики методів для перетворення текстових атрибутів об’єктів на вектори Метод Критерій BERT - подібн а модел ь TF-IDF Bag of Words Позиції об'єктів, які є однокатегоріальними із переглянутими об'єктами, у рейтингу 1-3 позиції 1,2 та 5 позиції 1,3 та 6 позиції Дискримінативна здатність 13% 57% 40% Кількість переваг 1 1 0 Дані таблиці показують, що безсумнівним кращім рішенням щодо вибору методу ембедингу буде метод BERT . Хоча інші методи мають більшу дискримінативну здатність, позиції текстів непереглянутих об’єктів категорії «Перша» в рейтингу свідч ать, тільки, про великий розрив серед перших та останніх місць рейтингу. Отже, на основі отриманих результатів можна зробити висновок , що використання BERT -подібної моделі є кращим способом перетворення текстових атрибутів об’єктів на вектори для побудови рекомендацій клієнтам в ІС агентства нерухомості . 60 3 ІНФОРМАЦІЙНА ТЕХНОЛОГІЯ ПОБУДОВИ РЕКОМЕНДАЦІЙ КЛІЄНТАМ В ІНФОРМАЦІЙНІЙ СИСТЕМІ АГЕНТСТВА НЕРУХОМОСТІ 3.1 Опис інформаційної технології побудови рекомендацій клієнтам в інформаційній системі агентства нерухомості В рамках роботи була розроблена ІТ побудови рекомендацій клієнтам в ІС агентства нерухомості. Для опису ІТ розроблено модель типу IDEF0 , яка представлена на рисунку 3.1. На цій діаграмі відображен і всі підпроцеси автоматизованого процесу побудови рекомендацій клієнтам в ІС агентства нерухомості та їх вхідні, вихідні дані, механізми та управління. При формуванні рекомендацій збираються запити користувача за допомогою модуля пошуку та перегляду житла та на основі них з використанням модуля рекомендацій, бази даних та реалізації BERT -подібної моделі формується електронний лист з рекомендаціями для к лієнта. Цей процес відбувається відповідно до закону України «Про захист персональних даних», політики конфіденційності ІС та content -based підходу. На даній діаграмі , яка описує ІТ, представлено сім основних етапів. Перший етап – це збирання інформації про дії користувача . Під час виконання цього етапу фіксуються об’єкти житла, які переглядав користувач та фільтри, які він задавав у пошуку. Вхідними даними етапу є запити користувача до системи та інформація про користувача. Керуючими елементами – політика конфіденційності та Закон України про захист персональних даних, які регламентують правила збору, обробки та зберігання користувацьких даних. До механізмів етапу відносяться модуль пошуку та перегляду житла та база даних ІС. Вихідними даними етапу є інформація про дії користувача (перегляди об’єктів житла та фільтри, що задавав користувач при пошуку житла) . 61 Рисунок 3.1 – Модель IDEF0 інформаційної технології побудови рекомендацій клієнтам в ІС агентства нерухомості 62 Другий етап – це формування профілю користувача . На цьому етапі на основі зібраної інформації формується профіль користувача, що містить інформацію про узагальнені атрибути житла, яке він переглядав та фільтри, які він задавав . Вхідними даними етапу є інформація про дії користувача та інформація про об'єкти житла . Керуючим елемент ом є content -based підхід до формування рекомендацій . До механізмів етапу відносяться база даних, модуль рекомендацій, модель та бібліотека для роботи з BERT . Вихідними даними етапу є сформований профіль користувача . Третій етап – це формування списку непереглянутих об'єктів житла . Під час виконання цього етапу відбираються об’єкти на сторінки яких не заходив користувач та з них формується список . Вхідними даними етапу є профіль користувача . Керуючим елементам content -based підхід . До механізмів етапу відносяться база даних, модуль рекомендацій, модель та бібліотека для роботи з BERT . Вихідними даними етапу є cписок непереглянутих об'єктів житла . Четвертий етап – оцінювання об'єктів на схожість з узагальненими характеристиками об'єктів житла з профілю користувача . Під час виконання цього етапу з використанням методів косинусної подібності та BERT -подібної моделі формується рейтинг житла за подібністю до узагальнених атрибутів житла з профілю користувача . Вхідними даними етапу є список непереглянутих об'єктів житла . Керуючим елемент ом content -based підхід . Механізм ом етапу є модуль рекомендацій . Вихідними даними етапу є список оцінених об'єктів житла . П’ятий етап – це коректування оцінки з урахуванням фільтрів користувача . На цьому етапі оцінки об’єктів житла знижуються у разі, якщо вони суперечать фільтрам, які задавав користувач, ступень зменшення залежить від ступеня суперечності фільтрам . Вхідними даними етапу є список оцінених об'єктів житла . Керуючим елемент ом content -based підхід . До механізмів етапу відносяться модуль рекомендацій . Вихідними даними етапу є скоректований список оцінених об'єктів житла . Шостий етап – це відбір найбільш підходящих об'єктів житла . Під час 63 його виконання обираються п’ять об’єктів з кращими оцінками подібності та на їх основі формується електронний лист з рекомендованим"}
{"chunk_id": "2025_M_IUS_Trebunskyh_OV.pdf:16", "source": "2025_M_IUS_Trebunskyh_OV.pdf", "text": "разі, якщо вони суперечать фільтрам, які задавав користувач, ступень зменшення залежить від ступеня суперечності фільтрам . Вхідними даними етапу є список оцінених об'єктів житла . Керуючим елемент ом content -based підхід . До механізмів етапу відносяться модуль рекомендацій . Вихідними даними етапу є скоректований список оцінених об'єктів житла . Шостий етап – це відбір найбільш підходящих об'єктів житла . Під час 63 його виконання обираються п’ять об’єктів з кращими оцінками подібності та на їх основі формується електронний лист з рекомендованим житлом . Вхідними даними етапу є скоректований список оцінених об'єктів житла . Керуючим елемент ом content -based підхід . До механізмів етапу відносяться модуль рекомендацій . Вихідними даними етапу є список рекомендованого житла Сьомий етап – це відправляння електронного листа з рекомендованим житлом . На цьому етапі формується електронний лист з інформацією та посиланням на п’ять найбільш відповідних об’єктів житла та надсилається на пошту користувачу . Для надсилання електронного листа клієнту має використовуватися поштовий сервіс. Вхідними даними етапу є список рекомендованого житла . Керуючим елемент ом content -based підхід . До механізмів етапу відносяться модуль рекомендацій та поштовий сервіс . Вихідними даними етапу є електронни й лист з рекомендованим житлом . 3.2 Опис впровадження інформаційної технології побудови рекомендацій клієнтам в інформаційній системі агентства нерухомості Інформаційна технологія буде впроваджена через реалізацію модуля рекомендацій у ІС . Для розробки модуля було обране середовище розробки IntelliJ IDEA. До плюсів IntelliJ IDEA можна віднести: а) IntelliJ IDEA підтримує безліч мов і фреймворків, але головне він підтимує мовуJava, на якій буде написаний модуль; б) IntelliJ IDEA має відмінні інструменти рефакторингу , які дозволяють безпечно змінювати структуру коду без ризику зламати проект. Наприклад, при перейменуванні методу або класу IDE автоматично оновлює всі пов'язані місця; в) IntelliJ IDEA має автодоповнювач коду, який з контексту, пропонує 64 методи, змінні та типи даних; г) IntelliJ IDEA дозволяє швидко переходити до класів, методів, використання змінних, реалізації інтерфейсів. Для роботи з базою даних був обраний MySQL Workbench через такі переваги: а) MySQL Workbench це офіційний інструмент від Oracle, який має повну сумісність з MySQL і отримує регулярні оновлення; б) MySQL Workbench має зручний та звичний візуальний інтерфейс, який дозволяє роботати з базами даних, таблицями та зв'язками без необхідності писати усе вручну; в) MySQL Workbench є безкоштовним , і всі його функцій доступні без придбання ліцензії . Модуль побудови рекомендацій написаний на Java . Головною причиною такого рішення є те, що програмне забезпечення ІС, для якої ро зробляє ться модуль , написаний на Java, але окрім цього Java є надійним та безпечним варіантом з великою спільнотою користувачів, високою продуктивністю, зручний для створення масштабованих та підтримуваних проектів та має гарне середовище розробки у виді IntelliJ IDEA. Як BERT -модель має використовуватися реалізація нейронної моделі sentence -transformers/paraphrase -multilingual -MiniLM -L12-v2. Це багатомовна модель для отримання векторних представлень речень. Вона навчена так, щоб речення зі схожим змістом мали близькі вектори, навіть якщо вони написані різними мовами, що є поїзною рисою у специфіці українського ринку оренди . Для використання цієї моделі має бути застосоване API, тому використовується Deep Java Library . Deep Java Library – це, відкрита бібліотека для роботи зі штучним інтелектом і машинним навчанням на Java. Вона надає зручний API для завантаження, навчання та використання моделей машинного навчання, підтримує кілька популярних AI-двигунів і дозволяє вбудовувати інтелектуальні функції безпосередньо в Java-додатки. 65 4 ПРОГРАМНА Р ЕАЛІЗАЦІЯ МОДУЛ Я ПОБУДОВИ РЕКОМЕНДАЦІЙ ТА ЕКСПЕРИМЕНТАЛЬНА ПЕРЕВІРКА НАУКОВИХ РЕЗУЛЬТАТІВ 4.1 Реалізація модул я побудови рекомендацій в інформаційній системі агентства нерухомості Розроблена ІТ має бути реалізована в ІС агенства нерухомості за допомогою модул я побудови рекомендацій . Модуль має монолітну архітектуру з архітектурним шаблоном Model - View -Controller . Монолітна архітектура – це класичний метод побудови програмного забезпечення, коли всі складові системи: користувацький інтерфейс, бізнес -логіка та логіка обробки даних, об'єднані в один програмний код і розгортаються разом. Така структура працює як єдин е ціле, де всі частини використовують спільні ресурси, пам'ять і обчислювальну потужність. Така архітектура, зазвичай, реалізується через багаторівневу модель, у якій рівень користувацького інтерфейсу, рівень бізнес -логіки та рівень доступу до даних структуровані горизонтально із визначеною ієрархією залежностей. Робота всіх модулів у спільному контексті виконання забезпечує зручність тестування функціоналу. Монолітні системи зазвичай будуються за відомими корпоративними шаблонами, такими як Model -View -Controller MVC або багаторівневі архітектури, які розділяють функції, але залишаються простими у впровадженні. Зміна будь -якої частини моноліту означає, що треб а оновлювати весь додаток цілком — це забезпечує узгодженість, але ускладнює часті оновлення. Монолітний підхід полегшує початок розробки, бо компоненти не потребують складної взаємодії між собою, а транзакціями легше керувати. Тому він добре працює для до датків з чіткими і стабільними вимогами [ 22, 23]. Модуль використовує бібліотеку DJL для роботи з реалізованою 66 моделлю обробки природної мови. Для опису функціонального складу модуля була розроблена функціональна модель у вигляд і діаграм потоків даних (DFD ). Контекстна діаграма потоків даних модуля наведен о на рисунку 4.1. Рисунок 4.1 – Контекстна DFD модуля побудови рекомендацій Як можна"}
{"chunk_id": "2025_M_IUS_Trebunskyh_OV.pdf:17", "source": "2025_M_IUS_Trebunskyh_OV.pdf", "text": "полегшує початок розробки, бо компоненти не потребують складної взаємодії між собою, а транзакціями легше керувати. Тому він добре працює для до датків з чіткими і стабільними вимогами [ 22, 23]. Модуль використовує бібліотеку DJL для роботи з реалізованою 66 моделлю обробки природної мови. Для опису функціонального складу модуля була розроблена функціональна модель у вигляд і діаграм потоків даних (DFD ). Контекстна діаграма потоків даних модуля наведен о на рисунку 4.1. Рисунок 4.1 – Контекстна DFD модуля побудови рекомендацій Як можна побачити з контекстної діаграми, модуль побудови рекомендацій має зовнішні зв’язки тільки з сервісом електронної пошти, тому що він викликається автоматично з періодичністю вказаною у таблиці 1. 1. А генеровані персоналізовані рекомендації модуль розміщує у складі сформованого електронного листа відповідному клієнту. Всі необхідні дані для відправки електронного листа передаються сервісу електронної пошти. Таким чином клієнт взагалі не взаємодіє з м одулем безпосередньо, а рекомендації щодо житла отр имує їх від сервісу електронної пошти у вигляду електронного листа . Схема функціональної структури модуля наведена на рис унку 4.2 у вигляді DFD першого рівня декомпозиції. 67 Рисунок 4.2 – Діаграма декомпозиції першого рівня контекстної DFD модуля побудови рекомендацій 68 Ця діаграма включає шість основних функцій модуля побудови рекомендацій : а) функція «Формування списку користувачів для формування рекомендацій » визначає , якім користувачам треба відправити листи з рекомендованим житлом та ініціюється процес створення рекомендацій для них; б) функція «Формування профіля користувача» формує профіль користувача на основі переглянутого житла та заданих фільтрів; в) функція «Розрахунок міри подібності житла до профілю користувача » розрах овує ступінь відповідн ості непереглянут ого об’єкт а житла профілю користувача з використанням методів косинусної подібності та BERT - подібної моделі ; г) функція «Коригування міри подібності з урахуванням фільтрів » розраховує ступінь відповідн ості непереглянут ого об’єкт а житла фільтрам. Отримана оцінка відповідності застосовується для зниження оцінки відповідності профілю в залежності від рівня порушення фільтрів користувача; ґ) функція «Формування списку релевантного житла » реалізує формування рейтингу непереглянутих об’єктів та відбір п’яті перших об’єктів у рейтингу ; д) функція «Формування та надсилання електронного листа з рекомендованим житлом користувачу » формує дані електронн ого лист а та його вміст у вигляд і html-коду для відображення короткої інформації про рекомендован і об’єкти житл а. Крім того html-код реалізує гіперпосилання на сторінки ІС для більш детального перегляду інформації про об’єкт . Дані електронного листа та вміст у вигляді html-код передаються на вхід поштового сервісу. Модуль побудови рекомендацій , пов’язаний з іншими частинами інформаційної системи через базу даних . Так він бере інформацію від модуля облікових записів, а модуль пошуку та перегляду житла фіксує дії 69 користувачів для модуля рекомендацій. Діаграма класів реалізованого модуля рекомендацій ІС агентства нерухомості представлена нижче на рисунку 4. 3. На даній діаграмі присутні такі класи, що відносяться до модуля рекомендацій ІС агентства нерухомості або пов’язані з ним: а) Mailing – основний клас модуля, який ініціює основні його процеси від відбору користувачів, яким треба надіслати листи, до надсилання цих листів; б) User – містить інформацію про зареєстрованого користувача: його особисті дані, останню, активність, розміщені об'єкти, історію переглядів та інше; в) Property – містить інформацію про об'єкт нерухомості з повним набором характеристик: опис, орендна плата, розташування, кількість кімнат і ліжок, площа, поверх та інше. Кожен об'єкт пов'язаний з власником, списком користувачів які його переглядали, обла днанням та іншим; г) Filter – містить інформацію про фільтри, які задавав користувач, пов’язаний з користувачем та обладнанням; ґ) Profile – агрегує інформацію про вподобання користувача на основі переглянутих об’єктів житла та заданих фільтрів; д) Equipment – містить інформацію про обладнання та додаткові параметри, пов’язаний з об’єктами житла та фільтрами; е) BertTextToVector – перетворює текстові описи в числові вектори за допомогою BERT; є) CosineSimilarity – на основі непереглянутого житла та профілю користувача видає список найрелевантнішого житла для користувача з використанням методів косинусної подібності та BERT -подібної моделі ; 70 Рисунок 4. 3 – Діаграма класів реалізованого модул я 71 ж) UserService – відповідає за роботу з користувачами. Представлен і метод и дозволя ють модулю вибирати користувачів за датою їх останньої активності, що використовується для відправки листів відповідно до дати їх останньої активності та записувати, яке житло переглядав користувач ; з) PropertyService – управляє об'єктами нерухомості, надає модулю метод для отримання списку непереглянутих користувачем об'єктів , метод для видалення інформації про перегляди та минулі рекомендації старше п’яти місяців та метод для створення зв’язку між користувачем та об’єктом, який йому вже рекомендували ; и) EmailService – сервіс для відправи електронних листів. Використовується для надсилання листів з рекомендованим житлом ; і) ViewedProperty – містить інформацію про перегляд и об’єкт ів житла зареєстрованими користувач ами та те що житло вже було рекомендовано користувачу . 4.2 Експериментальна перевірка наукових результатів Для перевірки коректності запропонованої ІТ було проведено експериментальне тестування в умовах, наближених до реальної експлуатації системи. До ІС агентства нерухомості бул и внесен і п'ятнадцять тестових об'єктів житла, які представлені на рисунку 4. 4. Після чого, було змодельовано поведінку користувача системи, яка включала"}
{"chunk_id": "2025_M_IUS_Trebunskyh_OV.pdf:18", "source": "2025_M_IUS_Trebunskyh_OV.pdf", "text": "листів. Використовується для надсилання листів з рекомендованим житлом ; і) ViewedProperty – містить інформацію про перегляд и об’єкт ів житла зареєстрованими користувач ами та те що житло вже було рекомендовано користувачу . 4.2 Експериментальна перевірка наукових результатів Для перевірки коректності запропонованої ІТ було проведено експериментальне тестування в умовах, наближених до реальної експлуатації системи. До ІС агентства нерухомості бул и внесен і п'ятнадцять тестових об'єктів житла, які представлені на рисунку 4. 4. Після чого, було змодельовано поведінку користувача системи, яка включала такі дії: а) перегляд двох об'єктів житла , інформацію про які можна побачити на рисунку 4.5; б) застосування двох фільтрів для пошуку об'єктів житла, значення полів яких , можна побачити на рисунку 4. 6. 72 Рисунок 4. 4 – Інформація про о б’єкти житла , які були використані для експерименту Рисунок 4. 5 – Номери відвіданих об’єктів житла Рисунок 4.6 – Значення полів ф ільтр ів, задан их при пошуку 73 Виходячи з цих даних, метод буде працювати вірно , якщо сформує рекомендаційний лист з об’єктами 153, 155, 156 та 157, бо вони є найбільш схожими на переглянуті об’єкти та більш відповідають фільтрам ніж інші непереглянуті об’єкти житла. Після додавання даних бу в запущений механізм формування рекомендаційного листа та після його отримання можна буде побачити , чи попали необхідні об’єкти житла до списку рекомендованих об’єктів . Cформований модулем html -код змісту рекомендаційного листа можна побачити у лістингу А.1 додатку А. При формуванні коду використовувалися дані об’єктів житла , відібраних модул ем для рекомендаці ї, та ім’я клієнта , якому надсилається електронний лист . Відображення цього коду можна побачити на с кріншот і рекомендаційного листа , представлен ого на аркушах рисунк у 4.7. Рисунок 4. 7 – Скріншот рекомендаційного листа 74 Рисунок 4.7, аркуш 2 Аналіз отриманих результатів свідчить про повну відповідність фактичного виводу системи очікуваним результатам. Усі чотири прогнозовані об'єкти (153, 155, 156, 157) зайняли перші чотири позиції у рекомендаційному листу , що підтверджує коректність функціонування розробленого модуля рекомендацій. Результати експериментальної перевірки демонструють належну роботу модуля рекомендацій у складі ІС агентства нерухомості. Модуль побудови рекомендаций успішно ідентифікує найбільш релевантні об'єкти житла на основі аналізу переглянутих користувачем об’єктів житла та параметрів , заданих ним , фільтрів, що підтверджує ефективність обраних методів та розробленої інформаційної технології . 75 ВИСНОВКИ У межах кваліфікаційної роботи було проведено дослідження методів і моделей, придатних для використання під час побудови рекомендацій клієнтам в ІС агентства нерухомості. У роботі сформульовано проблему побудови рекомендацій клієнтам щодо вибору нерухомості, виконано аналіз існуючих методів та моделей, які необхідні для побудови рекомендацій. Під час виконання роботи було проведене експериментальне дослідження трьох методів розрахунку ступеня подібності об'єктів та двох методів і моделі перетворення текстових атрибутів об’єктів на вектори з метою визначення кращих для використання при побудові рекомендацій щодо житла у ІС агентства нерухомості. Розроблено системи критеріїв вибору методів розрахунку ступеня подібності об’єктів та методів для перетворення текстових атрибутів об’єктів на вектори. Найкращі результати показа в метод косинусної подібності та BERT - подібна модель sentence -transformers/paraphrase -multilingual -MiniLM -L12-v2. З використанням цих методів була розроблена ІТ побудови рекомендацій клієнтам в ІС агентства нерухомості та виконана програмна реалізація ІТ у вигляді модуля побудови рекомендацій. Також п роведена експериментальна перевірка отриманих наукових результатів, яка підтвердила ефективність обраних методів та розробленої ІТ. Результати проведеного дослідження можна використовувати при реалізації рекомендаційних функцій в ІС, які використовуються у сфері нерухомості. Доцільним вважаю продовження досліджень з метою визначення ефективних методів для інформаційних технологій формування персоналізованих рекомендацій, які мають бути реалізовані безпосередньо на сторінках ІС , без обмеження лише форматом рекомендаційних лис тів. 76 Результати роботи були опубліковані на п’ятій міжнародній науково - теоретичній конференції «Current scientific goals, approaches and challenges» у форматі тез доповіді на тему «Дослідження методів побудови рекомендацій клієнтам в інформаційній системі агентства нерухомості» [24]. 77 ПЕРЕЛІК ДЖЕРЕЛ ПОСИЛАННЯ 1. Петров К.Е., Левикін В.М., Чалий С.Ф., Євланов М.В., Саєнко В.І., Міхнов Д.К., Міхнова А.В., Чала О.В., Методичні вказівки щодо розробки та оформлення кваліфікаційної роботи (для студентів усіх форм навчання другого (магістерського) рівня вищої освіти с пеціальності 122 Комп’ютерні науки освітньо -професійної програми «Інформаційні управляючі системи та технології») . Харків: ХНУРЕ, 2021. 24 с. 2. ДСТУ 3008:2015. Інформація та документація. Звіти у сфері науки і техніки. Структура та правила оформлювання . Чинний від 22.06.2015. Київ: ДП «УкрНДНЦ», 2016. 26 с. 3. ДСТУ 8302:2015. Інформація та документація. Бібліографічне посилання. Загальні положення та правила складання. Чинний від 2016 -07-01. Вид. офіц. Київ : УкрНДНЦ, 2016. 16 с. 4. Kim J., Choi I., Li Q. Customer Satisfaction of Recommender System: Examining Accuracy and Diversity in Several Types of Recommendation Approaches. Sustainability. 2021. Т. 13, № 11. С. 6165. URL: https://doi.org/10.3390/su13116165 (дата звернення: 10.12.2025) . 5. Deep Learning Based Recommender System / S. Zhang та ін. ACM Computing Surveys. 2019. Т. 52, № 1. С. 1 –38. URL: https://doi.org/10.1145/3285029 (дата звернення: 01.11.2025) . 6. Дослідження методів побудови рекомендаційних систем для розв’язання задачі вибору найбільш релевантного відео при створенні віртуальних арт -композицій / A."}
{"chunk_id": "2025_M_IUS_Trebunskyh_OV.pdf:19", "source": "2025_M_IUS_Trebunskyh_OV.pdf", "text": "I., Li Q. Customer Satisfaction of Recommender System: Examining Accuracy and Diversity in Several Types of Recommendation Approaches. Sustainability. 2021. Т. 13, № 11. С. 6165. URL: https://doi.org/10.3390/su13116165 (дата звернення: 10.12.2025) . 5. Deep Learning Based Recommender System / S. Zhang та ін. ACM Computing Surveys. 2019. Т. 52, № 1. С. 1 –38. URL: https://doi.org/10.1145/3285029 (дата звернення: 01.11.2025) . 6. Дослідження методів побудови рекомендаційних систем для розв’язання задачі вибору найбільш релевантного відео при створенні віртуальних арт -композицій / A. Kuliahin та ін. Системи управління, навігації та зв’язку. Збірник наукових праць : матеріали Міжнар. наук. конф., м. м. Полтава, 29 листоп. 2022 р. м. Полтава, 2022. С. 94 –99. URL : https ://doi.org/10.26906/ sunz.2022.4.094 (дата звернення: 13.11.2025). . 7. Ryngksai I., Chameikho L. Recommender Systems: Types of Filtering Techniques. International Journal of Engineering Research & Technology (IJERT). 78 2014. Т. 3, № 11. С. 251 –254. URL: https://www.ijert.org/research/recommender - systems -types -of-filtering -techniques -IJERTV3IS110197.pdf (дата звернення: 11.12.2025) . 8. Çano E., Morisio M. Hybrid recommender systems: A systematic literature review. Intelligent Data Analysis. 2017. Т. 21, № 6. С. 1487 –1524. URL: https://doi.org/10.3233/ida -163209 (дата звернення: 01.11.2025) . 9. Gómez J., Vázquez P. -P. An Empirical Evaluation of Document Embeddings and Similarity Metrics for Scientific Articles. Applied Sciences. 2022. Т. 12, № 11. С. 5664. URL: https://doi.org/10.3390/app12115664 (дата звернення: 12.11.2025) . 10. Levy A., Shalom B. R., Chalamish M. A guide to similarity measures and their data science applications. Journal of Big Data. 2025. Т. 12, № 1. URL: https://doi.org/10.1186/s40537 -025-01227 -1 (дата звернення: 12.11.2025) . 11. Çolakoğlu H. B. On the distance formulae in the generalized taxicab geometry. Turkish journal of mathematics. 2019. Т. 43, № 3. С. 1578 –1594. URL: https://doi.org/10.3906/mat -1809 -78 (дата звернення: 12.11.2025) . 12. Etherington T. R. Mahalanobis distances for ecological niche modelling and outlier detection: implications of sample size, error, and bias for selecting and parameterising a multivariate location and scatter method. PeerJ. 2021. Т. 9. С. e11436. URL: https ://doi.org/10.7717/peerj.11436 (дата звернення: 11.12.2025) . 13. Aizawa A. An information -theoretic perspective of tf –idf measures. Information Processing & Management. 2003. Т. 39, № 1. С. 45 –65. URL: https://doi.org/10.1016/s0306 -4573(02)00021 -3 (дата звернення: 11.12.2025) . 14. Devlin J., Chang M. -W., Lee K., Toutanova K. BERT: Pre -training of Deep Bidirectional Transformers for Language Understanding. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) : тези доп., м. Міннеаполіс, черв. 2019 р. Minneapolis : Association for Computational Linguistics, 2019. С. 4171 –4186. URL: https://aclanthology.org/N19 -1423/ (дата звернення: 2 9.11.2025) . 79 15. Tian M., Ekstrand M. D. Estimating Error and Bias in Offline Evaluation Results. CHIIR '20: Proceedings of the 2020 Conference on Human Information Interaction and Retrieval, м. Vancouver, 14 берез. 2020 р. New York, 2020. С. 392 – 396. URL: https://dl.acm.o rg/doi/10.1145/3343413.3378004 (дата звернення: 24.11.2025) . 16. Comparative Study of Filtering Methods for Scientific Research Article Recommendations / D. El Alaoui та ін. Big Data and Cognitive Computing. 2024. Т. 8, № 12. С. 190. URL: https://doi.org/10.3390/bdcc8120190 (дата звернення: 11.12.2025) . 17. Gebremeskel G. G., de Vries A. P. Recommender Systems Evaluations: Offline, Online, Time and A/A Test. Conference and Labs of the Evaluation Forum : матеріали Міжнар. наук. конф., м. Évora, 5 верес. 2016 р. Aachen, 2016. С. 642–656. URL: https://ceur -ws.or g/Vol -1609/16090642.pdf (дата звернення: 14.11.2025) . 18. A/B testing: A systematic literature review / F. Quin та ін. Journal of Systems and Software. 2024. С. 112011. URL: https://doi.org/10.1016/j.jss.2024.112011 (дата звернення: 11.12.2025) . 19. Sohangir S., Wang D. Improved sqrt -cosine similarity measurement. Journal of Big Data. 2017. Т. 4, № 1. URL: https://doi.org/10.1186/s40537 -017- 0083 -6 (дата звернення: 11.12.2025) . 20. Mussabayev R. Optimizing Euclidean Distance Computation. Mathematics. 2024. Т. 12, № 23. С. 3787. URL: https://doi.org/10.3390/math12233787 (дата звернення: 11.12.2025) . 21. A modified and weighted Gower distance -based clustering analysis for mixed type data: a simulation and empirical analyses / P. Liu та ін. BMC Medical Research Methodology. 2024. Т. 24, № 1. URL: https://doi.org/10.1186/s12874 - 024-02427 -8 (дата звернення: 11.12.2025) . 22. Mooghala S. A Comprehensive Study of the Transition from Monolithic to Micro services -Based Software Architectures. Journal of Technology and Systems. 2023. Т. 5, № 2. С. 27 –40. URL: https://doi.org/10.47941/jts.1538 (дата 80 звернення: 09.12.2025) . 23. Kambhammettu A. P. Monolithic versus Microservice Architectures: A Comparative Analysis for Enterprise Applications. European Journal of Computer Science and Information Technology. 2025. Т. 13, № 51. С. 65 –75. URL : https ://doi.org/10.37745/ ejcsit .2013/ vol13n516575 (дата звернення: 11.12.2025) . 24. Требунських О., Борисенко Т. Дослідження методів побудови рекомендацій клієнтам в інформаційній системі агентства нерухомості. V International Scientific and Theoretical Conference «Currentscientific goals, approaches and challenges» : матеріали Міжнар. на ук. конф., м. Dresden, 12 груд. 2025 р. Dresden, 2025. Kambhammettu A. P. Monolithic versus Microservice Architectures: A Comparative Analysis for Enterprise Applications. European Journal of Computer Science and Information Technology. 2025. Т. 13, № 51. С. 65 –75. URL : https ://doi.org/10.37745/ ejcsit .2013/ vol13n516575 (дата звернення: 11.12.2025) . 24. Требунських О., Борисенко Т. Дослідження методів побудови рекомендацій клієнтам в інформаційній системі агентства нерухомості. V International Scientific and Theoretical Conference «Currentscientific goals, approaches and challenges» : матеріали Міжнар. на ук. конф., м. Dresden, 12 груд. 2025 р. Dresden, 2025. С. 296 –299. URL: https://previous.scientia.report/index.php/archive/issue/view/12.12.2025 (дата звернення: 12.12.2025) ."}
{"chunk_id": "2025_M_IUS_Yemelyanov_AV.pdf:0", "source": "2025_M_IUS_Yemelyanov_AV.pdf", "text": "Міністерство освіти і науки України Харківський національний університет радіоелектроніки Факультет Комп’ютерних наук (повна назва) Кафедра Інформаційних управляючих систем (повна назва) КВАЛІФІКАЦІЙНА РОБОТА Пояснювальна записка рівень вищої освіти другий (магістерський) Дослідження методів формування рекомендацій вакансій для працівників фріланс -платформи (тема) Виконав: здобувач 2 року навчання, групи ІУСТм –24–1 Артем ЄМЕЛЬЯНОВ (Власне ім’я, ПРІЗВИЩЕ) Спеціальність 122 К омп’ютерні науки (код і повна назва спеціальності) Тип програми освітньо –професійна (освітньо –професійна або освітньо –наукова) Освітня програма Інформаційні управляючі системи та технології ( повна назва освітньої програми) Керівник зав. каф. ІУС Костянтин ПЕТРОВ (посада, власне ім’я, ПРІЗВИЩЕ) Допускається до захисту Зав. кафедри ІУС Костянтин ПЕТРОВ (підпис) (власне ім’я, ПРІЗВИЩЕ) 2025 р. Харківський національний університет радіоелектроніки ЗАТВЕРДЖУЮ: Зав. кафедри (підпис) “ 24 ” листопада 20 25 р. ЗАВДАННЯ НА КВАЛІФІКАЦІЙНУ РОБОТУ здобувачеві Ємельянову Артему Віталійовичу (прізвище, ім’я, по батькові) 1. Тема роботи Дослідження методів формування рекомендацій вакансій для працівників фріланс -платформи затверджена наказом університету від «24» __листопада __20 25_ р. № _ 1055Ст _ 2. Термін подання студентом роботи до екзаменаційної комісії «19» грудня 2025 р. 3. Вихідні дані до роботи наукова література з тематики кваліфікаційної роботи, матеріали передатестаційної практики, матеріали про методи рекомендації, джерела інформації про рекомендаційні системи 4. Перелік питань, що потрібно опрацювати в роботі аналіз предметної області та опис рекомендаційних систем; аналіз існуючих інформаційних систем і методів надання рекомендацій щодо пошуку вакансій та фріланс -проєктів, їх переваг та недоліків; аналіз науково -технічної літератури у сфері побудови персоналіз ованих рекомендаційних систем; постановка задачі дослідження; дослідження методів формування рекомендацій вакансій; розробка гібридного методу побудови рекомендацій у інформаційній системі фріланс - платформи; практична реалізація гібридного методу; експериментальна оцінка гібридного методу. Факультет Комп’ютерних наук Кафедра Інформаційних управляючих систем Рівень вищої освіти другий (магістерський) Спеціальність 122 К омп’ютерні науки (код і повна назва) Тип програми освітньо -професійна (освітньо -професійна або освітньо -наукова) Освітня програма Інформаційні управляючі системи та технології (повна назва) КАЛЕНДАРНИЙ ПЛАН № Назва eтапів роботи Терміни виконання етапів роботи Примітка 1 Аналіз предметної області 24.11.2025 –25.11.2025 Виконано 2 Опис існуючих методів формування рекомендацій вакансій 26.11.2025 –27.11.2025 Виконано 3 Постановка задачі дослідження 28.11.2025 –29.11.2025 Виконано 4 Дослідження існуючих методів для формування рекомендацій 30.11.2025 –02.12.2025 Виконано 5 Розробка гібридного методу побудови рекомендацій вакансій 03.12.2025 –04.12.2025 Виконано 6 Практична реалізація гібридного методу 05.12.2025 –07.12.2025 Виконано 7 Експериментальна оцінка гібридного методу 08.12.2025 –09.12.2025 Виконано 8 Оформлення пояснювальної записки 10.12.2025 Виконано 9 Оформлення графічних матеріалів 11.12.2025 Виконано 10 Захист кваліфікаційної роботи 19.12.2025 Виконано Дата видачі завдання «24»_листопада ___20 25 р. Здобувач ___________________________________ (підпис) Kepiвник роботи ________________________ зав. каф. ІУС Костянтин ПЕТРОВ (підпис) (посада, власне ім’я, ПРІЗВИЩЕ) 4 РЕФЕРАТ Пояснювальна записка кваліфікаційної роботи: 113 с., 7 рис., 2 табл., 2 дод., 30 джерел. BERT, TF -IDF, ФРІЛАНС -ПЛАТФОРМА, РЕКОМЕНДАЦІЙНА СИСТЕМА, ГІБРИДН ИЙ МЕТОД, КОНТЕНТНО -ОРІЄНТОВАНИЙ ПІДХІД, КОЛАБОРАТИВНА ФІЛЬТРАЦІЯ, СЕМАНТИЧН А МОДЕЛ Ь. Об’єктом дослідження є процес формування персоналізованих рекомендацій вакансій. Предметом дослідження є методи формування рекомендацій вакансій. Метою дослідження є розробка гібридного методу формування рекомендацій вакансій на фріланс -платформах для підвищення релевантності та точності підбору вакансій. У ході роботи проаналізовано особливості ринку фріланс -послуг . Розгялнуті основи методи формування рекомендацій Виявлено основні проблеми рекомендаційних систем . Розроблено гібридний метод формування рекомендацій вакансій для фріланс -платформи . Створено програмний прототип рекомендаційної підсистеми із використанням Java, Python, Spring Boot, MySQL та зовнішнього сервісу семантичної обробки текстів. Експериментальна перевірка на основі стандартних метрик показала, що запропонований гібридний підхід забезпечує кращі показники якості порівняно з окремим застосуванням контентного, колаборативного та семантичного методів. 5 ABSTRACT Explanatory note of the qualification work: 113 p., 7 fig., 2 table, 2 appendix, 30 sources. BERT, TF-IDF, FREELANCE PLATFORM, RECOMMENDATION SYSTEM, HYBRID METHOD, CONTENT -ORIENTED APPROACH, COLLABORATIVE FILTERING, SEMANTIC MODEL . The object of the study is the process of forming personalized job recommendations. The subject of the study is the methods of forming job recommendations. The purpose of the work is to analyze modern approaches to building recommendation systems for the freelance services market, identify their limitations and develop a hybrid method for forming job recommendations. In the course of the work, the features of the freelance services market were analyzed. The basics of the recommendation formation method were discussed. The main problems of the recommendation system were identified. A hybrid method for generating job recommendations for a freelance platform has been developed. A software prototype of the recommendation subsystem has been created using Java, Python, Spring Boot, MySQL, and an external semantic text processing service. Experimental evaluation based on standard metrics has shown that the proposed hybrid approach pr ovides better quality indicators compared to the separate use of content, collaborative, and semantic methods. 6 ЗМІСТ Скорочення та умовні познаки ................................ ................................ .............. 8 Вступ ................................ ................................ ................................ ......................... 9 1 Аналіз предметної області та постановка задачі дослідження ...................... 10 1.1 Аналіз предметної області та опис рекомендаційних систем ................ 10 1.1.1 Стан та специфіка ринку фріланс -послуг в Україні та світі ............ 10 1.1.2 Сутність та функціональні особливості"}
{"chunk_id": "2025_M_IUS_Yemelyanov_AV.pdf:1", "source": "2025_M_IUS_Yemelyanov_AV.pdf", "text": "on standard metrics has shown that the proposed hybrid approach pr ovides better quality indicators compared to the separate use of content, collaborative, and semantic methods. 6 ЗМІСТ Скорочення та умовні познаки ................................ ................................ .............. 8 Вступ ................................ ................................ ................................ ......................... 9 1 Аналіз предметної області та постановка задачі дослідження ...................... 10 1.1 Аналіз предметної області та опис рекомендаційних систем ................ 10 1.1.1 Стан та специфіка ринку фріланс -послуг в Україні та світі ............ 10 1.1.2 Сутність та функціональні особливості рекомендаційних систем . 11 1.1.3 Аналіз існуючих методів надання рекомендацій, що використовуються в ІС ................................ ................................ ................. 13 1.2 Аналіз існуючих ІС та методів надання рекомендацій щодо пошуку вакансій, їх переваги та недоліки ................................ ................................ .... 14 1.2.1 Огляд функціональних можливостей провідних платформ пошуку роботи ................................ ................................ ............................... 14 1.2.2 Порівняльний аналіз існуючих рішень ................................ .............. 16 1.3 Аналіз існуючих методів та моделей для вирішення задач рекомендації вакансій ................................ ................................ ....................... 18 1.3.1 Метод контентної фільтрації на основі векторної моделі ............... 18 1.3.2 Метод колаборативної фільтрації на основі матричної факторизації ................................ ................................ ................................ ... 19 1.3.3. Моделі на основі семантичних векторів та нейронних мереж ....... 20 1.4 Постановка задачі дослідження ................................ ................................ . 21 2 Розробка гібридного методу формування рекомендацій вакансій для фріланс платформи ................................ ................................ ................................ 23 2.1 Дослідження контентно -орієнтованої моделі ................................ .......... 23 2.2 Дослідження методу колаборативної фільтрації ................................ ..... 25 2.3 Дослідження семантичних методів ................................ ........................... 27 2.4 Розробка гібридного методу для формування рекомендацій вакансій . 29 7 3 Інформаційна технологія формування рекомендацій вакансій на фріланс - платформі ................................ ................................ ................................ ............... 33 3.1 Контекстна діаграма процесу формування рекомендацій вакансій на фріланс -платформі ................................ ................................ ............................ 33 3.2 Опис діаграми декомпозиції ................................ ................................ ...... 35 4 Практична реалізація гібридного методу ................................ ........................ 38 4.1 Програмна реалізація методів рекомендаційної системи ....................... 38 4.1.1 Обгрунтування технологічних засобів ................................ .............. 38 4.1.2 Опис реалізації контентно -орієнтованої моделі ............................... 39 4.1.3 Опис реалізації колаборативної фільтрації ................................ ....... 40 4.1.4 Опис реалізації семантичної моделі та BERT ................................ ... 41 4.1.5 Опис реалізації гібридного методу ................................ .................... 42 4.1.5 Демонстрація працездатності розробленого гібридного методу .... 43 4.2 Експериментальна перевірка працездатності гібридного методу ......... 47 4.2.1 Опис використаних метрик оцінювання ................................ ............ 47 4.2.2 Результати експерименту та висновки ................................ ............... 50 Висновки ................................ ................................ ................................ ................ 53 Перелік джерел посилання ................................ ................................ ................... 55 Додаток А Фрагменти коду ................................ ................................ .................. 59 А.1 – Реалізація контентно -орієтованої моделі ................................ ............. 59 А.2 – Реалізація колаборативної фільтрації ................................ ................... 65 А.3 – Реалізація семантичної моделі ................................ ............................... 74 А.4 – Реалізація BERT ................................ ................................ ..................... 80 А.5 – Реалізація гібридної моделі ................................ ................................ .... 82 Додаток Б Графічний матеріал кваліфікаційної роботи ................................ . 100 8 СКОРОЧЕННЯ ТА УМОВНІ ПОЗНАКИ ІС – інформаційна система ІУС – інформаційні управляючі системи КФ – колаборативна фільтрація AP – average precision BERT – Bidirectional Encoder Representations from Transformers DCG – discounted cumulative gain IDCG – ideal discounted cumulative gain MAP – mean average precision NDCG – normalized discounted cumulative gain NLP – Natural Language Processing NPS – net promoter score TF-IDF – term frequency – inverse document frequency 9 ВСТУП Розвиток цифрових технологій призвів до зростання популярності фріланс -платформ, які поєднують замовників і виконавців на основі проєктів. Зі збільшенням кількості користувачів і вакансій зростає й інформаційне навантаження: фахівцям стає складно знаходити релевантні пропозиції, а замовникам – швидко відбирати відповідних кандидатів. Це зумовлює потребу у використанні рекомендаційних систем, які автоматизують підбір вакансій. У сучасних рекомендаційних системах використовують різні підходи до побудови рекомендацій: аналіз вмісту об’єктів, узагальнення поведінки користувачів, а також методи, що враховують семантичний зміст текстів на основі моделей глибокого навчання. Разом із т им кожен із цих підходів окремо має обмеження – розрідженість даних, ефект «холодного старту», неповне розуміння контексту текстових описів. Тому актуальним є пошук таких рішень, які дозволяють поєднувати сильні сторони різних методів у межах єдиного реком ендаційного механізму. Об’єктом дослідження є процес формування персоналізованих рекомендацій вакансій. Предметом дослідження є методи формування рекомендацій вакансій. Метою дослідження є розробка гібридного методу формування рекомендацій вакансій на фріланс -платформах для підвищення релевантності та точності підбору вакансій. Для досягнення мети проаналізовано існуючі методи рекомендацій, розроблено каскадний гібридний метод надання рекомендацій , реалізовано програмний прототип рекомендаційної підсистеми та проведено експериментальну оцінку його ефективності. 10 1 АНАЛІЗ ПРЕДМЕТНОЇ ОБЛАСТІ ТА ПОСТАНОВКА ЗАДАЧІ ДОСЛІДЖЕННЯ 1.1 Аналіз предметної області та опис рекомендаційних систем 1.1.1 Стан та специфіка ринку фріланс -послуг в Україні та світі Сучасна світова економіка все більше відзначається поширенням гнучких форм зайнятості, що відобразилося у так званій \"гіг -економіці\". Ця економічна модель, що будується на короткострокових контрактах та проєктній роботі замість звичної довгострокової робот и, показує швидкі темпи зростання [ 1]. Основою та рушієм цього процесу стали цифрові фріланс -платформи. Такі світові гравці, як Upwork, Freelancer та Toptal, працюють як складні двосторонні ринки, виступаючи посередниками, що з'єднують мільйони замовників із виконавцями по всьому світу. Стан цього ринку характеризується високою динамікою та постійним зростанням"}
{"chunk_id": "2025_M_IUS_Yemelyanov_AV.pdf:2", "source": "2025_M_IUS_Yemelyanov_AV.pdf", "text": "світова економіка все більше відзначається поширенням гнучких форм зайнятості, що відобразилося у так званій \"гіг -економіці\". Ця економічна модель, що будується на короткострокових контрактах та проєктній роботі замість звичної довгострокової робот и, показує швидкі темпи зростання [ 1]. Основою та рушієм цього процесу стали цифрові фріланс -платформи. Такі світові гравці, як Upwork, Freelancer та Toptal, працюють як складні двосторонні ринки, виступаючи посередниками, що з'єднують мільйони замовників із виконавцями по всьому світу. Стан цього ринку характеризується високою динамікою та постійним зростанням обсягів [ 2]. Водночас, стрімке зростання кількості учасників та проєктів на цих платформах створює ключову проблему – інформаційне перевантаження. Ця проблема стосується обох сторін – виконавці не в змозі вручну обробити тисячі нових вакансій, що з'являються щодня, а з амовники стикаються зі складним завданням вибору кваліфікованого кандидата з величезної кількості претендентів [ 3]. Така ситуація призводить до значних втрат часу та зусиль, знижує загальну ефективність ринку, що в кінцевому підсумку веде до втрати потенці йних контрактів та зниження задоволеності користувачів. Україна є важливою частиною світового ринку фрілансу та посідає на ньому помітні позиції, особливо у сегменті інформаційних технологій. Український ІТ -сектор є однією з ключових експортних галузей, причому значна частка фахівців працює саме у форматі фріла нсу на світових платформах [ 4]. Окрім цього, в Україні працюють і власні місцеві платформи та ІТ -агрегатори, що мають свою специфіку та орієнтовані на місцевий ринок праці. Висока конкуренція серед виконавців, з одного боку, та постійний попит на 11 кваліфіковані кадри, з іншого, роблять задачу точного та своєчасного зіставлення вакансій та кандидатів особливо важливою для українського сегменту [ 5]. Таким чином, загальний стан ринку характеризується гострою потребою в автоматизованих інструментах для роботи з великими та різнорідними потоками даних. 1.1.2 Сутність та функціональні особливості рекомендаційних систем В умовах інформаційного перевантаження, головним інструментом для підвищення ефективності цифрових платформ стають рекомендаційні системи. Рекомендаційну систему можна визначити як клас програм, що автоматично прогнозує ступінь зацікавленості користувача ( у вигляді рейтингу або уподобання) до певних об'єктів (наприклад, вакансій) та надає йому персоналізований список найбільш відповідних пропозицій [ 6]. На відміну від класичних пошукових систем, де користувач повинен сам ввести, що він шукає, рекомендаційні системи намагаються передбачити його справжні потреби. Вони роблять це, базуючись на аналізі профілю користувача, його попередньої поведінки та поведінки інших користувачів. Сутність рекомендаційних систем полягає у переході від загального формулювання \"о дин для всіх\" до особистого підходу до кожного користувача, що стало стандартом для успішних веб - платформ. Основна мета впровадження рекоменда ційної системи має дві сторони: - підвищення користі сервісу для кінцевого користувача шляхом спрощення пошуку; - досягнення конкретних бізнес -цілей платформи. Залежно від сфери, це може бути збільшення обсягів продажів, підвищення тривалості перебування на сайті, або, як у випадку фріланс -платформ, збільшення кількості успішних взаємодій (поданих заявок, укладених 12 контрактів). Функціонування рекомендаційної системи є постійним процесом і складаються з трьох основних етапів [ 6]. Перший етап – збір даних. Це базовий крок, що визначає якість майбутніх рекомендацій. Система збирає інформацію про те, як корис тувачі взаємодіють з об'єктами. Збір інформації може здійснюватися двома способами: - явний – коли користувач свідомо надає оцінку; - неявний – збирається автоматично, коли система стежить за поведінкою. До таких даних належать перегляди сторінок вакансій, час, проведений на сторінці, кліки, додавання до обраного, подання заявки або, навпаки, ігнорування пропозиції. Наступний етап – аналіз та навчання. На цьому етапі зібрані дані обробляються з використанням алгоритмів машинного навчання. Мета полягає у пошуку прихованих закономірностей та аналізу уподобання користувачів. Саме тут застосовуються різні підходи (контент -базовані, колаборативні, гібридні тощо), які аналізують або схожість об'єктів між собою, або схожість у поведінці користувачів. Заключний етап – формування рекомендації. Це результат, який безпосередньо бачить користувач. Формування рекомендацій починається з того моменту, коли система обчислює числове значення для кожного об'єкта - кандидата. Далі відбувається ранжування, де на осно ві цих прогнозів, система формує впорядкований список, сортуючи об'єкти від найбільш до найменш цікавих. 13 1.1.3 Аналіз існуючих методів надання рекомендацій, що використовуються в ІС Аналіз практичних реалізацій рекомендаційних систем у провідних технологічних компаніях дозволяє прослідкувати еволюцію методів та оцінити їхню ефективність у різних предметних областях. Одним із фундаментальних підходів, що довів свою комерційну ефективні сть, є система, що впроваджена Amazon. У класичній науковій праці описано їх підхід, що базується на методі колаборативної фільтрації \"елемент -до-елементу\". На відміну від підходу \"користувач -до-користувача\", який є обчислювально складним на великих набора х даних через необхідність пошуку \"сусідів\" у динамічному просторі користувачів, метод Amazon полягає у розрахунку статичної матриці схожості між елементами на основі аналізу спільних покупок. Такий підхід характеризується високою масштабованістю та здатні стю генерувати релевантні рекомендації в реальному часі, що стало індустріальним стандартом для сфери електронної комерції [ 7]. Сфера медіа -стрімінгу, представлена Netflix та Spotify, стала каталізатором для розробки більш складних моделей. Сучасні системи Netflix є складними багаторівневими гібридними архітектурами, що інтегрують колаборативні дані, контентні ознаки та контекстну інформацію. Особливістю є глибока персоналізація, що виходить за межі простого ранжування контенту і включає динамічну адаптацію користувацького інтерфейсу, наприклад, персоналізацію"}
{"chunk_id": "2025_M_IUS_Yemelyanov_AV.pdf:3", "source": "2025_M_IUS_Yemelyanov_AV.pdf", "text": "елементами на основі аналізу спільних покупок. Такий підхід характеризується високою масштабованістю та здатні стю генерувати релевантні рекомендації в реальному часі, що стало індустріальним стандартом для сфери електронної комерції [ 7]. Сфера медіа -стрімінгу, представлена Netflix та Spotify, стала каталізатором для розробки більш складних моделей. Сучасні системи Netflix є складними багаторівневими гібридними архітектурами, що інтегрують колаборативні дані, контентні ознаки та контекстну інформацію. Особливістю є глибока персоналізація, що виходить за межі простого ранжування контенту і включає динамічну адаптацію користувацького інтерфейсу, наприклад, персоналізацію постерів до кінофільмів [ 6]. Платформа Spotify також використовує гібридний підхід, комбінуючи колаборативний аналіз плейлистів користувачів з глибоким контент -базованим аналізом аудіосигналів для виявлення схожості треків за музичними характеристиками . У контексті даного дослідження, особливий інтерес представляє рекомендаційна система професійної мережі LinkedIn, оскільки вона вирішує 14 схожу задачу відповідності на ринку праці. Система LinkedIn є яскравим прикладом гібридної моделі, адаптованої до специфіки задачі \"job matching\" [ 8]. Вона використовує комбінацію контент -базованого аналізу та потужних колаборативних сигналів. Ефективність системи досягається шляхом врахування багатовимірних даних профілю, що дозволяє рекомендувати не лише очевидні, але й потенційно корисні кар'єрні м ожливості. Таким чином, аналіз промислових рекомендаційних систем доводить, що провідні платформи відійшли в ід простих алгоритмів на користь складних гібридних моделей, що глибоко інтегровані у бізнес -логіку платформи. 1.2 Аналіз існуючих ІС та методів надання рекомендацій щодо пошуку вакансій, їх переваги та недоліки 1.2.1 Огляд функціональних можливостей провідних платформ пошуку роботи У рамках дослідження предметної області доцільно провести детальний аналіз існуючих ІС, що займають провідні позиції на світовому ринку праці та фрілансу. Об'єктами аналізу обрано популярні платформи, що репрезентують різні концептуальні підходи до організації взаємодії між замовником та виконавцем: - Upwork; - LinkedIn; - Fiverr; - Freelancer. Upwork є найбільшою глобальною платформою для фрілансу. Її підсистема рекомендацій відіграє ключову роль у розподілі трудових ресурсів. Функціонал платформи базується на використанні алгоритму оптимального підбору, який аналізує семантичну відповідність між опис ом проєкту та 15 профілем виконавця [ 9]. Окрім прямого збігу професійних навичок, система враховує приховані фактори, такі як історія успішного виконання проєктів у конкретних категоріях та зворотний зв'язок від попередніх замовників. Особливістю алгоритму є інтеграція прогнозної аналітики – система оцінює не лише релевантність навичок, але й ймовірність успішного завершення контракту на основі аналізу поведінкових патернів користувача. LinkedIn використовує відмінний підхід, зумовлений наявністю специфічного типу даних – соціального графа [ 8]. Рекомендаційний механізм платформи виходить за межі стандартного аналізу тексту резюме, інтегруючи інформацію про професійні зв'язки користувача. Алгоритм ранжування надає пріоритет вакансіям у компаніях, де вже працюють особи з мережі контактів користу вача або випускники того ж навчального закладу. Такий підхід базується на гіпотезі про те, що наявність соціального капіталу позитивно корелює з ймовірністю працевлаштування. Додатково застосовуються методи колаборативної фільтрації, що аналізують поведінк у користувачів зі схожими професійними профілями для генерації релевантних пропозицій. Fiverr реалізує модель \"послуга як товар\", де об'єктом рекомендації виступають стандартизовані картки послуг, а не вакансії. Архітектура рекомендаційної системи даної платформи наближена до систем електронної комерції. Механізм базується на аналізі історії перегл ядів, пошукових запитів та транзакцій [ 10]. Система формує персоналізовану стрічку пропозицій, використовуючи методи пошуку схожих об'єктів. Для виконавців реалізовано функцію аналітичної рекомендації, що на основі аналізу ринкового попиту пропонує створення нових типів послуг, які можуть бути затребувані в поточному періоді. Freelancer представляє одну з найстаріших моделей на ринку, що поєднує класичний фріланс із конкурсною механікою. Рекомендаційна система платформи значною мірою є контент -базованою і спирається на чітке співставлення заявлених навичок із вимогами проєкту [ 11]. Відмінною рисою є інтеграція елементів гейміфікації: рейтинг та видимість пропозицій користувача 16 залежать від набраних балів досвіду та внутрішніх кваліфікаційних іспитів. Система характеризується високою інтенсивністю сповіщень, використовуючи механізми миттєвого інформування про нові проєкти, що відповідають ключовим словам профілю, часто нехтуючи г либоким контекстним аналізом на користь швидкості доставки пропозиції. Проведений аналіз дозволяє стверджувати, що сучасні системи не обмежуються єдиним методом фільтрації, а використовують комплексний підхід, в якому поєднуються аналіз тексту, поведінкова історія та соціальні зв’язки. 1.2.2 Порівняльний аналіз існуючих рішень На основі огляду функціональних можливостей провідних платформ проведено систематизацію ключових характеристик, що визначають ефективність рекомендаційних систем у даній предметній області. Метою порівняльного аналізу є виявлення переваг та недоліків існую чих реалізацій для врахування їх при розробці власної моделі . Основними критеріями порівняння визначено: тип алгоритмічної моделі, джерела вхідних даних для аналізу та специфічні функціональні обмеження. Результати порівняльного аналізу наведено в табл. 1 .1. Таблиця 1 .1. Порівняльний аналіз рекомендаційних систем фріланс - платформ Платформа Тип моделі Ключові дані для аналізу Особливості реалізації Переваги Upwork Гібридна Історія пропозицій, теги навичок, семантика опису вакансії, рейтинг успішності. Маркування вакансій за ступенем відповідності; персоналізована стрічка. Висока точність для користувачів з історією; врахування економічних факторів. 17 Продовження таблиці 1 .1. Платформа Тип моделі Ключові дані для аналізу Особливості реалізації Переваги LinkedIn Гібридна Профіль користувача, соціальний граф, дані про компанії, навички. Окремий розділ рекомендованих вакансій; сповіщення про наявність контактів у компанії. Використання унікальних соціальних даних; високий рівень довіри до рекомендацій. Fiverr Об'єктно - орієнтована Історія переглядів, транзакцій, пошукові запити. Стрічки"}
{"chunk_id": "2025_M_IUS_Yemelyanov_AV.pdf:4", "source": "2025_M_IUS_Yemelyanov_AV.pdf", "text": "Гібридна Історія пропозицій, теги навичок, семантика опису вакансії, рейтинг успішності. Маркування вакансій за ступенем відповідності; персоналізована стрічка. Висока точність для користувачів з історією; врахування економічних факторів. 17 Продовження таблиці 1 .1. Платформа Тип моделі Ключові дані для аналізу Особливості реалізації Переваги LinkedIn Гібридна Профіль користувача, соціальний граф, дані про компанії, навички. Окремий розділ рекомендованих вакансій; сповіщення про наявність контактів у компанії. Використання унікальних соціальних даних; високий рівень довіри до рекомендацій. Fiverr Об'єктно - орієнтована Історія переглядів, транзакцій, пошукові запити. Стрічки рекомендацій на основі історії переглядів. Ефективна робота зі структурованим каталогом послуг; адаптивність до зміни інтересів. Freelancer Контент - базована Заявлені навички, результати кваліфікаційних тестів, бали досвіду. Інтенсивні поштові розсилки; фільтрація проєктів за тегами; конкурси. Прозорість роботи; швидкість інформування; гейміфікація процесу. Резуль тати аналізу, наведені в табл. 1 .1, свідчать про те, що найбільш перспективною є гібридна модель, реалізована у системах Upwork та LinkedIn. Такий підхід дозволяє компенсувати обмеження окремих методів фільтрації. Водночас, спільним недоліком розглянутих систем є низький рівень інтерпрет ованості результатів для кінцевого користувача та високий бар'єр входження для нових учасників через відсутність накопиченої історії взаємодій. Враховуючи виявлені особливості, при розробці власної системи доцільно орієнтуватися на гібридну архітектуру з посиленим блоком семантичного аналізу текстових даних для вирішення проблеми нових вакансій та користувачів. 18 1.3 Аналіз існуючих методів та моделей для вирішення задач рекомендації вакансій 1.3.1 Метод контентної фільтрації на основі векторної моделі Векторна модель є фундаментальним підходом в теорії інформаційного пошуку, який застосовується для формалізації текстових даних у задачах рекомендації вакансій. Основна ідея методу полягає у представленні текстових документів як наборів числових характерис тик у багатовимірному просторі, де кожен вимір відповідає певному слову зі словника. Ключовим етапом даного підходу є визначення важливості кожного терміна в документі, для чого традиційно використовується статистична міра TF - IDF. Ця метрика дозволяє оцінити вагу слова, базуючись на двох протилежних факторах: - частота терміна – чим частіше слово зустрічається в конкретному документі (наприклад, вакансії), тим важливішим воно вважається для опису змісту цього документа; - обернена частота документа – цей компонент знижує вагу слів, які зустрічаються занадто часто в усій базі даних, якщо слово зустрічається скрізь, воно не є унікальним і не допомагає відрізнити одну вакансію від іншої. Таким чином, найбільшу вагу отримують слова, які часто вживаються в конкретній вакансії, але рідко зустрічаються в інших. Для оцінки ступеня схожості між профілем користувача та вакансією використовується косинусна міра подібності. Вона дозволяє визначити, наскільки близькими є два документи за своїм змістом, вимірюючи кут між їхніми векторами. Якщо вектори знаходяться в одно му напрямку, документи вважаються дуже схожими. Перевагою цього методу є те, що він коректно порівнює тексти різної довжини. Однак суттєвим обмеженням є ігнорування змісту слів [ 12]. 19 1.3.2 Метод колаборативної фільтрації на основі матричної факторизації Методи матричної факторизації належать до класу моделей з латентними факторами і є найбільш ефективним підходом у рамках колаборативної фільтрації. Задача рекомендації в цьому контексті розглядається як спроба передбачити, як користувач оцінив би вакансію, з якою він ще не взаємодіяв, на основі його минулих дій . Суть методу полягає у розкладанні величезної таблиці взаємодій (де рядки – це користувачі, а стовпці – вакансії) на дві менші, компактні матриці: - перша матриця описує кожного користувача через набір прихованих характеристик; - друга матриця описує кожну вакансію через той самий набір характеристик. Ці \"приховані характеристики\" не задаються вручну, а виявляються алгоритмом автоматично під час навчання на основі аналізу закономірностей у діях усіх користувачів системи. Прогнозована оцінка для конкретної пари \"користувач -вакансія\" розраховується шляхом поєднання їхніх векторів прихованих характеристик. Головна перевага цього методу – здатність знаходити неочевидні зв'язки та рекомендувати вакансії, які користувач міг би пропустити при звичайному пошуку. Головним недоліком є проблема \"холодного старту\": метод не може сформувати профіль прихованих факторів для нового користувача або нової вакансії, доки вони не накопичать історію взаємодій [ 13]. 20 1.3.3. Моделі на основі семантичних векторів та нейронних мереж Для подолання обмежень класичних статистичних методів, які ігнорують змістовий зв'язок між словами, у сучасних рекомендаційних системах застосовуються моделі глибокого навчання для генерації щільних векторних представлень. Фундаментальною відмінністю цього підходу є перехід від простого підрахунку слів до розуміння їхнього контексту. В основі даного класу моделей лежить гіпотеза, що слова, які зустрічаються у схожих контекстах, мають близькі значення. Реалізація цього принципу відбулася у два етапи: - статичні моделі – архітектури типу Word2Vec та GloVe використовують нейронні мережі для навчання на великих обсягах тексту. У результаті кожному слову зі словника ставиться у відповідність фіксований набір чисел. Схожість слів визначається відстанню між ни ми у математичному просторі. Проте недоліком є те, що вектор для слова залишається незмінним незалежно від контексту; - контекстуальні моделі – сучасний етап розвитку базується на архітектурі трансформерів. Ключовим елементом тут є механізм уваги, який дозволяє моделі аналізувати зв'язки між усіма словами в реченні одночасно. На відміну від статичних підходів, BERT генерує унікальне представлення для слова в кожному конкретному випадку його використання, динамічно змінюючи його значення залежно від оточення. Для задач рекомендації вакансій це дозволяє системі аналізувати та розуміти зміст"}
{"chunk_id": "2025_M_IUS_Yemelyanov_AV.pdf:5", "source": "2025_M_IUS_Yemelyanov_AV.pdf", "text": "ми у математичному просторі. Проте недоліком є те, що вектор для слова залишається незмінним незалежно від контексту; - контекстуальні моделі – сучасний етап розвитку базується на архітектурі трансформерів. Ключовим елементом тут є механізм уваги, який дозволяє моделі аналізувати зв'язки між усіма словами в реченні одночасно. На відміну від статичних підходів, BERT генерує унікальне представлення для слова в кожному конкретному випадку його використання, динамічно змінюючи його значення залежно від оточення. Для задач рекомендації вакансій це дозволяє системі аналізувати та розуміти зміст цілих абзаців тексту. Отримані семантичні вектори дозволяють знаходити релевантні вакансії навіть за повної відсутності спільних ключових слів, спираючись виключно на змістов у близькість понять. Встановлено, що класичні методи мають критичні обмеження для сфери фрілансу. Прості статистичні методи не враховують семантику, а методи факторизації не працюють з новими об'єктами без історії. 21 Після аналізу існуючих систем -аналогів, методів та моделей та їх переваг і недоліків, можна зробити висновок, що для якісного вирішення задачі рекомендацій вакансій на фріланс -платформі необхідно розробити гібридний метод формування рекомендацій [ 14]. 1.4 Постановка задачі дослідження В ході аналізу існуючих систем -аналогів та методів формування рекомендацій вакансій виявлено низку суттєвих недоліків, що обмежують їх ефективність у динамічних умовах фріланс -платформ: - неспроможність класичних текстових методів враховувати семантичний зв'язок слів, що призводить до втрати релевантних вакансій, якщо вони описані іншими термінами; - проблема \"холодного старту\" в методах колаборативної фільтрації, що унеможливлює надання точних рекомендацій для нових користувачів та нових вакансій, які ще не мають історії взаємодій; - проблема розрідженості даних, характерна для великих платформ, де один користувач взаємодіє лише з мізерною часткою доступних проєктів, що знижує точність прогнозів; - складність інтерпретації результатів роботи складних нейромережевих моделей, що знижує довіру користувачів до системи. Таким чином, враховуючи вищезазначені недоліки, можна сформулювати мету, об'єкт та предмет дослідження. Метою дослідження є розробка гібридного методу формування рекомендацій вакансій на фріланс -платформах для підвищення релевантності та точності підбору вакансій. Для досягнення поставленої мети необхідно вирішити такі основні завдання: 22 - провести детальний аналіз існуючих підходів до побудови рекомендаційних систем та визначити їх слабкі сторони в контексті ринку праці; - дослідити методи векторизації тексту на основі глибокого навчання для покращення розуміння змісту вакансій; - розробити власний гібридний метод формування рекомендацій вакансій на фріланс -платформах; - розробити програмний прототип рекомендаційної системи для експериментальної перевірки працездатності запропонованого методу. - оцінити ефективність розробленого методу за допомогою стандартних метрик якості та порівняти його з базовими підходами. Об’єктом дослідження є процес формування персоналізованих рекомендацій вакансій. Предметом дослідження є методи формування рекомендацій вакансій. Основним результатом роботи є розробка гібридного методу формування рекомендацій та реалізація, на його основі, програмного модуля рекомендаційної системи. Цей модуль повинен забезпечити можливість автоматичного ранжування вакансій з урахуванням як явних в имог, так і прихованого змісту описів, що дозволить значно скоротити час пошуку роботи для фрілансерів. Таким чином, основна задача полягає у створенні гібридного методу, який нівелює недоліки окремих методів фільтрації, забезпечуючи високу точність рекомендацій навіть за умов неповноти даних або відсутності історії дій, що сприятиме підвищенню ефективності взаємодії між замовниками та виконавцями. 23 2 РОЗРОБКА ГІБРИДНОГО МЕТОДУ ФОРМУВАННЯ РЕКОМЕНДАЦІЙ ВАКАНСІЙ ДЛЯ ФРІЛАНС ПЛАТФОРМИ 2.1 Дослідження контентно -орієнтованої моделі Контентно -орієнтована модель для рекомендації вакансій спирається на перетворення текстів профілів користувачів і вакансій у вектори, потім порівнює їхню схожість. На фріланс -платформах це можуть бути заголовки замовлень, деталі роботи, технічні умови, нео бхідна кваліфікація чи знання мов – окремо взяті або разом з даними про працівника. Для реалізації цього підходу текстові дані подаються у вигляді векторів ознак у просторі термінів. Однією з базових і найбільш поширених схем зважування термів є TF -IDF , що використовується в інформаційному пошуку та текстових рекомендаційних системах [15]. Нехай 𝐷 – корпус документів (описів вакансій), 𝑁=∣𝐷∣ – кількість документів у корпусі, 𝑑∈𝐷– окремий документ, 𝑡 – термін. Термінна частота у документі 𝑑 визначається за наступною формулою [15]: tf(𝑡,𝑑)=𝑓𝑡,𝑑 ∑∗𝑡′∈𝑑𝑓𝑡′,𝑑, (2.1) де 𝑓𝑡,𝑑– кількість входжень терміна 𝑡у документі 𝑑; ∑∗𝑡′∈𝑑𝑓𝑡′,𝑑 – загальна кількість термінів у документі. Обернена частота документа, яка відображає рідкісність терміна в корпусі, визначається за наступною формулою [15]: idf(𝑡,𝐷)=log⁡𝑁 𝑛𝑡, (2.2) де 𝑁 – кількість документів у корпусі; 𝑛𝑡 – кількість документів, що містять термін 𝑡. 24 Відповідно, вага терміна за схемою TF -IDF записується у вигляді [15]: tfidf(𝑡,𝑑,𝐷)=tf(𝑡,𝑑)⋅idf(𝑡,𝐷), (2.3) де tf(𝑡,𝑑)– нормована частота терміна в документі; idf(𝑡,𝐷)– обернена частота документа; tfidf(𝑡,𝑑,𝐷)– підсумкова вага терміна 𝑡 у документі 𝑑 відносно корпусу 𝐷. За допомогою такої схеми зважування зменшується вплив поширених, малозмістовних слів і підсилює терміни, характерні для певних доменів чи технологій, що є важливим для рекомендацій технічних вакансій. Кожен опис вакансії 𝑑 подається як вектор: v𝑑=(tfidf(𝑡1,𝑑,𝐷),…,tfidf(𝑡𝑚,𝑑,𝐷)) (2.4) де 𝑡1,…,𝑡𝑚– терміни словника; v𝑑 – вектор ознак документа у просторі розмірності 𝑚. Аналогічним чином створюється вектор профілю користувача, якщо проаналізувати його профіль із усіма описовими характеристиками. Після процесу побудови векторів, наступним етапом є визначення міри подібності між профілем користувача та вакансією. Для знаходження подібності використовується косинусна міра схожості. Вона визначається за наступною формулою [16]: simcos(v𝑢,v𝑖)=𝐯𝑢⋅𝐯𝑖 ∥𝐯𝑢∥ ∥𝐯𝑖∥=∑∗𝑚 𝑘=1𝑣𝑢,𝑘𝑣𝑖,𝑘 √∑∗𝑚 𝑘=1𝑣𝑢,𝑘2 √∑∗𝑚 𝑘=1𝑣𝑖,𝑘2 (2.5) де"}
{"chunk_id": "2025_M_IUS_Yemelyanov_AV.pdf:6", "source": "2025_M_IUS_Yemelyanov_AV.pdf", "text": "є важливим для рекомендацій технічних вакансій. Кожен опис вакансії 𝑑 подається як вектор: v𝑑=(tfidf(𝑡1,𝑑,𝐷),…,tfidf(𝑡𝑚,𝑑,𝐷)) (2.4) де 𝑡1,…,𝑡𝑚– терміни словника; v𝑑 – вектор ознак документа у просторі розмірності 𝑚. Аналогічним чином створюється вектор профілю користувача, якщо проаналізувати його профіль із усіма описовими характеристиками. Після процесу побудови векторів, наступним етапом є визначення міри подібності між профілем користувача та вакансією. Для знаходження подібності використовується косинусна міра схожості. Вона визначається за наступною формулою [16]: simcos(v𝑢,v𝑖)=𝐯𝑢⋅𝐯𝑖 ∥𝐯𝑢∥ ∥𝐯𝑖∥=∑∗𝑚 𝑘=1𝑣𝑢,𝑘𝑣𝑖,𝑘 √∑∗𝑚 𝑘=1𝑣𝑢,𝑘2 √∑∗𝑚 𝑘=1𝑣𝑖,𝑘2 (2.5) де v𝑢 – векторне подання профілю користувача; v𝑖 – векторне подання опису вакансії; 𝑣𝑢,𝑘 та 𝑣𝑖,𝑘 – ваги 𝑘-го терміна у векторах відповідно; 𝑚 – розмірність простору ознак; 25 ∥v𝑢∥ та ∥v𝑖∥– евклідові норми векторів. Значення цієї міри інтерпретується як косинус кута між двома векторами: чим ближчим до одиниці є значення, тим більш схожими за текстовим змістом є профіль користувача та вакансія. Поєднуючи TF -IDF з косинусною мірою подібності для обробки текстових описів дозволяє описати як зміст вакансій, так і характеристики профілів фрілансерів і використовувати цю інформацію для побудови контентно - орієнтованих рекомендацій. 2.2 Дослідження методу колаборативної фільтрації Колаборативна фільтрація виконує задачу рекомендацій з точки зору аналізу історії взаємодій користувачів із вакансіями. Алгоритм колаборативної фільтрації полягає в тому, що вона підбирає користувачів з подібними взаєомдіями щодо однакових об’єктів та мают ь схожі вподобання і в майбутньому може будуть зацікавлені в подібних вакансіях. У рамках фріланс - платформи взаємодіями можуть бути перегляди, відгуки, прийняття пропозицій , успішне завершення контрактів , збереження та інші . Нехай матриця взаємодій позначається як: 𝑅=(𝑟𝑢,𝑖) (2.6) де 𝑢 – індекс користувача; 𝑖 – індекс вакансії; 𝑟𝑢,𝑖 – показник взаємодії користувача 𝑢 із вакансією 𝑖. У варіанті КФ на основі користувачів для кожного користувача шукають множину користувачів з подібною поведінкою – і на основі їхніх взаємодій прогнозують корисність вакансій для цільового користувача. Для оцінки 26 подібності профілів користувачів часто застосовується коефіцієнт лінійної кореляції Пірсона. Припустимо, що 𝐼𝑢 є множиною вакансій, з якими взаємодіяв користувач 𝑢, а через 𝐼𝑢𝑣=𝐼𝑢∩𝐼𝑣 позначимо множину вакансій, спільних для двох користувачів 𝑢 та 𝑣. Середнє значення взаємодій користувача 𝑢 позначимо як 𝑟ˉ𝑢. Тоді коефіцієнт Пірсона між користувачами 𝑢 та 𝑣 буде обчислюватись за наступною формулою [17]: simP(𝑢,𝑣)=∑∗𝑖∈𝐼𝑢𝑣(𝑟𝑢,𝑖−𝑟ˉ𝑢)(𝑟𝑣,𝑖−𝑟ˉ𝑣) √∑∗𝑖∈𝐼𝑢𝑣(𝑟𝑢,𝑖−𝑟ˉ𝑢)2 √∑∗𝑖∈𝐼𝑢𝑣(𝑟𝑣,𝑖−𝑟ˉ𝑣)2, (2.7) де 𝑟𝑢,𝑖та 𝑟𝑣,𝑖 – значення взаємодій користувачів 𝑢 та 𝑣⁡з вакансією 𝑖; 𝑟ˉ𝑢 та 𝑟ˉ𝑣 – середні значення відповідних взаємодій по користувачах; 𝐼𝑢𝑣 – множина вакансій, з якими взаємодіяли обидва користувачі. Значення simP(𝑢,𝑣) змінюється у діапазоні від −1 до 1 і відображає ступінь лінійної залежності між шаблонами оцінювання двох користувачів. Після процесу обчислення схожості користувачів обирається підмножина найбільш подібних 𝑁𝑢(𝑖) для користувача 𝑢 при прогнозуванні відповідної вакансії 𝑖. Для прогрозу значення 𝑟̂𝑢,𝑖 використовується класична формула колаборативної фільтрації на основі користувачів, яка виглядає як [17]: 𝑟̂𝑢,𝑖=𝑟ˉ𝑢+∑ ∗ 𝑣∈𝑁𝑢(𝑖)simP(𝑢,𝑣) (𝑟𝑣,𝑖−𝑟ˉ𝑣) ∑ ∗ 𝑣∈𝑁𝑢(𝑖)∣simP(𝑢,𝑣)∣ (2.8) де 𝑟̂𝑢,𝑖 – прогнозоване значення взаємодії користувача 𝑢 з вакансією 𝑖; 𝑟ˉ𝑢 – середнє значення оцінок користувача 𝑢; 𝑁𝑢(𝑖) – множина сусідів користувача 𝑢, які вже взаємодіяли з вакансією 𝑖; simP(𝑢,𝑣) – коефіцієнт Пірсона між користувачами 𝑢 та 𝑣; 𝑟𝑣,𝑖 – значення взаємодії сусіда 𝑣 із вакансією 𝑖; 𝑟ˉ𝑣 – середнє значення його оцінок. 27 Така нормалізація дозволяє компенсувати індивідуальні зсуви у шкалах оцінювання різних користувачів. Підбиваючи підсумки, можна сказати, що КФ дає змогу враховувати досвід фрілансерів та виявляти вакансії, які виявились якісними для користувачів із подібними взаємодіями. Але варто врахувати, що даний підхід має відомі обмеження, у вигляді, розрідженості м атриці взаємодій при великій кількості користувачів та проблема «холодного старту», у випадку, коли користувач не має історії взаємодії. Для розширення колаборативної фільтрації на основі користувачів, зазвичай, використовуються КФ на основі об’єктів, у якій для прогнозування значень використовується подібність між вакансіями. Ідея даного типу фільтрації полягає в тому, що об’єкти, з якими взаємодіяли одні і ті ж самі користувачі, можуть мати спільні характеристики, і тому їхня схожість може бути використана для визначення, наскілька інша вакансія підходить для користувача [18]. Подібність між вакансіями обчислюється аналогічним чином, як і для користувачів, але з транспонованою матрицею взаємодій. КФ на основі об’єктів вважається більш стійким у випадках, коли кількість вакансій суттєво менша за кількість користувачів, оскільки поведінкові профілі вакансій формуються щільніше, ніж профілі користувачів. У фріланс -платформі цей підхід дозволяє ефективно визначати вакансії , які підходять користувачеві, навіть при відносно невеликій кількості взаємодій, оскільки вагому роль відіграє структура схожості самих вакансій. 2.3 Дослідження семантичних методів При дослідженні контентно -орієнтованої моделі, можна прийти до висновку, що TD -IDF трактує документи як набір незалежних термінів і не може 28 якісно враховувати контекстні зв’язки між словами. Дане обмеження є проблемним для фріланс -платформ, так як у багатьох випадках, зміст вакансій може бути сформульований різними словами та конструкціями. Для вирішення цієї проблеми існують сучасні підходи, які орієнтуються на контекстні векторні представлення, що будуються за допомогою трансформерних моделей. Однією із найбільш популярних та впливових моделей такого типу є BERT (Bidirectional Encoder Representations from Transformers) . BERT складається з послідовності енкодерних шарів. Кожен такий шар містить механізм лінійного перетворення, завдяки чому текстова"}
{"chunk_id": "2025_M_IUS_Yemelyanov_AV.pdf:7", "source": "2025_M_IUS_Yemelyanov_AV.pdf", "text": "28 якісно враховувати контекстні зв’язки між словами. Дане обмеження є проблемним для фріланс -платформ, так як у багатьох випадках, зміст вакансій може бути сформульований різними словами та конструкціями. Для вирішення цієї проблеми існують сучасні підходи, які орієнтуються на контекстні векторні представлення, що будуються за допомогою трансформерних моделей. Однією із найбільш популярних та впливових моделей такого типу є BERT (Bidirectional Encoder Representations from Transformers) . BERT складається з послідовності енкодерних шарів. Кожен такий шар містить механізм лінійного перетворення, завдяки чому текстова послідовність на вході переводиться у послідовність векторів прихованих станів . Сукупність шарів утворює енкодер, тобто відображення від простору текстів до просору векторних представлень фіксованого розміру. Якщо казати узагальнено, енкодер можна розшлядати як функцію, що отримує токенізований текст і повертає для нього числове пода ння, яке в подальшому буде використовуватись дл я порівняння та класифікації [19]. Для практичних застосувань, створено декілька варіантів архітектури, зокрема BERT -Base (12 енкодерних шарів, розмір прихованого стану 768) та BERT -Large (24 шари, прихований розмір 1024), які відрізняються якістю представлень та обчислювальною вартістю [19]. Нехай enc(⋅) позначає енкодер, побудований на основі BERT. Тоді опис вакансії 𝑖 і текстовий опис профілю користувача 𝑢 можуть бути перетворені у вектори фіксованої розмірності за формулами: e𝑖=enc(опис вакансії 𝑖);⁡e𝑢=enc(профіль користувача 𝑢) (2.9) де e𝑖 – векторне представлення текстового опису вакансії 𝑖; e𝑢 – векторне представлення тексту профілю користувача 𝑢; enc(⋅) – відображення, що реалізується послідовністю енкодерних шарів BERT і перетворює вхідну послідовність токенів на один вектор фіксованої 29 розмірності. Наступним етапом, після перетворення описів профіля користувача та вакансії, є обчислення семантичної близькості між ними. Доцільно це робити також за допомогою косинусної міри подібності [20]: simBERT(𝑢,𝑖)=e𝑢⋅e𝑖 ∥e𝑢∥ ∥e𝑖∥, (2.10) де e𝑢 та e𝑖 – вектори, отримані з моделі BERT для профілю користувача та вакансії; ∥e𝑢∥ і ∥e𝑖∥– їх евклідові норми. Таким чином, високе значення simBERT(𝑢,𝑖) вказує на семантичну близькість двох текстів з урахуванням контексту, а не лише збігу окремих термінів. 2.4 Розробка гібридного методу для формування рекомендацій вакансій На основі проаналізованих методів розроблено гібридний підхід до формування рекомендацій вакансій для фріланс -платформи. Запропонований метод має каскадну структуру та працює у двох режимах залежно від стану користувача. Для користувачів з наявною історією взаємодій застосовуються колаборативна фільтрація, контентно -орієнтований метод та семантичний підхід. Для нових користувачів використовується модифікований підхід , що спирається на контентн ий та семантичн ий методи з додатковим урахуванням популярності та новизни вакансій. Такий підхід дозволяє поєднати сильні сторони колаборативної та контентно -орієнтованої фільтрації, а також сучасних семантичних моделей на основі BERT, мінімізуючи їхні окремі недоліки. У разі, коли користувач має заповнений профіль із навичками й хоча б одну взаємодію з вакансіями, система працює у каскадному режимі. Розглядається 30 матриця взаємодій (2. 6). Для кожного користувача обчислюється середнє значення його взаємодій. Міра с хожості між двома користувачами обчислюєтся за формолою (2.7) . На основі цього розрахунку, для кожної вакансії, яка ще не отримувала оцінки від поточного користувача, прогнозується очікуване значення взаємодій за схемою КФ на основі взаємодій користувачів (2. 8). Таким же чином використовується КФ на основі об'єктів, де аналізуються схожі вакансії. Для підвищення стійкості оцінки, запропонована комбінована модель, де підсумковий колаборативний бал розраховується за формулою: 𝑠𝑢,𝑖CF=0,5 𝑟̂𝑢,𝑖user+0,5 𝑟̂𝑢,𝑖item, (2.11) де 𝑢– індекс користувача в системі; 𝑖– індекс вакансії; 𝑠𝑢,𝑖𝐶𝐹– узагальнений колаборативний бал вакансії 𝑖 для користувача 𝑢, отриманий у результаті комбінування двох типів колаборативної фільтрації; 𝑟̂𝑢,𝑖𝑢𝑠𝑒𝑟– прогнозований рейтинг вакансії 𝑖 для користувача 𝑢, обчислений колаборативною фільтрацією на основі схожості між користувачами та їхніх історій взаємодій; 𝑟̂𝑢,𝑖𝑖𝑡𝑒𝑚– прогнозований рейтинг вакансії 𝑖 для користувача 𝑢, обчислений колаборативною фільтрацією на основі схожості між вакансіями; Коефіцієнти відображають рівну вагу обох типів КФ. На основі значень 𝑠𝑢,𝑖CF формується впорядкована множина кандидатів. Другий етап у каскадному режимі реалізує контентно -орієнтовану фільтрацію в межах сформованого списку кандидатів. Для кожного користувача формується текстове представлення профілю, що об’єднує його навички, біографічну інформацію та опис досвіду. Аналогічн а процедура відбуваєтсья по відношенню до кожної вакансії . Формуються тексти, які містять назву, опис, вимоги до навичок та категоріальну інформацію. На основі цих текстів , 31 створюються TF-IDF-вектори (2.1) – (2.4), а подібність між профілем користувача та вакансією оцінюється за косинусною мірою (2.5) . Третій етап каскадного режиму полягає у семантичному ранжуванню, попередньо відібраних кандидатів, за допомогою моделі BERT. Для профілю користувача та описів вакансій будуються контекстні векторні представлення, сформовані енкодером BERT (2.9) . Перед обчисленням міри подібності, ці вектори нормалізуються до одиничної довжини. Семантичний бал визначається за косинусною мірою (2. 10). Отримані на різних етапах бали мають різні шкали, тому перед об’єднанням вони нормалізуються за схемою min –max для кожного користувача окремо. Нехай 𝑠𝑢,𝑖CF, 𝑠𝑢,𝑖contта 𝑠𝑢,𝑖sem– відповідно колаборативний, контентний і семантичний бали для вакансії 𝑖, а 𝑖 перебирає множину кандидатів для користувача 𝑢. Для кожного методу 𝑚∈{CF,cont,sem} вводяться значення 𝑠𝑢𝑚,min⁡та 𝑠𝑢𝑚,max⁡, що відповідають мінімальному та максимальному значенню оцінки серед усіх кандидатів. Нормалізований бал визначається за формулою [21]: 𝑠̃𝑢,𝑖𝑚=𝑠𝑢,𝑖𝑚−𝑠𝑢𝑚,𝑚𝑖𝑛⁡ 𝑠𝑢𝑚,𝑚𝑎𝑥⁡−𝑠𝑢𝑚,𝑚𝑖𝑛⁡,⁡ (2.12) де 𝑠̃𝑢,𝑖𝑚– нормалізований бал методу 𝑚 для вакансії 𝑖; 𝑠𝑢,𝑖𝑚– вихідне значення; 𝑠𝑢𝑚,min⁡та 𝑠𝑢𝑚,max⁡ – мінімальне і максимальне значення серед кандидатів"}
{"chunk_id": "2025_M_IUS_Yemelyanov_AV.pdf:8", "source": "2025_M_IUS_Yemelyanov_AV.pdf", "text": "об’єднанням вони нормалізуються за схемою min –max для кожного користувача окремо. Нехай 𝑠𝑢,𝑖CF, 𝑠𝑢,𝑖contта 𝑠𝑢,𝑖sem– відповідно колаборативний, контентний і семантичний бали для вакансії 𝑖, а 𝑖 перебирає множину кандидатів для користувача 𝑢. Для кожного методу 𝑚∈{CF,cont,sem} вводяться значення 𝑠𝑢𝑚,min⁡та 𝑠𝑢𝑚,max⁡, що відповідають мінімальному та максимальному значенню оцінки серед усіх кандидатів. Нормалізований бал визначається за формулою [21]: 𝑠̃𝑢,𝑖𝑚=𝑠𝑢,𝑖𝑚−𝑠𝑢𝑚,𝑚𝑖𝑛⁡ 𝑠𝑢𝑚,𝑚𝑎𝑥⁡−𝑠𝑢𝑚,𝑚𝑖𝑛⁡,⁡ (2.12) де 𝑠̃𝑢,𝑖𝑚– нормалізований бал методу 𝑚 для вакансії 𝑖; 𝑠𝑢,𝑖𝑚– вихідне значення; 𝑠𝑢𝑚,min⁡та 𝑠𝑢𝑚,max⁡ – мінімальне і максимальне значення серед кандидатів для цього користувача. Після нормалізації фінальний бал вакансії 𝑖 для користувача 𝑢 обчислюється як зважена сума нормалізованих оцінок [22]: 𝑠𝑢,𝑖=0,2 𝑠̃𝑢,𝑖CF+0,4 𝑠̃𝑢,𝑖cont+0,4 𝑠̃𝑢,𝑖sem, (2.13) 32 де 𝑠𝑢,𝑖 – підсумкова оцінка релевантності вакансії, 𝑠̃𝑢,𝑖CF, 𝑠̃𝑢,𝑖contта 𝑠̃𝑢,𝑖sem – нормалізовані бали колаборативного, контентного та семантичного методів. Ці коефіцієнти задають баланс між компонентами гібридної моделі. Колаборативній фільтрації надано меншу вагу, оскільки її якість суттєво залежить від наявності достатньої історії взаємодій і вона є більш вразливою до проблеми розрідженості даних та «холодн ого старту». Контентному та семантичному підходам призначено однакові ваги , що відображає припущення про їх більшу стійкість і універсальність. Сума коефіцієнтів дорівнює 1, що забезпечує інтерпретованість підсумкового бала як комбінації трьох методів рекомендації. У режимі «холодного старту», коли користувач не має історії взаємодій або профіль недостатньо заповнений для застосування колаборативної фільтрації, використовується спрощений варіант гібридної моделі. За наявності мінімального текстового опису профілю на першому етапі , формуються TF-IDF вектори та обчислюється косинусна подібність із усіма вакансіями, що дозволяє сформувати початкову множину кандидатів. На другому етапі , ця множина сортується за допомогою семантичної моделі, яка уточнює порядок вакансій з урахуванням контекстної близькості текстів. Завершальний етап передбачає врахування показників популярності та новизни вакансій, що компенсує відсутність поведінкових даних користувача. Запропонований гібридний метод формування рекомендацій вакансій об’єднує вищезазначені методи в єдину каскадну структуру, що забезпечує підвищену точність та адаптивність рекомендацій [23]. 33 3 ІНФОРМАЦІЙНА ТЕХНОЛОГІЯ ФОРМУВАННЯ РЕКОМЕНДАЦІЙ ВАКАНСІЙ НА ФРІЛАНС -ПЛАТФОРМІ 3.1 Контекстн а діаграм а процесу формування рекомендацій вакансій на фріланс -платформі Для відображення процесу формування рекомендацій вакансій на фріланс - платформі та демонстрації логіки взаємодії основних інформаційних потоків, методів і технічних засобів, що забезпечують роботу рекомендаційної підсистеми, створено контекстну діаграму. Головний блок «Формування рекомендацій вакансій на фріланс -платформі» відображає інтегрований процес опрацювання даних користувача, у межах якого відбувається об’єднання колаборативних, контентних та семантичних підходів до створення рекомендацій вакансій. П роцес функціонує на основі гібридного методу. До системи надходять два базові інформаційні потоки, які визначають зміст подальшого аналізу. Історія взаємодій користувача – включає відомості про перегляди вакансій, їх збереження, подані заявки та інші поведінкові ознаки, що формують основу для застосув ання колаборативної фільтрації. Другий потік складають описові характеристики профілю користувача, які містять інформацію про його навички, професійний досвід, категорію діяльності та інші текстові атрибути, необхідні для контентно -орієнтованого та семанти чного аналізу. Ці дані виступають визначають здатність системи формувати релевантні персоналізовані рекомендації. Функціонування головного процесу контролюється трьома різними підходами, які накладають обмеження та задають правила обробки даних. Колаборативна фільтрація спрямовує систему на виявлення схожості користувачів та вакансій на основі взаємодії користувачів; контентно - орієнтована модель забезпечує побудову векторних подань текстових описів та оцінювання їхньої подібності за допомогою TF -IDF і косинусної міри. Семантична модель, заснована на контекстних векторних репрезентаціях, в 34 нашому випадку це BERT, дозволяє системі аналізувати зміст вакансій на рівні контекст. У сукупності ці методи формують керуючий вплив, який визначає послідовність оброблення даних, спосіб вибору релевантних вакансій та правила побудови фінального ранжуванн я. Для реалізації процесу використовуються технічні засоби нижнього рівня діаграми. Мова програмування Java забезпечує реалізацію серверної логіки платформи. СУБД MySQL відповідає за зберігання даних користувачів, вакансій і взаємодій між ними. Фреймворк Spri ng Boot виконує роль базового середовища для організації веб -сервісів і викликів, пов’язаних із рекомендаційним модулем. Сукупність цих засобів формує механізм системи, який забезпечує виконання алгоритмів і доступ до даних у режимі реального часу. Результатом роботи процесу, позначеним на правій стороні діаграми, є сформовані рекомендації – впорядкований список вакансій, який уже пройшов каскадну обробку початкових даних за допомогою всіх зазначених методів. Цей вихідний потік відображає кінцеву мет у системи – надання користувачеві максимально релевантних пропозицій, що відповідають його професійному профілю та історії поведінки на платформі. Таким чином, діаграма показує узагальнену архітектуру рекомендаційної підсистеми, у якій вхідні дані користувача перетворюються на персоналізований набір вакансій завдяки поєднанню методів і технологічних засобів. Контекстна діаграма зображена на рисунку 3.1 35 Рисунок 3.1 – Контекстна діаграма 3.2 Опис діаграми декомпозиції Для відображення деталізованої структури процесу формування рекомендації вакансій прийнято рішення провести декомпозицію контекстної діаграми. Дана діаграма демонструє внутрішню логіку функціонування рекомендаційної підсистеми, фіксуючи послідовність етапі в обробки даних та взаємодію між ними. Кожен елемент моделі виконує специфічну функцію, пов’язану з аналізом поведінкових або текстових характеристик користувача, що дозволяє поєднувати колаборативні, контентні та семантичні методи у єдиному алгоритмі. Перший блок «Отримання та обробка існуючих даних користувача» – відповідає за первинний прийом інформації, яка надходить до системи у вигляді історії взаємодій та описових характеристик профілю. На цьому етапі обробляються дані про перегляди вакансій, заяв ки, попередні"}
{"chunk_id": "2025_M_IUS_Yemelyanov_AV.pdf:9", "source": "2025_M_IUS_Yemelyanov_AV.pdf", "text": "Дана діаграма демонструє внутрішню логіку функціонування рекомендаційної підсистеми, фіксуючи послідовність етапі в обробки даних та взаємодію між ними. Кожен елемент моделі виконує специфічну функцію, пов’язану з аналізом поведінкових або текстових характеристик користувача, що дозволяє поєднувати колаборативні, контентні та семантичні методи у єдиному алгоритмі. Перший блок «Отримання та обробка існуючих даних користувача» – відповідає за первинний прийом інформації, яка надходить до системи у вигляді історії взаємодій та описових характеристик профілю. На цьому етапі обробляються дані про перегляди вакансій, заяв ки, попередні контракти, а також текстова інформація про навички, досвід та інші атрибути профілю. Отримані 36 відомості структуруються й уніфікуються, після чого передаються до наступного процесу. Саме цей блок вирішує задачу відокремлення поведінкових сигналів від текстової інформації, готуючи їх до подальшого використання у різних компонентах гібридного методу. Блок «Відбір схожих користувачів та формування початкового списку вакансій» – реалізується колаборативна фільтрація. На основі даних, сформованих на попередньому етапі, здійснюється пошук користувачів зі схожими взаємодіями, що дозволяє побудувати початковий н абір вакансій - кандидатів. У цьому ж блоці генерується початкова множина вакансій, яка надалі підлягає уточненню за допомогою текстового та семантичного аналізу. Блок «Формування векторних представлень та порівняння вакансій із профілем користувача» – відповідає за контентно -орієнтовану обробку інформації. На даному етапі система трансформує текстовий опис профілю користувача та кожної вакансії у векторні подання, п обудовані на основі TF -IDF. За допомогою косинусної міри подібності отримуємо оцінку, яка каже про схожість змісту вакансій із атрибутами профілю користувача. Цей процес виконує функцію фільтрації та перерозподілу початкової множини кандидатів, отриманої з попереднього блоку, уникаючи вакансі й, які текстово не відповідають профілю користувача. Блок «Ранжування вакансій та формування кінцевого списку рекомендацій» – реалізує завершальну фазу рекомендаційного процесу. На цьому етапі до системи входять результати попередніх компонентів, які піддаються кінцевій обробці із застосуванням семантичної моделі. Використовуючи контекстні векторні уявлення, що базуються на архітектурі трансформерів, алгоритм виконує високоточне порівняння вакансій з профілем користувача та формує підсумковий ранжований список. Завдяки ц ьому завершується каскадна інтеграція вс іх методів . Колаборативн а фільтрація забезпечу є пошук релевантних вакансій за взаємодіями , контентн о-орієнтована модель відповідає за структурну текстову відповідність, а семантична модель займається глибок им зіставлення м змісту. 37 Таким чином, діаграма декомпозиції зображує послідовність процесів, починаючи із збору та нормалізації даних користувача закінчуючи формування м кінцевої множини персоналізованих рекомендацій. Вона демонструє логі ку гібридної моделі, у якій кожен етап уточнює результати попереднього, забезпечуючи цілісність, адаптивність і високу точність рекомендацій у динамічних умовах фріланс -платформи. Діаграма декомпозиції зображена на рис . 3.2. Рисунок 3.2 – Діаграма декомпозиції 38 4 ПРАКТИЧНА РЕАЛІЗАЦІЯ ГІБРИДНОГО МЕТОДУ 4.1 Програмна реалізація методів рекомендаційної системи 4.1.1 Обгрунтування технологічних засобів Розробка гібридної рекомендаційної системи для фріланс -платформи повинна використовувати технології, які здатні забезпечити стабільну обробку великих обсягів даних, гнучкість архітектури та високу продуктивність під час формування рекомендацій. Мовою програмування для серверної частини обрано Java. Ця мова є однією з найпоширеніших мов у розробці високонавантажених систем, що робить її оптимальною для проєктів, де важлива продуктивність, масштабованість і безпека. Вона забезпечує сувору типобезпе ку, що змен шує кількість помилок на етапі компіляції та підвищує надійність розробки. Java має величезну екосистему бібліотек, що спрощує інтеграцію з базами даних, системами безпеки, сервісами машинного навчання та інструментами документування API. У процесі формування семантичних ознак система прийнято рішення використову вати додатковий сервіс, реалізований мовою Python, який відповідає за обробку текстових даних за допомогою моделей сімейства BERT. Python -сервіс функціонує як окремий компонент, який отримує текстові дані, генерує високорозмірний embedding і повертає його у вигляді масиву чисел для подальшого використання на Java -бекенді [24]. У ролі бекенд -фреймворку використано Spring Boot . Його обрано завдяки своїй модульності, широкій екосистемі та підтримці автоматичної конфігурації, яка значно скорочує час розробки. Spring Boot надає велику кількість готових рішень, що дозволяє зосередитися на реалізації бізнес -логіки рекомендаційної системи. Для збереження даних обрано MySQL 8.0+, що є надійною реляційною системою з оптимізованими механізмами індексації. 39 Сукупність обраних технологій - Java, Python, Spring Boot, MySQL - формує надійну основу для гібридної рекомендаційної системи. Така архітектура забезпечує масштабованість, продуктивність, гнучкість і легку підтримку. Вона дозволяє безпечно працювати з даними користувачів, оптимізує процес формування рекомендацій та забезпечує швидку роботу сервісу в умовах зростаючого навантаження. 4.1.2 Опис реалізації контентно -орієнтованої моделі Контент но-орієнтована модель реалізована через сервіс ContentBasedFilteringService . Фрагмент коду А.1 з додатку А, відображає реалізацію даного сервісу, який оцінює релевантність вакансій на основі інформації з профілю користувача та текстового опису вакансій. Реалізація поєднує TF -IDF векторизацію та точне зіставлення навичок. На першому кроці формується текстове представлення профілю користувача та вакансії. Побудовані текстові об’єкти перетворюються на числові вектори методом TF-IDF. У модулі використовується фіксований словник технічних термінів . На основі отриманих векторів , обчислюється косинусна схожість між профілем користувача та вакансією, що дає оцінку текстової схожості. Паралельно виконується зіставлення навичок. Навички нормалізуються, приводяться до однакового регістру і розбиваються на множини. Далі визначається кількість збігів між множинами навичок користувача та вакансії . Формується окрема оцінка навичок, що є важливим доповненням до TF -IDF. Фінальна"}
{"chunk_id": "2025_M_IUS_Yemelyanov_AV.pdf:10", "source": "2025_M_IUS_Yemelyanov_AV.pdf", "text": "кроці формується текстове представлення профілю користувача та вакансії. Побудовані текстові об’єкти перетворюються на числові вектори методом TF-IDF. У модулі використовується фіксований словник технічних термінів . На основі отриманих векторів , обчислюється косинусна схожість між профілем користувача та вакансією, що дає оцінку текстової схожості. Паралельно виконується зіставлення навичок. Навички нормалізуються, приводяться до однакового регістру і розбиваються на множини. Далі визначається кількість збігів між множинами навичок користувача та вакансії . Формується окрема оцінка навичок, що є важливим доповненням до TF -IDF. Фінальна оцінка контентно -орієнтованої моделі визначається як комбінація TF -IDF та оцінки навичок. Якщо збіг навичок є суттєвим, частка навичок у комбінованій оцінці збільшується, що дозволяє надавати пріоритет вакансіям, які користувач потенційно може виконати професійно. 40 4.1.3 Опис р еалізац ії колаборативної фільтрації Колаборативна фільтрація реалізована у вигляді сервісу CollaborativeFilteringService , який працює з історією взаємодій користувачів. Реалізація поєднує два підходи –КФ на основі взаємодій користувачів та КФ на основі об’єктів, що дозволяє формувати більш узгоджені прогнози. Реалізація даного сервісу наведено у фрагменті коду А.2 з додатку А. На першому етапі для кожного користувача формується вектор його взаємодій із вакансіями. Усі події переводяться у числову оцінку, а взаємодії з однією вакансією сумуються. У такий спосіб формується набір рейтингів, необхідний для подальших обчислень. КФ на основі взаємодії користувачів реалізовано через обчислення кореляції Пірсона між користувачами. Для кожної пари користувачів враховуються лише спільні вакансії, а для коректної оцінки необхідна наявність мінімум двох спільних елементів у векторах. Кореляція використовується як коефіціє нт схожості . Значення , які менше або дорівнюють 0 відкидаються, а позитивні коефіцієнти слугують вагами під час формування прогнозів. Для кожної вакансії, з якою взаємодіяли схожі користувачі, обчислюється зважена оцінка , що є прогнозом для цільового користувача. КФ на основі об’єктів реалізовано аналогічним чином . Замість порівняння користувачів порівню ються вакансії. Для кожної нової вакансії визначаються інші, з якими вона має значущу схожість за історією оцінок. Після обчислення кореляції вакансії формуються прогнози на основі рейтингів, поставлених користувачем схожим вакансіям. Фінальна оцінка методу є комбінацією двох видів КФ . Для об’єднання використовується середнє з фіксованими вагами, що дозволяє збалансувати внесок кожного підходу в загальний результат. 41 4.1.4 Опис реалізації семантичної моделі та BERT Семантична фільтрація реалізована в модулі SemanticFilteringService ґрунтується на використанні векторних представлень тексту, сформованих за допомогою моделі Sentence -BERT. Даний підхід дозволяє аналізувати вакансії не лише на рівні ключових слів, а й на рівні сенсових та контекстуальних зв’язків між профілем користувача та вакансіями. Реалізація даного сервісу наведено у фрагменті коду А.3 з додатку А. Завдяки цьому сервісу виявляється концептуальна схожість між запитами, які можуть мати суттєві лексичні розбіжності. Генерація семантичних векторів винесена у зовнішній Python -сервіс, оскільки сучасні трансформерні моделі значно ефективніше виконуються у Python -екосистемі, що забезпечує кращу продуктивність та стабільність. Реалізація BERT -сервісу наведено у фрагменті ко ду А.4 з додатку А. Сервіс працює на основі моделі all -MiniLM -L6-v2, яка створює embeddings розмірністю 384 компоненти, оптимізовані для семантичного порівняння коротких та середніх текстів. Java передає на сервіс сформований текст, отримує embedding у фор маті JSON та перетворює його у масив чисел типу float. Той самий механізм використовується як для профілів користувачів, так і для вакансій. Семантична схожість між профілем і вакансіями визначається за допомогою косинусної міри, яка є стандартом для порівняння багатовимірних векторів. Косинусна схожість дозволяє ефективно вимірювати схожість між embeddings, незалежно від їх абсолютних значень, що робить метод стійким до варіацій текстових описів. На основі цього для кожної вакансії формується числовий показник, який відображає ступінь концептуальної відповідності між профілем користувача та вакансією. Завдяки застосуванню BERT, сервіс здатний розпізнавати синонімію, парафрази, контекстно близькі формулювання та семантичні зв’язки. Це підвищує якість рекомендацій, особливо у випадках, коли описи вакансій або 42 профілі користувачів недостатньо структуровані або містять різні варіанти термінології. Узагальнюючи, семантична фільтрація забезпечує найвищий рівень змістовної точності в системі, оскільки оперує повноцінним семантичним простором. Вона підсилює обидва попередні методи формування рекомендацій і дозволяє формувати фінальні рекомендації, які найбільш повно відображають реальні компетенції та професійні інтереси користувача. 4.1.5 Опис реалізації гібридного методу Гібридний метод у системі виконує роль інтеграційного механізму, який поєднує результати трьох незалежних підходів – колаборативного, контентно - орієнтованого та семантичного – у єдину модель формування рекомендацій. Реалізаці ю гібридного методу наведено у фрагменті коду А.5 з додатку А. На початку гібридний модуль формує оцінки від колаборативної фільтрації. Цей етап виконує звуження списку вакансій, відокремлюючи ті, які мають поведінкову схожість. КФ працює з великими масивами даних взаємодій, тому вона здатна виявити закономірності, що будуються на реальному досвіді користувачів платформи. Результатом першого етапу є сформований набір вакансій, відсортованих за взаємодіями та придатних для подальшого етапу аналізу. Після визначення цього набору, алгоритм переходить до контентно - орієнтованого аналізу. Контентна модель переглядає кожну із відібраних вакансій та порівнює їх із профілем користувача за допомогою TF -IDF представлень та оцінки відповідності навичок. На цьому етапі система поступово переходить від поведінкових сигналів до змістовних характеристик, аналізуючи, наскільки текстові описи вакансій та вказані навички відповідають даним користувача. Результати контентн о-орієнтованого аналізу повторно 43 впорядковують"}
{"chunk_id": "2025_M_IUS_Yemelyanov_AV.pdf:11", "source": "2025_M_IUS_Yemelyanov_AV.pdf", "text": "платформи. Результатом першого етапу є сформований набір вакансій, відсортованих за взаємодіями та придатних для подальшого етапу аналізу. Після визначення цього набору, алгоритм переходить до контентно - орієнтованого аналізу. Контентна модель переглядає кожну із відібраних вакансій та порівнює їх із профілем користувача за допомогою TF -IDF представлень та оцінки відповідності навичок. На цьому етапі система поступово переходить від поведінкових сигналів до змістовних характеристик, аналізуючи, наскільки текстові описи вакансій та вказані навички відповідають даним користувача. Результати контентн о-орієнтованого аналізу повторно 43 впорядковують вакансії, підвищуючи позиції тих, що мають значний змістовний збіг. Таким чином формується більш точний список вакансій , який переходить на семантичний метод рекомендації . Вакансії, відібрані по змісту, проходять семантичну обробку. На цьому етапі для кожної вакансії і профілю користувача , за допомогою Sentence -BERT , генеруються векторні представлення, які відображають внутрішню семантику текстів. Косинусна міра подібності між цими векторами дозволяє визначити концептуальну близькість між професійним описом користувача та описом вакансій, навіть якщо текстові формулювання відрізняються по сенсу . На основі цієї обробки формується найточніша оцінка відповідності, здатна врахову вати контекстні зв’язки, синонімію та відтінки значення. Після отримання результатів трьох методів виконується їхнє об’єднання. Усі оцінки нормалізуються до єдиного масштабу, після чого беруть участь у фінальній ваговій комбінації. Комбінація цих методів будує збалансовану кінцеву оцінку . Після впорядкування за підсумковим значенням , формується остаточний список рекомендацій. У результаті гібридний метод забезпечує найвищий рівень точності рекомендації . Він компенсу є слабкі сторони кожного окремого підходу. Така архітектура дозволяє системі стабільно працювати як для нових користувачів з невеликим профілем, так і для досвідчених фрілансерів із великою історією взаємодій, забезпечуючи універсальність та надійність реко мендацій. 4.1.5 Демонстрація працездатності розробленого гібридного методу Для того, щоб продемонструвати роботу запропонованого гібридного методу була розроблена підсистема надання рекомендацій вакансій на фріланс - платформ і. Розглянемо її роботу детальн іше. 44 На першому етапі необхідно авторизуватись або зареєструватись у системі. Так, як у нас є список існуючих користувачів, авторизуємось у профіль одного із них. Даний користувач має інформацію у профілі та взаємодії по проектах, тому гібридний метод працюватиме успішно. Проф іль користувача та сформовані рекомендації представлені на рис . 4.1-4.2. Рисунок 4.1 – Профіль користувача 45 Рисунок 4.2 – Сформовані за допомогою гібридн ого метод у рекомендації Як ми можемо побачити, рекомендації формуюються успішно та підходять для нашого користувача. Також відображається відсоток релелвантності вакансії для зручності вибору. Тепер переглянемо випадок, коли у систему приходить новий користувач і виникає проблема «холодного старту» із -за відсутності взаємодій. Це означає, що при відсутній історій взаємодій, колаборативна фільтрація не буде працювати взагалі. У системі дану пробл ему автоматично вирішують контентно - орієнтована та сематнична моделі. Так як, при реєстрації, користувач повинен внести про себе інформацію, ці два методи спрацьовують одразу при створенні його профіля і форму ють рекомендацїі. Спробуємо створити користувач а та показати, чи коректно працює гібридний метод. Процес реєстрації користувача у системі та формування рекомендацій при ситуації «холодного старт у» наведено на рис . 4.3 – 4.4. 46 Рисунок 4.3 – Реєстрація профіля користувача Рисунок 4.4 – Сформовані рекомендації для нового користувача Як бачимо, рекомендації сформовані успішно та відповідають профілю створеного користувача. Єдиний недолік даного підходу , для вирішення «холодного старт у», полягає у швидкості формування рекомендацій. Так як відсутня КФ, яка звужує набір релевантних рекомендацій, то контентно - 47 орієнтованій та семантичній моделям необхідно проходити по всьому списку вакансій, тим самим уповільнюючи час формування пропозицій. Для прискорення роботи підходу, користувачу достатньо почати взаємодіяти із будь - якою вакансією. 4.2 Експериментальна перевірка працездатності гібридного методу 4.2.1 Опис використаних м етрик оцінювання Метрика Precision@K характеризує частку релевантних об’єктів серед перших 𝐾 елементів списку рекомендацій. Вона відображає “чистоту” верхньої частини списку: чим вищою є точність, тим менше нерелевантних вакансій система подає користувачу серед перших 𝐾 позицій. Формула Precision@K виглядає як [2 5]: 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 @𝐾(𝑢)=кількість релевантних елементів у топ -𝐾 𝐾 (4.1) де 𝐾 – розмір списку; 𝑢 – індекс користувача. Метрика Recall@K відображає, яку частку від усіх релевантних вакансій система змогла виявити в межах перших 𝐾 елементів списку. Таким чином, Recall@K характеризує повноту рекомендацій. Формула Recall@K виглядає як [2 5]: 𝑅𝑒𝑐𝑎𝑙𝑙@𝐾(𝑢)=кількість релевантних елементів у топ -𝐾 загальна ⁡кількість ⁡релевантних ⁡елементів (4.2) де 𝐾 – розмір списку; 𝑢 – індекс користувача. 48 Average Precision@K для користувача 𝑢 обчислює скільки релевантних вакансій знайдено в топ -K, і на яких позиціях вони знаходяться. На кожній позиції 𝑘, де рекомендований елемент релевантний, враховується Precision@k, після чого середнє цих значень нормується на кількість релевантних елементів. Формула для користувача 𝑢 [26]: 𝐴𝑃@𝐾(𝑢)=1 𝑟𝑢∑∗𝐾 𝑘=1𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 @𝑘(𝑢)⋅𝑟𝑒𝑙𝑢(𝑘) (4.3) де 𝐾 – розмір списку; 𝑘 – позиція в ранжованому списку; 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 @𝑘(𝑢) – точність на префіксі довжини 𝑘 для користувача 𝑢; 𝑟𝑒𝑙𝑢(𝑘) – індикатор релевантності елемента на позиції 𝑘 для користувача 𝑢; 𝑢 – індекс користувача; 𝑟𝑢 – загальна кількість релевантних елементів для користувача 𝑢. Mean Average Precision@K є середнім значенням AP@K по всіх користувачах. Ця метрика узагальнює якість ранжування в системі. Оцінка метрики висока тільки тоді, коли для більшості користувачів"}
{"chunk_id": "2025_M_IUS_Yemelyanov_AV.pdf:12", "source": "2025_M_IUS_Yemelyanov_AV.pdf", "text": "користувача 𝑢 [26]: 𝐴𝑃@𝐾(𝑢)=1 𝑟𝑢∑∗𝐾 𝑘=1𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 @𝑘(𝑢)⋅𝑟𝑒𝑙𝑢(𝑘) (4.3) де 𝐾 – розмір списку; 𝑘 – позиція в ранжованому списку; 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 @𝑘(𝑢) – точність на префіксі довжини 𝑘 для користувача 𝑢; 𝑟𝑒𝑙𝑢(𝑘) – індикатор релевантності елемента на позиції 𝑘 для користувача 𝑢; 𝑢 – індекс користувача; 𝑟𝑢 – загальна кількість релевантних елементів для користувача 𝑢. Mean Average Precision@K є середнім значенням AP@K по всіх користувачах. Ця метрика узагальнює якість ранжування в системі. Оцінка метрики висока тільки тоді, коли для більшості користувачів вакансії зосереджені у верхніх позиціях списку. Формула MAP@K виглядає як [2 6]: 𝑀𝐴𝑃@𝐾=1 𝑁∑∗𝑁 𝑢=1𝐴𝑃@𝐾(𝑢) (4.4) де 𝑁 – кількість користувачів у тестовій вибірці; 𝐴𝑃@𝐾(𝑢) – середня точність для користувача 𝑢; 𝑢 – індекс користувача; Метрика NDCG@K використовується для оцінювання якості ранжування в рекомендаційних системах. Вона показує, наскільки якісно система розміщує релевантні елементи у верхніх позиціях списку рекомендацій, порівнюючи отриманий порядок із ідеальним, у якому всі релевантні елементи стоять 49 максимально високо. CG@K – це метрика, що просто підсумовує релевантність елементів у топ - 𝐾, без урахування їх позицій. Формула CG@K виглядає як [27]: 𝐶𝐺@𝐾=∑∗𝐾 𝑖=1𝑟𝑒𝑙𝑖 (4.5) де 𝐶𝐺@𝐾 – сумарний здобуток релевантності у межах перших 𝐾 позицій; 𝐾 – розмір списку; 𝑟𝑒𝑙𝑖 – релевантність елемента на позиції 𝑖. DCG вводить знижувальний коефіцієнт, через який релевантні елементи на нижчих позиціях роблять менший внесок. Формула DCG@K виглядає як [27]: 𝐷𝐶𝐺@𝐾=𝑟𝑒𝑙1+∑∗𝐾 𝑖=2𝑟𝑒𝑙𝑖 log⁡2(𝑖+1) (4.6) де 𝐷𝐶𝐺@𝐾 – знижена кумулятивна релевантність у топ -𝐾; 𝑟𝑒𝑙1 – релевантність першого елемента: перша позиція не дисконтується; 𝑟𝑒𝑙𝑖 – релевантність елемента на позиції 𝑖; log⁡2(𝑖+1) – дисконт, що зменшує внесок елементів на нижчих позиціях; 𝑖 – позиція у списку рекомендацій. IDCG@K – це DCG, обчислений для ідеального впорядкування, де всі релевантні елементи стоять на максимально високих позиціях. Формула IDCG@K виглядає як [27]: 𝐼𝐷𝐶𝐺@𝐾=∑∗𝐾 𝑖=1𝑟𝑒𝑙𝑖∗ log⁡2(𝑖+1) (4.7) де 𝐼𝐷𝐶𝐺@𝐾 – максимально можлива величина DCG для тих самих релевантностей; 50 𝑟𝑒𝑙𝑖∗ – релевантність елемента на позиції 𝑖у ідеальному ранжуванні; log⁡2(𝑖+1) – дисконт, що зменшує внесок елементів на нижчих позиціях; 𝐾 – розмір списку. Розкривши значення та формули попередніх метрик, з яких складається NDCG@K, можемо сформувати його формулу, яка виглядає як [27]: 𝑁𝐷𝐶𝐺@𝐾=𝐷𝐶𝐺@𝐾 𝐼𝐷𝐶𝐺@𝐾 (4.8) де 𝑁𝐷𝐶𝐺@𝐾 – нормалізована оцінка якості ранжування; 𝐷𝐶𝐺@𝐾 – фактичний показник ранжування моделі; 𝐼𝐷𝐶𝐺@𝐾 – максимальний можливий DCG; 4.2.2 Результати експерименту та висновки В ході дослідження було п роведено експеримент та отримані результати експериментальної оцінки розробленого гібридного методу формування рекомендацій вакансій та здійснено їх порівняння з базовими підходами . У табл . 4.1 наведено узагальнені результати експериментальної оцінки якості роботи рекомендаційної системи. Таблиця 4.1 – Узагальнені результати експериментальної оцінки якості роботи рекомендаційної системи Метод K Precision@K Recall@K MAP@K NDCG@K Гібридний метод 5 0.2556 0.8056 0.5907 0.6220 10 0.1333 0.8333 0.5709 0.6334 15 0.1074 0.9722 0.5551 0.6770 20 0.0833 1.0000 0.5561 0.6850 51 Продовження таблиці 4.1. Метод K Precision@K Recall@K MAP@K NDCG@K Контентно - орієнтований метод 5 0.2000 0.5833 0.4833 0.4921 10 0.1278 0.7500 0.4969 0.5566 15 0.0926 0.8333 0.5018 0.5804 20 0.0778 0.9167 0.4949 0.6047 Семантичний метод 5 0.2222 0.6667 0.5250 0.5515 10 0.1167 0.6944 0.5343 0.5636 15 0.0926 0.8611 0.5471 0.6111 20 0.0750 0.9167 0.5381 0.6269 Колаборативна фільтрація 5 0.2500 0.5556 0.3954 0.4205 10 0.2242 0.6389 0.4042 0.4539 15 0.2205 0.6667 0.3954 0.4634 20 0.2205 0.6667 0.3954 0.4634 Аналізуючі значення метрик з табл 4.1 , можна зробити висновок , що гібридний метод демонструє себе як найбільш збалансований і якісний підхід для формування рекомендацій вакансій. За інтегральними метриками MAP@K та NDCG@K саме гібридний підхід стабільно випереджає всі інші методи для всіх значень K. Це означає, що р елевантні вакансії не просто присутні у списку, а й займають відносно високі позиції, тобто користувач бачить якісні вакансії у верхній частині списку рекомендацій. Варто звернути увагу, що Recall для гібридного підходу є дуже високим, що свідчить про здат ність системи охопити практично всі релевантні вакансії для користувача в межах сформованого списку. Контентно -орієнтований підхід на основі TF -IDF та семантичний метод з BERT виглядають логічною конструкцією для формування рекомендацій, але окремо вони поступаються гібридному підходу, тоді як поєднуючи їх з колаборативною фільтрацією вдається досягти помітного приросту якості пропозицій. Не дивлячись на гарні результати у більшості метрик, виявились також і 52 недоліки. Система все ще має простір для покращення. Хоча гібридний метод і випереджає інші підходи за MAP та NDCG, його Precision@K при K = 10, 15, 20 помітно зменшується, що означає збільшення кількості нерелевантних вакансій у списку. Гібридний підхід п оступається колаборативній фільтрації, бо вона, попри нижчі інтегральні метрики, іноді демонструє вищу точність, особливо на великих значеннях K, що вказує на потенціал для більш агресивного використання її сигналів у верхній частині списку. Таким чином, отримані метрики одночасно підтверджують коректність і ефективність обраного гібридного підходу, але й підкреслюють, що подальше налаштування ваг та логіки переранжування може суттєво покращити якість системи з точки зору користувацького досві ду. Графік и результат ів експериментальної оцінки рекомендаційної системи наведен о на рис . 4.5. Рисунок 4. 5 – Результати експериментальної оцінки рекомендаційної системи"}
{"chunk_id": "2025_M_IUS_Yemelyanov_AV.pdf:13", "source": "2025_M_IUS_Yemelyanov_AV.pdf", "text": "метрики, іноді демонструє вищу точність, особливо на великих значеннях K, що вказує на потенціал для більш агресивного використання її сигналів у верхній частині списку. Таким чином, отримані метрики одночасно підтверджують коректність і ефективність обраного гібридного підходу, але й підкреслюють, що подальше налаштування ваг та логіки переранжування може суттєво покращити якість системи з точки зору користувацького досві ду. Графік и результат ів експериментальної оцінки рекомендаційної системи наведен о на рис . 4.5. Рисунок 4. 5 – Результати експериментальної оцінки рекомендаційної системи 53 ВИСНОВКИ У кваліфікаційній роботі вирішено задачу підвищення якості формування рекомендацій вакансій у інформаційній системі фріланс -платформи шляхом розробки гібридного методу, що поєднує контентно -орієнтовану, колаборативну та семантичну фільтрацію. Показано, що ринок фріланс -послуг характеризується великою кількістю користувачів і проєктів, інформаційним перевантаженням та наявністю сценаріїв «холодного старту», де класичні методи працюють обмежено. У ході дослідження проаналізовано переваги й недоліки контентно - орієнтованої моделі, колаборативної фільтрації та семантичного підходу на основі моделей типу BERT. На основі цього запропоновано каскадний гібридний метод: для активних користувачів спочатку генерується множина кандидатів за допомогою КФ, потім вони ранжуються контентно -орієнтованим методом і, на завершальному етапі, формується кінцевий список семантичним модулем. Для нових користувачів застосовується спрощена схема, що поєднує контентно - орієн тований та семантичний аналіз із урахуванням популярності та новизни вакансій. Розроблено інформаційну технологію і програмний прототип рекомендаційної підсистеми на базі Java, Spring Boot, MySQL і зовнішнього сервісу семантичної обробки текстів. Експериментальна перевірка показала, що запропонований гібридний метод демонструє кращі значення метрик якості у порівнянні з окремим застосуванням проаналізованих методів по окремості. Таким чином, експерименти підтвердили успішність розробленого підходу і довели, що поєднання кіл ькох методів у гібридний метод дозволяє підвищити релевантність і стабільність рекомендацій для користувачів фріланс -платформи. 54 Роботу виконано відповідно до вимог методичних вказівок щодо розробки та оформлення кваліфікаційних робіт [26], а також із дотриманням положень державних стандартів ДСТУ 8302:2015 [27] і ДСТУ 3008:2015 [28 ]. 55 ПЕРЕЛІК ДЖЕРЕЛ ПОСИЛАННЯ 1. Freelance Forward 2022. Upwork . URL: https://www.upwork.com/research/freelance -forward -2022 (date of access: 01.11.2025). 2. The Gig Economy and the Future of Work. The World Bank . URL: https://documents1.worldbank.org/curated/en/099060524074041161/pdf/P1796 47-e104d722 -f2f4-4ec8 -9397 -d1ead6456d2e.pdf (date of access: 01.11.2025). 3. Jiang K. Reducing Information Overload in Recommender Systems. 2023. 60 p. URL: https://www.politesi.polimi.it/retrieve/43862559 -79b4 -491d -a7d2 - a11d247693b4/Dissertation_Kexin%20Jiang.pdf (date of access: 01.11.2025). 4. Digital Tiger: the Power of Ukrainian IT – 2023 - IT Ukraine Association. IT Ukraine Association . URL: https://itukraine.org.ua/digital -tiger-the-power -of- ukrainian -it-2023/ (date of access: 02.11.2025). 5. Статистика зарплат. Djinni . URL: https://djinni.co/salaries/ (дата звернення: 02.11.2025). 6. Recommendations: what and why?. Google for Developers . URL: https://developers.google.com/machine - learning/recommendation/overview (date of access: 06.11.2025). 7. Hardesty L. The history of Amazon's recommendation algorithm. Amazon Science . URL: https://www.amazon.science/the -history -of-amazons - recommendation -algorithm (date of access: 05.11.2025). 8. Engineering Blog. LinkedIn Engineering . URL: https://engineering.linkedin.com/blog (date of access: 05.11.2025). 9. How Upwork improves site performance. Upwork . URL: https://www.upwork.com/careers/engineering -blog/upwork -improves -site- performance (date of access: 05.11.2025). 10. Hustler T. What is Fiverr and How does it work - The Complete Beginner’s Guide. Medium . URL: https://medium.com/@TheHustler/what -is-fiverr -and-how- 56 does-it-work -the-complete -beginners -guide -c95ae449f8f5 (date of access: 05.11.2025). 11. Welcome to the Freelancer.com API Docs. Freelancer developers . URL: https://developers.freelancer.com/docs (date of access: 05.11.2025). 12. Research of Information Filtering Based on Vector Space Model. IEEE Xplore . URL: https://ieeexplore.ieee.org/document/5403435 (date of access: 15.11.2025). 13. THANDAPANI S. P. Recommendation Systems: Collaborative Filtering using Matrix Factorization – Simplified. Medium . URL: https://medium.com/sfu - cspmp/recommendation -systems -collaborative -filtering -using -matrix -factorization - simplified -2118f4ef2cd3 (date of access: 15.11.2025). 14. Zuidema W. Vector -Based and Neural Models of Semantics. ResearchGate . URL: https://www.researchgate.net/publication/364526728_Vector - Based_and_Neural_Models_of_Semantics (date of access: 15.11.2025). 15. Contributors to Wikimedia projects. tf –idf - Wikipedia. Wikipedia, the free encyclopedia . URL: https://en.wikipedia.org/wiki/Tf –idf (date of access: 30.11.2025). 16. Douglas C. Finding Word Similarity using TF -IDF and Cosine in a Term - Context Matrix from Scratch in Python. https://towardsdatascience.com/finding -word - similarity -using -tf-idf-in-a-term-context -matrix -from -scratch -in-python - e423533a407/ . URL: https://towardsdatascience.com/finding -word -similarity -using - tf-idf-in-a-term-context -matrix -from -scratch -in-python -e423533a407/ (date of access: 30.11.2025). 17. Collaborative Filtering. Statistics & Data Science . URL: https://wikistat.fr/pdf/st -m-datSc3 -colFil.pdf (date of access: 30.11.2025). 18. Item-item collaborative filtering. Grokipedia . URL: https://grokipedia.com/page/Item -item_collaborative_filtering (date of access: 30.11.2025). 19. Devlin J., Chang M.-W. BERT: Pre -training of Deep Bidirectional Transformers for Language Understanding. arXiv.org e -Print archive . URL: https://arxiv.org/pdf/1810.04805 (date of access: 01.12.2025). 57 20. Kaushicbaravind. Similarity of Word Embeddings with BERT: A Comprehensive Discussion. Medium . URL: https://medium.com/@kaushicbaravind/similarity -of-word -embeddings -with- bert-a-comprehensive -discussion -a641a5710325 (date of access: 02.12.2025). 21. Normalization: Complete Guide to Feature Scaling with Min -Max Implementation - Interactive | Michael Brenndoerfer. Michael Brenndoerfer | Data & AI, Private Equity, Technology . URL: https://mbrenndoerfer.com/writing/normalization -feature -scaling -min-max- machine -learning -guide (date of access: 02.12.2025). 22. Weighted Sum Method. ScienceDirect . URL: https://www.sciencedirect.com/topics/computer -science/weighted -sum- method (date of access: 02.12.2025)."}
{"chunk_id": "2025_M_IUS_Yemelyanov_AV.pdf:14", "source": "2025_M_IUS_Yemelyanov_AV.pdf", "text": "01.12.2025). 57 20. Kaushicbaravind. Similarity of Word Embeddings with BERT: A Comprehensive Discussion. Medium . URL: https://medium.com/@kaushicbaravind/similarity -of-word -embeddings -with- bert-a-comprehensive -discussion -a641a5710325 (date of access: 02.12.2025). 21. Normalization: Complete Guide to Feature Scaling with Min -Max Implementation - Interactive | Michael Brenndoerfer. Michael Brenndoerfer | Data & AI, Private Equity, Technology . URL: https://mbrenndoerfer.com/writing/normalization -feature -scaling -min-max- machine -learning -guide (date of access: 02.12.2025). 22. Weighted Sum Method. ScienceDirect . URL: https://www.sciencedirect.com/topics/computer -science/weighted -sum- method (date of access: 02.12.2025). 23. Ємельянов А ., Петров К. Гібридний метод формування рекомендацій вакансій для працівників фріланс -платформи [Електронний ресурс] // Evolving Science : Theories , Discoveries and Practical Outcomes , December 15-17, 2025. – Zurich , Switzerland . – Режим доступу: https ://www .eoss-conf.com/wp- content /uploads /2025/12/ Zurich _Switzerland _15.12.25. pdf (дата звернення: 12.12.2025). – С. 229-232. 24. Step-by-Step Guide: How to Use BERT Word Embeddings in Python. Medium . URL: https://medium.com/@Roy.Wong/step -by-step-guide -how-to- use-bert-word -embeddings -in-python -ac7b621771d8 (date of access: 04.12.2025). 25. Precision and recall at K in ranking and recommendations. Evidently AI - AI Evaluation & LLM Observability Platform . URL: https://www.evidentlyai.com/ranking -metrics/precision -recall -at-k (date of access: 0 7.12.2025). 26. Rink K. Mean Average Precision at K (MAP@K) clearly explained. Toward Data Science . URL: https://towardsdatascience.com/mean -average -precision -at-k- map-k-clearly -explained -538d8e032d2/ (date of access: 0 7.12.2025). 27. Normalized Discounted Cumulative Gain (NDCG) explained. Evidently AI - AI Evaluation & LLM Observability Platform . 58 URL: https://www.evidentlyai.com/ranking -metrics/ndcg -metric (date of access: 07.12.2025). 28. Методичні вказівки щодо розробки та оформлення кваліфікаційної роботи другого (магістерського) рівня вищої освіти за освітньо -професійною програмою «Інформаційні управляючі системи та технології» спеціальності 122 Комп’ютерні науки / Упоряд.: К.Е. Петров, В.М. Левикін, С.Ф. Чалий, М.В. Євланов, В.І. Саєнко, Д.К. Міхнов, А.В. Міхнова, О.В. Чала. ХНУРЕ: Харків, 2024. 24 с. 29. ДСТУ 8302:2015. Бібліографічне посилання. Загальні положення та правила складання. К.: ДП «УкрНДНЦ», 2016. 20 c. 30. ДСТУ 3008:2015. Державний стандарт України. Інформація та документація. Звіти у сфері науки і техніки. Структура та правила оформлювання. К.: ДП «УкрНДНЦ», 2016. 31 с."}
